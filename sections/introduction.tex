\section{Introduction}

% Since the 1970s, artists such as Suzanne Ciani~\cite{ciani_life_in_waves} have explored the use of audio synthesizers to replicate natural sounds—such as a bottle cap popping off—in ways that preserve the essential qualities of the original while showcasing the unique characteristics of the synthesizer~\cite{creativecherep2024}. Rather than merely replicating sounds, this approach uses synthesizers for artistic reinterpretation. However, the automation of this interpretive approach to sound design using digital audio synthesizers remains impractical for sound-designers, and significant knowledge gaps remain even in the simpler problem of replicating sounds. One such gap is the lack of diversity and depth in the analysis of sound-similarity measures and their interaction with various synthesis methods. As a stepping stone toward automated interpretive sound design, we use automatic and manual sound-evaluations to investigate whether the optimal choice of a sound-similarity measure is dependent on the synthesis method.

A digital audio synthesizer is any software used for the creation and manipulation of audio. The possibilities afforded by these synthesizers is infinite, leading to their widespread adaption by artists and sound-designers~\cite{lyons1997understanding,russ1999sound,stranneby2004digital}. A typical synthesizer has a number of parameterizable functions which affect the output sound in various ways, and manual sound-design often involves modifying the parameters until a conceptualized target sound is reached. The automation of this approach has commonly been referred to as "sound-matching", with reduction of tinkering time and creation of ``interesting" sounds as the main motivators ~\cite{krekovic2019insights,turian2020sorry,horner1993machine,salimi2020make,esling2019flow,engel2020ddsp,mitchell2007evolutionary,shier2020spiegelib,masuda2021soundmatch,masuda2023improving}. The main requirements of sound-matching are a target sound, a similarity metric (or loss function), and a heuristic to find parameters for a synthesizer that replicate \textit{all} or \textit{some} of the characteristics of the target sound as best as possible~\cite{horner1993machine,mitchell2007evolutionary,masuda2023improving}. However, despite the seeming simplicity of the problem and decades of research, sound-matching is not yet practical solution for sound-designers. Here we are motivated to understand why this is the case, and what future research should prioritize in order to improve the field. 

We provide a review of past works in order to understand what is lacking, a nomenclature for different subtypes of sound-matching, and illuminate major perennial issues. In particular, we are concerned with the issues of \LossSelect~and \SynthSelect. The former refers to the search for an optimal loss function, lack of novel algorithms, and the---perhaps needless---focus on outperforming state of the art (\gls{SOTA}), while the latter refers to the lack of diversity in synthesis methods. As a consequence of these problems, we find that not much is known about the interaction between different methods of synthesis and different loss functions. Is there a universally best performing loss function for audio? 


The main hypothesis here is that \textit{the performance of a similarity measure (or loss function) is influenced by other factors in the environment, particularly the method of synthesis}. While there have been many works and experiments comparing the accuracy of loss functions~\cite{vahidi2023mesostructures,turian2020sorry,engel2020ddsp,uzrad2024diffmoog,han2023perceptual,masuda2021soundmatch,turian2020sorry,bruford2024synthesizer}, we note that claims regarding the effectiveness of one function versus another have consistently been made in limited contexts that may not generalize to other settings. Beyond this central hypothesis, we also investigate three additional research questions: (Q1) To what extent do automatic evaluation metrics agree with manual listening tests? (Q2) Can novel loss functions based on Dynamic Time Warping (DTW) and Scale-Invariant Mean Squared Error (SIMSE) provide advantages over standard loss functions, and under what conditions?
(Q3) Is an iterative differentiable optimization a viable strategy for design of sound-matching experiments?



% This main hypothesis (along with our forthcoming research questions) would help with important issues regarding the path of future research. Should we be searching for a universally best performing loss function across synthesizers? Do current, SOTA loss functions satisfy the creative needs of sound-designers? 

 We take an \textit{iterative} and \textit{differentiable} approach to defining our sound-matching experiments. The iterative approach better mimics the manual process of recursive listening and parameter adjustments towards sound design, while a differentiable environment allows access to the loss function gradients that can be used to better understand the nature of the problem. We define four differentiable synthesizers (each showcasing a fundamental method of synthesis used in modern synthesizers) and pair them with four different loss functions (two established methods, one utilizing DTW, and one utilizing SIMSE). We evaluate the final similarity with two different automatic methods, as well as manual listening tests conducted by two of the authors. Using the Non-parametric Scott-Knott tests~\cite{tantithamthavorn2017mvt,tantithamthavorn2018optimization}, we compare the distributions of the evaluation scores in order to rank the loss functions from best to worst. 


The results of these experiments show that when accounting for different synthesizers, there is no consistent choice of sound similarity measure for optimal performance. Regarding Q1, We see a large overlap between automatic and manual evaluation methods, but continue to encourage manual listening tests as the gold standard for audio evaluation as extreme disagreements can occur. In the case of Q2, we confirm that \DTWEnv~outperforms other methods when envelope modulation plays a central role, and \SIMSESpec~is the best performer in simple subtractive synthesis. For Q3, we found the declaration of differentiable sound-matching experiments using Jax and transpiled Faust code to be a convenient and resource efficient, and and overall viable general strategy for sound-matching in differentiable settings. These results imply that rather than aiming to outperform the SOTA and finding a universal best performer, more effort could be placed towards diversifying the existing similarity measures of sound as well as conducting experiments using a variety of synthesis methods. Sound design is an inherently creative endeavor, and would likely benefit from a diversified range of solutions for different approaches.




% Here, several common characteristics of previous work are avoided in order to gain fundamental, practical takeaways from the experiments. In order to directly analyze the interaction between the loss and method of synthesis, the synthesizer here are implemented with simple, functions that are used as the building blocks of complex synthesizers, and the gradients flow from the loss function to the synthesizer parameters with no extra layers between the two. Furthermore, since the true target parameters are unknown in any realistic sound-design scenario, parameter loss (P-Loss) is only used as a secondary measure of performance, and not used as a loss function.

% The simple synthesizers tested here represent fundimental building blocks of more complex implementations, and direct use of loss function gradients assuming 
%   A primary conclusion of the work is that there is no one best sound similarity measure capable of working across all synthesizers.
% The core problem that no single loss function performs optimally across synthesizer types
