\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{csquotes}
\usepackage{makecell}
% my added packages and commands =======================
\usepackage[numbers]{natbib} %removed cite package
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{listings}

\usepackage{caption}               % re-enable caption functionality
\captionsetup[lstlisting]{justification=centering, singlelinecheck=false}

\usetikzlibrary{positioning,shapes.geometric}

\providecommand{\gls}[1]{#1}
\definecolor{highlight}{RGB}{00,150,00}
\definecolor{todo}{RGB}{200,50,00}
\definecolor{rank1}{HTML}{70FF70}
\definecolor{rank2}{HTML}{858585}
\definecolor{rank3}{HTML}{454545}
\definecolor{rank4}{HTML}{000000}

\newcommand{\staronsquare}{
    \tikz[baseline=0ex]{
        % Blue square with increased size
        \fill[blue] (0,0) rectangle (2ex,2ex);
        % Yellow star character centered on the square, larger font size
        \node at (1ex,0.8ex) {\textcolor{yellow}{\large *}};
    }
}

\newcommand{\vertdashred}[1][0.3cm]{% Default height is 2cm
    \tikz[baseline]{\draw[red, dashed,line width=0.5mm] (0,0) -- (0,#1);}
}

\newcommand{\SIMSESpec}{\texttt{SIMSE\_Spec}}
\newcommand{\LoneSpec}{\texttt{L1\_Spec}}
\newcommand{\JTFS}{\texttt{JTFS}}
\newcommand{\DTWEnv}{\texttt{DTW\_Envelope}}

\newcommand{\LossSelect}{\textbf{Loss Selection}}
\newcommand{\SynthSelect}{\textbf{Synthesis Selection}}
\newcommand{\PeriodicLoss}{\textbf{Periodic Loss}}
\newcommand{\OutDomain}{\textbf{Out-Domain Generation}}

\newcommand{\BPNoise}{\textbf{BP-Noise}}  
\newcommand{\AddSineSaw}{\textbf{Add-SineSaw}}  
\newcommand{\AmpMod}{\textbf{Noise-AM}}  
\newcommand{\FMMod}{\textbf{SineSaw-AM}}  

\input{code_style}
%  ================================


\begin{document}

\title{Sound Similarity, Sound Matching, and Differentiable Audio Synthesizer Control}

\author{-}
\maketitle


\begin{abstract}
Manual sound design with a synthesizer is an iterative process. When an artist has a target sound in mind, they listen to the output of their chosen synthesizer, determine whether the output is closer or farther from the target, and adjust the synthesizer's parameters until satisfied. There are practically an infinite number of parameter combinations, and their effect on the output sound is hard to predict. Iterative sound-matching is the automation of this process, more formally, the automatic programming of an audio synthesizer to generate outputs similar to a target sound while guided by a loss function that quantifies sound similarity.
Past works in sound-matching often relied on a narrow range of synthesizers and overlooked the possible interactions between the loss function and synthesis parameters.
We analyzed the interplay between four simple subtractive, additive, and FM synthesizers when guided by four different novel or established similarity functions.
For each loss function and synthesizer pair, we ran 300 experiments with randomized initial values and targets, and used the final parameter configurations and spectrogram distances as key performance evaluation metrics of the performance. A post-hoc analysis of these performance measures showed that the best‐performing loss function varies based on the method of synthesis at hand, which highlights the value of novel similarity measures and de-prioritizes the search for the elusive universally optimal sound similarity measure.
\end{abstract}

\begin{IEEEkeywords}
Machine Learning, Signal Processing, Sound-Matching
\end{IEEEkeywords}

\section{Introduction}
\subsection{Digital Synthesis With Sound-Matching}
Since the 1970s, artists such as Suzanne Ciani~\cite{ciani_life_in_waves} have explored the use of audio synthesizers to replicate natural sounds—such as a bottle cap popping off—in a way that preserves the essential qualities of the original while showcasing the characteristics of the synthesizer. This approach goes beyond mere replication and uses synthesizers for artistic re-interpretation of sounds. Inspired by this, we aim to emulate and automate this process using digital audio synthesizers.

A digital audio synthesizer is any software used for the creation and manipulation of audio. Since the advent of digital signal processing (\gls{DSP}) in the 1960s~\cite{stranneby2004digital}, a wide variety of parametric audio generation functions have been proposed~\cite{lyons1997understanding,russ1999sound,shier2020spiegelib}. Manual sound design with a synthesizer is an iterative process of trial and error; when the artist has a desired (or target) sound in mind, sound design involves listening to the output, determining whether the output is closer or farther from the target, and adjusting the parameters accordingly until the target is reached~\cite{russ1999sound}. This paper explores the automation of this process, which we call \textit{iterative sound-matching}.

Automatic sound design has been a subject of research for decades, with reduction of tinkering time and creation of ``interesting" sounds as the main motivators \cite{krekovic2019insights,turian2020sorry,horner1993machine,salimi2020make,esling2019flow,engel2020ddsp,mitchell2007evolutionary,shier2020spiegelib}. \textbf{Sound-matching} is a common form of automatic sound design explored in literature, where the goal is to find parameters for a synthesizer that replicate \textit{all} or \textit{some} of the characteristics of a target sound as best as possible, given some kind of similarity metric~\cite{horner1993machine,mitchell2007evolutionary}. Not all sound-matching works follow the iterative process of manual sound design. \textit{Iterative} sound-matching refers to works which mimic the back and forth process of listening to a synthesizer's output and updating the parameters until the termination conditions are met.

A brief description of the iterative sound-matching workflow may be useful here: The two necessary components for sound-matching are the \textit{loss function}, which determines how close two sounds are, and the \textit{parametric synthesizer} that outputs sounds based on input parameters. In the iterative workflow, a target sound is selected, the synthesizer outputs sounds based on its parameters, and the loss function determines the proximity of the output to the target. From there, the synthesizer parameters are updated, the output and target are compared, and so on. 

With the goal of implementing effective iterative sound-matching algorithms for sound designers, we identify a number of weaknesses in current relevant literature (see Section~\ref{sec:lacking}). The experiments in this work pertain to two core issues, which we named \LossSelect~and \SynthSelect. The former refers to the search for an optimal loss function, lack of novel algorithms, and the focus on outperforming state of the art (\gls{SOTA}), while the latter refers to the lack of diversity in synthesis methods, and their interaction with the loss function.

In the case of \LossSelect, there have been many works and experiments comparing the accuracy of loss functions and choosing the best performers~\cite{vahidi2023mesostructures,turian2020sorry,engel2020ddsp,uzrad2024diffmoog,han2023perceptual,masuda2021soundmatch,turian2020sorry,bruford2024synthesizer}.  We note that these claims regarding the effectiveness of one function versus another have consistently been made in limited contexts that may not generalize to other settings. One such limitation is the synthesizers used, and ties into the problem of \SynthSelect, as typically only a single (and often, simple) synthesizer has been used for testing the effectiveness of loss functions. Furthermore, there is a lack of diversity in the underlying functions used by the synthesizers, with simple Frequency Modulation (\gls{FM}) being the most common method of synthesis~\cite{horner1993machine,vahidi2023mesostructures,mitchell2007evolutionary,caspe2023envelope}. 

Considering the interplay between the two problems, we design experiments that address them simultaneously. We hypothesize that the performance of a similarity measure is influenced by other factors in the environment, particularly the method of synthesis. We combine 4 loss functions with 4 differentiable synthesis methods, and compare their performance (as measured by the similarity of the final output or parameters to the target or target parameters). Our experiments show that when accounting for different synthesizers, there is no consistent choice of sound similarity measure for optimal performance in sound-matching. In some experiments, \DTWEnv, a novel loss function that only looks at envelope similarity, matches or outperforms commonly used methods of audio comparison. These results imply that rather than aiming to outperform the SOTA and finding a universal best performer, more effort should be placed towards diversifying the existing similarity measures of sound as well as conducting experiments using a variety of synthesis methods. Sound design is an inherently creative endeavor, and would likely benefit from a diversified range of solutions for different approaches. 

\subsection{Differentiable Sound-Matching}
An important caveat is that this work takes a \textit{differentiable}, machine learning (\gls{ML}) inspired approach to iterative sound-matching. A differentiable approach requires differentiable loss and synthesizer functions, allowing the synthesizer parameters to be updated using gradient descent~\cite{goodfellow2016deep,boyd2004convex}. Ideally, these updates are in a direction that improves audio similarity by reducing loss. Differentiable iterative sound-matching has only recently become viable thanks to continued advances in both computation hardware~\cite{moore2012introduction,schaller1997moore}, and machine learning software~\cite{deepmind2020jax,paszke2019pytorch}.
 
 As we discuss in Section~\ref{sec:related_works}, nearly all past works pertaining to iterative sound-matching have used genetic algorithms (\gls{GA})~\cite{holland1992genetic} where parameters are randomly changed with hopes of increasing similarity. Differentiable settings have the benefit of making more directed updates to the synthesizer parameters, but have major drawbacks. Other than requiring more computation power, differentiable functions require careful implementation of operations that are continuous, numerically stable, and of course, differentiable. 
 


\section{Background}
In this Section, we give a formal definition of sound-matching and the necessary components. The \textit{type} of sound-matching problem is dependent on the implementation of these components and the source of target sounds. We give a summary of different possible implementations and goals for sound-matching, and provide brief examples of previous works where applicable. We also give a typology of sound-matching types and major perennial issues that need to be addressed in future works. Finally, we introduce our formal definition of \textit{iterative sound-matching} in Section~\ref{sec:iterative_sound_matching}.

% Due to their pivotal role in our experiments, a discussion of similarity measures, or \textit{loss functions} is provided in Section~\ref{sec:loss_funcs}. A formal definition of loss functions and specific implementations of four loss functions are given. 

\label{sec:background}

\subsection{Digital Audio Synthesis}
\label{sec:diffSynth}
A digital audio synthesizer is any software used for the creation and manipulation of digital audio. Digital synthesizers use signal processing chains with a variety of digital signal processing (\gls{DSP}) functions to create sounds. These functions are often parametric, and the set of parameters given to the synthesizer's chain of DSP functions is called a synthesizer program.

The simplest form of DSP function is a sinusoidal tone. For example, consider:
\[ x(n) = sin( 2\pi n T)\]

where $T$ represents time in seconds, and $n$ is the frequency parameter. If $n$ is 1, then $x(n)$ would be a waveform with a frequency of 1 hertz as the value of $x(n)$ would oscillate once per second. Waveform generators are called oscillators for this reason. If $n$ is in the range of 30 to 14,000 (the upper limit can be higher for some), $x(n)$ would be an audible tone~\cite{smith2007mathematics,jilek2014reference}.

Since the advent of digital signal processing in the 1960s~\cite{stranneby2004digital}, a wide variety of parametric audio generation functions have been proposed~\cite{lyons1997understanding,russ1999sound,shier2020spiegelib}. This includes oscillators, filters, equalizers, and envelopes, which can be used sequentially or in parallel~\cite{lyons1997understanding,russ1999sound}.  Sound design with a synthesizer is done by modifying the parameters of the functions until reaching a desired output~\cite{roads1996computer,pinch2004analog}.  

A relatively small subset of these synthesis methods have been used for automatic sound-matching, notably additive and/or subtractive synthesizers~\cite{esling2019flow,yee2018automatic,mitchell2007evolutionary,salimi2020make}, physical modeling~\cite{riionheimo2003parameter}, and frequency/amplitude modulation (\gls{FM}/\gls{AM})~\cite{horner1993machine,vahidi2023mesostructures}. 

Differential DSP (\gls{DDSP}) is a general term for applications that combine machine learning techniques such as gradient descent~\cite{goodfellow2016deep,boyd2004convex} with DSP~\cite{engel2020ddsp}. Here, we take a differential approach to sound-matching using DDSP, i.e., the use of differentiable loss functions and gradient descent to update synthesizer parameters to reduce the difference between the output and the target sound. 

Implementing complex DSP functions in a differentiable manner can be challenging, and effective differentiable audio similarity measures require careful mathematical expression of the desired attributes of sound. These issues have likely contributed to the limited exploration of this domain. Perhaps the most relevant recent work is the comparison of two differentiable loss functions with a simple FM synthesizer by Vahidi \textit{et al.}~\cite{vahidi2023mesostructures}.

\subsection{Sound Representation and Loss Functions}
\label{sec:loss_funcs}
A digital sound (or an audio signal) is a series of numbers~\cite{smith1991viewpoints,smith2007mathematics}. To compare two digital sounds, the two corresponding series are passed to a function that measures their similarity. Two signals can sound identical to our ears, without having any values in common~\cite{moore2012introduction}. This necessitates the use of proxy representations (or feature extractors) when comparing sounds automatically. Similarity between the target sound and the synthesizer output is then measured by some form of subtraction and summation of the proxy representations.

In sound-matching, particularly in a Deep Learning (\gls{DL}) context~\cite{goodfellow2016deep}, the similarity function can also be called a \textit{loss} function, where the emphasis is on measurement and reduction of the distance between target and output. It is important to note that there is a close relationship between the loss function $L$ and the sound representation function $\phi$. A distance measure $d$ is applied to the features extracted by $\phi$. 

\[
L(\theta, t) =  d\langle\phi(t),\phi(x)\rangle
\]

\noindent

A proxy representation is the output of the function \( \phi \), which can be thought of as a feature extraction function that maps the sounds \( t \) and \( x \) to their respective representations. 
The proportionality or distance metric $d$ has typically been the L1 distance~\cite{turian2020sorry}, calculated as the mean of the absolute difference between every point in the proxy representation~\cite{engel2020ddsp,vahidi2023mesostructures}:
\[
L(\theta, t) = \left\| \phi(t) - \phi(x) \right\|_1
\]

Here we discuss four methods of audio representation and the corresponding loss functions. Three of which have been used in previous works, and a novel method which we will make use of in our own experiments. 

\subsubsection{Parameter Loss}
A common measure of similarity in sound-matching is the distance between synthesizer parameter sets, referred to as ``P-Loss"~\cite{han2023perceptual}. Typically, for the implementation of P-Loss the parameter sets are treated as vectors in space, and L1 or L2 distance is applied. There are two major limitations to this approach: First, the target and output sound must be made by the same synthesizer; otherwise the parameter sets cannot be compared (see Section~\ref{sec:matching_types}). Second, the relationship between synthesizer parameters and the audio output is not linear~\cite{shier2020spiegelib,han2023perceptual,esling2019flow}. 

\subsubsection{Fourier Spectrograms}
\label{sec:fourier_specs}
Fourier-based transformations such as short-time Fourier transforms (\gls{STFT}), Mel-spectrograms, and Mel-frequency cepstral coefficients have been viewed as the de facto and state-of-the-art representation of audio~\cite{beauchamp2003error,mitchell2007evolutionary,yee2018automatic}, however, there are many issues associated with their use in sound-matching~\cite{turian2020sorry,vahidi2023mesostructures,han2023perceptual,uzrad2024diffmoog}. Fourier transformations allow for the conversion of a signal from the time-domain to the frequency domain. Audio spectrograms can be generated by segmentation of a piece of audio into overlapping windows followed by the application of Fourier transforms to each window. They are costly to compute, but provide a better temporal view of changes in frequency content~\cite{muller2007dynamic,smith2007mathematics}. There are different types of spectrograms that have a basis in Fourier transformations, but the most notable and commonly used is the STFT.  

What we call \textit{Fourier-based Spectrograms} are variations on the STFT approach. For example, Mel-Spectrograms \textit{bin} frequencies on a near-logarithmic scale to better match human perception of frequencies~\cite{muller2007dynamic}. Multi-scale spectrograms (\gls{MSS}) used in recent works are a simple weighted average of multiple spectrograms with different parameters such as window size, number of frequency bins, and hop size~\cite{engel2020ddsp,vahidi2023mesostructures}; this may provide some improvements at a higher computational cost~\cite{turian2020sorry,engel2020ddsp}.


\subsubsection{Joint-Time Frequency Spectrum}
Recent works have focused on the limitations of parameter and spectral loss functions in sound-matching~\cite{vahidi2023mesostructures,uzrad2024diffmoog}, seeking to create more effective general solutions for the comparison of audio. 
Noting the aforementioned weaknesses of comparing STFT spectrograms, Vahidi \textit{et al.} proposed differentiable Joint-Time Frequency Scattering (\gls{JTFS})~\cite{anden2015joint} as an alternative to spectrogram loss in sound-matching, and showed improved performance in sound-matching with differentiable chirplet synthesizers~\cite{vahidi2023mesostructures}.  JTFS is the result of the application of a 2D wavelet transformation to the time-frequency representation of a signal~\cite{anden2015joint}. 

\subsubsection{Dynamic Envelope Warping}
Dynamic Time Warping (DTW) is a method for measuring similarity between multi-dimensional time-series~\cite{rabiner1993fundamentals,muller2007dynamic,giorgino2009computing}. Given any two time-series $X = \{x_1,x_2,...,x_m\}$ and $Y = \{y_1,y_2,...,y_n\}$, we have indices $i\in\{1...m\}$ and $j\in\{1...n\}$ defining $X$ and $Y$. When the series are \textit{warped}, these indices change to expand or contract different portions of the series. To borrow the notation given by Muller~\cite{muller2007dynamic}, warped indices are a sequence $p=(p_1,...,p_L)$, where \(p_\ell = (m_\ell, n_\ell) \in [1 : m] \times [1 : n] \text{ for } \ell \in [1 : L]\), meaning that the indices for $X$ and $Y$ are reorganized under special conditions. In classical DTW, these conditions are \textit{monotonicity}, \textit{boundary matching}, and \textit{single step-size}.

DTW measures the distance between the time-series \textit{after} alignments, typically using Euclidean distance. The distance between a time-series and shifted versions of itself would be 0, regardless of shift amount~\cite{tavenard.blog.dtw}. Additional rules can be imposed to keep alignments locally constrained~\cite{itakura1975minimum,sakoe1978dynamic}.
% \textcolor{red}{DTW was originally used for signal comparison but using it the way we do is novel}

\subsection{Automatic Sound Matching}
\label{sec:sound_matching_definition}
In past works the sound-matching problem has been tackled from a variety of perspectives as it involves a number of modular parts that need to be considered: the choice of synthesizer $g$, the target sounds of interest, the representation function $\phi$, and the heuristic for finding the optimal $\theta$. We discuss these components in the upcoming sections, and give a coherent typology of the sound-matching tasks in Section~\ref{sec:matching_types}. 

To give a formal definition of sound-matching, we expand the definition given by recent works~\cite{vahidi2023mesostructures,han2023perceptual}. The major components are as follows: 
\begin{itemize}
    \item $g(\theta)$: Parametric audio synthesizer $g$ which takes on parameters $\theta$ 
    \item $x$: The output of $g$, given a set of parameters $\theta$ or $g(\theta) = x$ 
    \item $t$: The target sound which we want to replicate or imitate. 
    \item $\phi$: Representation function or feature extractor. $\phi$ is applied to $x$ and $t$ to facilitate their comparison using the loss function.
    \item $L$: Loss or error function. $L(\theta,t)$ is a measure of distance between $x$ and $t$. Often proportional to the subtraction of their representations, or $ \phi(x) - \phi(t)$
\end{itemize}

We view the implementation of these components not as an optimization problem with a correct answer, but a \textit{creative} endeavor depending on the artist's needs. Consider the case where the goal is to create an 8-bit version of a high quality, organic snare drum~\cite{collins2007loop}: in such a case, there is no optimal or ``correct" version of an 8-bit snare sound, and multiple correct answers exist.   



\subsection{Sound Matching Types}
\label{sec:matching_types}

Sound-matching tasks can be categorized based on the choices of target sound $t$, the loss $L$ and representation function $\phi$, and the heuristic used for minimizing $\theta$. We will discuss the subtypes of sound-matching tasks in the upcoming sub-sections.

\subsubsection{In-Domain vs Out-of-Domain}
\label{sec:in-domain}
The choice of domain depends on whether we want to use the same synthesizer for the target and output sounds, which we call \textit{in-domain}, or have target sounds that came from sources other than the synthesizer, or \textit{out-of-domain}. To paraphrase the description given by Masuda \textit{et al.}~\cite{masuda2021soundmatch}, if $g$, the synthesizer of choice, can accurately replicate the target sound $t$, or put differently, if $t$ itself is an output of $g$, then the sound-matching task is \textit{in-domain}. If $t$ is not an output of $g$, then the sound-matching task is \textit{out-of-domain}. In-domain tasks in general are simpler, and often there is a guarantee that there is a correct answer to the sound-matching problem, particularly if the goal is accurate replication of the sound. If the target sound is out-of-domain, replication is not guaranteed, and the goal becomes the \textit{imitation} of some aspect of sound. 

\subsubsection{Replication vs Imitation}
Regardless of the domain, the generation goal can be \textit{replication} or \textit{imitation} of the target sound. In replication, the goal is to make an identical copy of the target sound. With imitation, the goal is to make new sounds that only retain a subset of the target's sonic features. While closely related to the in-domain versus out-of-domain problem, the choice of replication versus imitation is more dependent on how the loss and representation functions are defined. 

From a signal perspective, replication is a scenario where the goal is to find a $\theta$ such that $t-x=0$ where $x=g(\theta)$. From a sound perspective, this definition of replication is unnecessarily restrictive, as $t$ and $x$ can sound identical but contain very different values. For example, if $t$ and $x$ were periodic waveforms at an audible frequency $f$ with different phase values, their subtraction would not be 0, yet they would sound identical to our ears, as our ears are insensitive to phase values~\cite{moore2012introduction}. Representation functions such as STFT spectrograms or Mel-frequency cepstral coefficients (\gls{MFCC}) become viable in these cases, as they allow for separation and removal of the phase information within an audio signal. The \textit{sound replication} goal can be defined, in layman's terms, as ``$x$ sounds exactly like $t$". More formally, the goal is to find parameters such that $\phi(t)-\phi(x)=0$, where $\phi$ transforms an audio into a spectrogram that removes the phase information from sounds, but otherwise represents the sonic information as accurately as possible.

With \textit{sound imitation}, the goal is the replication of some aspect of the target sound. Instrument conversion is a common example of imitation. As an example, a note played by a violin is converted to the same note played by a piano. Conversion of even longer pieces has been made possible with large enough datasets~\cite{engel2020ddsp}. Imitation of sound envelopes (how the loudness changes over time) is another interesting idea~\cite{caspe2023envelope}. Transfer of other sound properties such as reverb and delay has; also been a subject of research~\cite{engel2020ddsp}. 

A target sound can have many properties of interest, and the imitation task is the transfer of a subset of those properties to other sounds. Relative to replication, imitation is a very open ended goal. From a research point of view, success in the replication task can be measured more reliably, often without the need for manual evaluation. With imitation, listening tests are needed to ensure quality of outputs, particularly in a supervised learning setting where learning is done by examples~\cite{salimi2020make,masuda2021soundmatch}. The need for subjective evaluations has perhaps contributed to the lack of research in imitation and out-of-domain sound-matching.

\subsubsection{Supervised Versus Direct Optimization}
\label{sec:optimization}

The goal of sound-matching is to find the optimal parameters $\theta^*$ that minimize the loss between the synthesizer output and the target sound. 
\[
\theta^* = \arg\min_{\theta} L(\theta,t)
\]


The heuristics in past works can be broadly split into two categories:
\begin{enumerate}
    \item Supervised methods which use datasets of sound and parameter pairs to describe the generation objective. Often with a large number of input and output examples~\cite{engel2020ddsp,salimi2020make,yee2018automatic,esling2019flow}
    \item Unsupervised or \textit{direct sound optimization} methods~\cite{uzrad2024diffmoog}. Usually require at most one target example, and a loss function that measures the difference between the target and synthesizer outputs. 
\end{enumerate}

Genetic algorithms (\gls{GA})~\cite{holland1992genetic} can be used for direct sound optimization, and have been the earliest and most common heuristic in sound-matching~\cite{horner1993machine,mitchell2007evolutionary,yee2018automatic,uzrad2024diffmoog}. These algorithms start with arbitrary parameter sets that can be treated as an evolving population where the genomes are the parameter values. The most fit members of the group are the parameters which perform the best in the loss function. The most fit parameters create a new generation of parameters via mutation (random change in subset of parameters) and crossovers (combination of parameter sets), and the process repeats until the goal or a maximum number of generations is reached. 

One important aspect of the heuristics is whether they imitate the iterative nature of manual sound design---that is, listening to the output and adjusting the synthesizer parameters recursively until the desired goal is reached. Supervised methods generate their final output in a single step, while direct sound optimization methods refine their outputs iteratively until termination, which resembles manual sound design more closely.

\subsection{Iterative Sound Matching}
\label{sec:iterative_sound_matching}
The diagram in Figure~\ref{fig:sound_design_loop_iterative} shows a general model for iterative sound-matching that covers previous works in this domain. To begin the process, the parameters are arbitrarily initialized (usually random generation), the similarity of the target and output is measured, and the parameter is \textit{optimized} with the goal of increasing the similarity, or alternatively, reducing the loss. This process repeats until termination. 

\begin{figure}[ht]
    \centering
\begin{tikzpicture}[node distance=2cm, auto]

    % Nodes
    \node (start) [text centered] {\( g (\hat{\theta}) = x \)};
    \node (L) [above of=start,right of=start, text centered] {\( L(\hat{\theta},t) {\ \propto \ } \phi(x) - \phi(t) \)};
    \node (optimize) [below of=L,right of=L, text centered] {Optimize $\hat{\theta}$};
    \node (new_theta) [below of=optimize,right of = start, text centered] {New \( \hat{\theta} \)};

    % Highlight and arrow for the start node
    \draw[->, very thick, red] (start) ++(-2,1.5) -- (start)
        node[midway, below, align=center, sloped, color=red] {Start}
        node[midway, above, align=center, sloped, color=red] {Random $\hat{\theta}$};
        
    % Arrows with multi-line labels
    \draw[->, bend left] (start) to node[midway, right, align=center] {} (L);
    \draw[->, bend left] (L) to node[midway, right, align=center] {} (optimize);
    \draw[->, bend left] (optimize) to node[midway, below, align=center] {} (new_theta);
    \draw[->, bend left] (new_theta) to node[midway, left, align=center] {} (start);

\end{tikzpicture}
    \caption{ Iterative approach to sound design. Synthesizer $g$ with arbitrary initialized parameters $\hat{\theta}$, creates sound $x$. The target sound $t$ is used as the goal. Parameters $\hat{\theta}$ are adjusted to minimize error (or loss) $L(\hat{\theta},t)$, where $L$ is proportional to the difference between the representations of $x$ and $t$. $\phi$ is the audio feature extractor, or representation function.}
    \label{fig:sound_design_loop_iterative}
\end{figure}

Genetic mutation has predominantly been the most common direct optimization method in previous works, particularly in non-differentiable settings. 
In a differentiable setting, gradient descent can be used to directly optimize the parameters of a synthesizer~\cite{vahidi2023mesostructures}. In the upcoming ``Past Works'' section, we will discuss the evolution of prior sound-matching research, with \textit{differentiable iterative sound-matching} being the latest extension of the field.


\begin{table*}[t]
\centering
\setlength{\tabcolsep}{3pt} % Reduce horizontal spacing
\renewcommand{\arraystretch}{1.05} % Reduce vertical spacing
\scriptsize % Reduce font size
\resizebox{\textwidth}{!}{
\begin{tabular}{|>{\raggedright\arraybackslash}m{2cm}|>{\raggedright\arraybackslash}m{2cm}|>{\raggedright\arraybackslash}m{2cm}|>{\raggedright\arraybackslash}m{2cm}|>{\raggedright\arraybackslash}m{1.2cm}|>{\raggedright\arraybackslash}m{1.2cm}|>{\raggedright\arraybackslash}m{1cm}|>{\raggedright\arraybackslash}m{1cm}|}
\hline
\textbf{Work} & \textbf{Synthesis} & \textbf{Loss} & \textbf{Heuristics} & \textbf{Goal} & \textbf{Domain} & \textbf{Year} & \textbf{Gen. } \\
\hline
Horner \textit{\textit{et al.}}~\cite{horner1993machine} & Non-diff FM & STFT (McAulay-Quatieri) & Direct & Replicate & In & 1991 & Iter \\
\hline
Mitchel \textit{\textit{et al.}}~\cite{mitchell2007evolutionary} & Non-diff FM & Spectral Relative & Evolutionary & Replicate & In (Contrived) & 2007 & Iter \\
\hline
Yee-King \textit{\textit{et al.}}~\cite{yee2018automatic} & Non-diff VST & P-Loss (for Supervised), Spectral (MFCC) & Supervised \& Evolutionary & Replicate & In & 2018 & Both \\
\hline
Esling \textit{\textit{et al.}}~\cite{esling2019flow} & Non-diff VST & Spec MSS/SC P-loss & Supervised \& Modeling & Replicate & In* \& Out & 2019 & 1-shot \\
\hline
Shier \textit{\textit{et al.}}~\cite{shier2020spiegelib} & Custom, non-diff & FFT \& Spectral & Supervised \& Direct & Replicate & In & 2020 & Both \\
\hline
Salimi \textit{\textit{et al.}}~\cite{salimi2020make} & Non-diff & STFT and Envelope & Supervised & Imitate & Out & 2020 & 1-shot \\
\hline
Masuda \textit{\textit{et al.}}~\cite{masuda2021soundmatch} & DDSP & P-Loss initially, fine-tune with Spec. & Supervised & Replicate? & In \& Out & 2021 & 1-shot \\
\hline
Han \textit{\textit{et al.}}~\cite{han2023perceptual} & NN-encoder $\rightarrow$ physical model & PNP (approx. of STFT) & Supervised & Replicate & In & 2023 & 1-shot \\
\hline
Vahidi \textit{\textit{et al.}}~\cite{vahidi2023mesostructures} & Differentiable FM & JTFS/MSS & Direct (grad desc.) & Replicate & In & 2023 & Iter \\
\hline
Uzrad \textit{\textit{et al.}}~\cite{uzrad2024diffmoog} & DDSP chains & Synth. Chain and P-loss & Supervised & Replicate & In \& Out & 2024 & 1-shot \\
\hline
\end{tabular}
}
\caption{Summary of select works in sound-matching. Gen. refers to whether the generations are done in 1-shot (generally, output by a neural network) or a search is conducted with the goal of iteratively getting closer to the target sound.}
\label{tab:summary}
\end{table*}

\section{Past Works}
\label{sec:related_works}
Table~\ref{tab:summary} contains an overview of relevant literature, covering important works in sound-matching.  Earlier works in this domain focused on FM synthesis and genetic algorithms (or direct optimization methods)\cite{justice1979analytic,horner1993machine,mitchell2007evolutionary}. Along with the exponential increase in computation power, later works utilized a wider range of approaches to parameter search, estimation, and audio synthesis; particularly with supervised learning and neural networks (\gls{NN})~\cite{yee2018automatic,engel2020ddsp,esling2019flow}. 

\subsection{Earlier Works}
FM synthesizers are simple to implement, yet very expressive~\cite{chowning1973synthesis}. Perhaps the earliest foundational work in sound-matching is the analytical approach by Justice~\cite{justice1979analytic} toward the decomposition and recreation of sounds with FM synthesis using one carrier, one modulator, and the corresponding envelopes:
\[ f(t) = I_c(t) \cos(\omega_c t + I_m(t) \cos(\omega_m t))
\]
Where $f(t)$ is the output signal, $I_c(t)$ is the carrier amplitude, $\omega_c$ is the carrier frequency, and $I_m(t)$ and $\omega_m$ are the envelope and frequency for the modulator~\cite{justice1979analytic}.  

 The sound-matching works that followed used other heuristics for finding the synthesizer parameters, yet the simple FM synthesizer structure remained unchanged. Horner \textit{et al.}~\cite{horner1993machine} used GAs for re-synthesis of sounds with an FM synthesizer using one modulator and one carrier oscillator, and the McAulay-Quatieri (MQ) method~\cite{mcaulay1986speech} for measuring loss. MQ calculates the STFT and marks the start and end of \textit{trends} in a sound with the assumption that the harmonic content belongs to trends that have a beginning and an end. 

Introduction of more complex models of synthesis such as wavetables~\cite{horner2003auto} and physical modeling~\cite{riionheimo2003parameter} rendered interesting results, but led to further questions about the nature and goals of sound-matching. Mitchell and Creasy noted the difficulty in distinguishing between \textit{inefficiency of the optimization engine} and \textit{synthesizer limitations} as the cause of failure or success in sound-matching~\cite{mitchell2007evolutionary}. They offered a \textit{contrived methodology} for finding the best evolutionary method for sound-matching using their FM synthesizer. The methodology postulates that the best search heuristic for in-domain search (where an exact target exists, and a wide range of targets can be produced by sampling from different points in the parameter space) would also be the best-performing heuristic for out-of-domain search on a dataset of muted trumpet tones~\cite{opolko1989mcgill}; however, testing was not extensive and yielded some contradictory results.

Testing sound-matching experiments can be difficult, in part due to the large number of parametric components and navigation of complex non-linear systems. The issue with finding the best approaches to sound-matching is exacerbated when slight changes to the pipeline render previous results moot. Perhaps due to these reasons, the contrived methodology was not extensively tested by Mitchell and Creasy, who only provided a single experiment where the best evolutionary strategy found by in-domain search would also perform well in out-of-domain synthesis. As they noted, modifications to the synthesizer, loss function, and sound domain would lead to a problem with an entirely new search space~\cite{mitchell2007evolutionary}. 

\subsection{Introduction of Deep-Learning}
Recent years have seen more works in sound-matching using machine learning techniques. In 2018, Yee-King \textit{et al.} rendered 60,000 audio-parameter pairs from the \textit{Dexed} \gls{VST} synthesizer~\footnote{https://asb2m10.github.io/dexed/}, and showed that NNs trained on this dataset can outperform GA and hill-climber (\gls{HC})~\cite{hoffmann2000heuristic} methods in rendering speed, with slight improvements in MFCC error (used as an objective performance test). The speed improvement appears trivial, considering the iterative nature of GAs when compared to offline training of supervised models. For GAs and HC optimizer, MFCCs were used as a measure of performance as well as a loss function. For training the networks, P-loss was used, since differentiable MFCCs were not possible in their pipeline. Importantly, informal hearing tests revealed that the performance of even the best NN model was unsatisfactory, possibly due to the complex nature of the synthesizer, which features 155 parameters.


% Programming synthesizers and querying sounds by imitating human vocals has also been a subject of interest~\cite{Cartwright2014SynthAssistQA,siamesevocalimitation2018}. For example, Cartwright and Pardo created a framework for searching pre-made patches for a synthesizer to best imitate vocals using audio features such as pitch, spectral centroid, and loudness~\cite{Cartwright2014SynthAssistQA}. 

Esling \textit{et al.} also noted the issue of using P-Loss in sound-matching~\cite{esling2019flow}, considering the non-linear mapping between parameters of a synth and the audio output. In their approach, dubbed as ``flowsynth'', they used a large parameter-to-audio dataset for a commercial \gls{VST} synthesizer. In flowsynth, audio is encoded to an auditory space, using a NN autoencoding approach~\cite{hinton2006reducing}. The encodings in the auditory space can then be converted to the parameter space, but only for that particular synthesizer. Using P-Loss and Spectral Convergence (Frobenius norm of the spectrograms) as metrics, flowsynth outperformed other NN approaches that did not use an auditory to parameter space conversion, but formal hearing tests were not conducted.

Masuda \textit{et al.} also applied supervised learning to synthesizer parameter estimation~\cite{masuda2021soundmatch}. Their work highlights the issue of non-linearity in parameter-to-synthesizer outputs and out-of-domain search. This work uses a differentiable subtractive synthesizer (two oscillators and an LP filter). A NN model was pre-trained using an in-domain dataset of randomly selected parameters and P-Loss. After training, the model was fine-tuned using 20,000 out-of-domain sounds from the NSynth dataset~\cite{engel2017neural} and multi-scale spectrogram loss~\cite{engel2020ddsp}. This approach proved more effective---that is, lower multi-level spectral difference in out-of-domain tests---than baseline models, which were either not exposed to out-of-domain sounds or trained exclusively with P-Loss. Subjective hearing tests were conducted, showing a preference for the fine-tuned model~\cite{masuda2021soundmatch}. 

Rather than focusing on a particular implementation, Shier \textit{et al.} presented Spieglib, a library for implementation of sound-matching pipelines (which Shier \textit{et al.} referred to as ``automatic parameter search'')~\cite{shier2020spiegelib}. This library provides extensible choices of deep-learning estimators with \textit{tensorflow}~\cite{abadi2016tensorflow}, genetic estimators with \textit{DEAP}~\cite{DEAP_JMLR2012}, feature extractors with \textit{librosa}~\cite{mcfee2015librosa}, and different choices for synthesizers, evaluators, dataloaders. Shier \textit{et al.} provide an example experiment using this library. This example has a setup similar to Yee-King \textit{et al.}~\cite{yee2018automatic}, where the \textit{Dexed} VST was used and different genetic and NN estimators were compared by using MFCC difference as the minimization objective. Depending on the NN architecture, MFCC or STFT spectrograms were used as the input. Unlike previous comparisons of NN and genetic algorithms, the best estimator was the multi-objective non-dominated sorting genetic algorithm III (NSGA-III). This algorithm had multiple minimization objectives: 13-band MFCC, STFT, and five spectral features. Shier \textit{et al.} report that all models performed relatively well at imitation of harmonic distributions, but the multi-objective function was the only optimizer which did \textit{not} struggle with matching temporal envelopes of the audio spectrum~\cite{shier2020spiegelib}.


Differentiable loss functions that use spectrogram differences can be computationally expensive. To mitigate this, Han \textit{et al.}~\cite{han2023perceptual} introduced ``perceptual-neural-physical loss'' (PNP). PNP is an approximation of spectrogram loss functions; specifically, a loss function that uses the L2 norm of the difference between the features of two sounds, or $||\phi(t) - \phi(x)||^2_2$. PNP loss functions are fast and differentiable, but require training and parameter estimation. A Riemannian metric M needs to be calculated for the minimization of locally linear approximation of the ``true" spectral loss function~\cite{han2023perceptual}. 


\[
\|\phi(t) - \phi(x)\|_2^2 = \langle \tilde{\theta} - \theta | \mathbf{M}(\theta) | \tilde{\theta} - \theta \rangle + O(\|\tilde{\theta} - \theta\|_2^3). \tag{4}
\]

During the training phase, this metric is calculated alongside the training of the neural parameter estimator. After training, PNP can be used as a fast approximation of the loss function, $\|\phi(t) - \phi(x)\|_2^2 $. Han \textit{et al.} observed slight improvements to run-time (for testing) and minor improvements to the loss values under certain conditions. These experiments were based on supervised learning and physical modeling (of a drum head) for synthesis~\cite{smith2010physical}, and compared PNP, P-Loss, MSS, and JTFS functions~\cite{han2023perceptual}. This approach was more recently applied to other supervised settings using physical and FM synthesizers~\cite{han2024learning}.

Uzrad \textit{et al.}~\cite{uzrad2024diffmoog} took another unique approach to sound-matching: using a differentiable \textit{synthesis chain} of DSP generators and effects and a loss function that combines P-Loss with a \textit{signal-chain loss}. Like other supervised learning replication approaches, during training a target sound $t$ is generated by random  $\theta$ parameter selection, and a model attempts to an output $\tilde{\theta}$ that generates a sound similar to $t$. The synthesizer is a customizable chain of effects, which feed one output as input to the next step of the chain; signal-chain loss compares the parameter and output difference at every output step in the chain~\cite{uzrad2024diffmoog}. Possible chain functionalities are FM/AM, Low Frequency Oscillators (\gls{LFO}), filters, and envelopes. Like the results shown by Masuda \textit{et al.}~\cite{masuda2021soundmatch}, better out-of-domain results were achieved when pre-trained on in-domain data and fine-tuned using out-of-domain NSynth data~\cite{engel2017neural}. 

Uzrad \textit{et al.}\cite{uzrad2024diffmoog} used log-spectral distance (LSD) as their loss function, where   
$\text{LSD} = \| \log(S(t)) - \log(S(x)) \|_F$, S is the mel-spectrogram of the sounds and F is the Frobenius norm~\cite{golub2013matrix}. The Frobenius norm of a matrix $A$ is defined as
$ \|A\|_F = \sqrt{\sum_{i,j} |a_{ij}|^2}$. Here, $A$ would be the 2D matrix resulting from subtracting two log-spectrograms. Along with other works, \textbf{non-convexity of loss surfaces when applying gradient optimization} is a noted issue~\cite{turian2020sorry,vahidi2023mesostructures,uzrad2024diffmoog}. 

\section{What is Lacking In the Field}
\label{sec:lacking}
  % First, we need to clearly define the characteristics we are looking for with non-generic loss functions, otherwise the chances of finding the sound(s) of interest is slim. Second, we need to move beyond direct replication, and focus on imitating target sounds that are outside the synthesizers capabilities. Third, more complex methods of synthesis need to be explored for iterative sound-matching. Fourth, we need to find optimization methods that can effectively navigate the the sinusoidal nature of loss function landscapes. 

Based on the literature reviewed, we find several gaps in past research that makes the application of sound-matching for sound-designers difficult. Thus far, replication of in-domain sounds has been the main subject of research. With in-domain replication, we already have the target sound \textit{and} the program that makes it. This simplifies the problem of measuring performance and is the reason why this type of sound-matching research is by far the most common. However, practical application of sound-matching requires answers to much harder problems. Sound-matching would be an interesting creative tool if we could use it to either:
\begin{enumerate}
 \item \textbf{Replicate out-of-domain sounds}: this would give us the  the parameters that could closely approximate a target sound, and we can modify the sound further by modulating the synthesizer. 
\item \textbf{Imitate in-domain or out-of-domain sounds}: This would allow for applying the characteristics of one sound to another. 
\end{enumerate}

We need to be aware of the ``open set recognition'' (\gls{OSR}) problem here \cite{mundt2019open,gers2000learning}.  OSR is a well documented issue in DL that manifests when the set or category that the network is supposed to reject or accept cannot be explained via examples. With imitation or replication of out-of-domain sounds, we can expect issues related to OSR to occur, since target and output sounds would be from different domains~\cite{salimi2021percussive}. 

Perhaps one method of addressing the OSR problem is the use of representations (or feature extraction) methods that reduce sounds to a feature vector in the same domain. For example, a representation that only looks at the envelope of sounds is far less likely to be subject to the OSR problem. \textcolor{highlight}{We highlight the lack of non-generic loss functions as a major weakness in previous works and an interesting area to explore}. 

Another weakness \textcolor{highlight}{is that the synthesis methods in past works are not representative of the common methods used by sound designers and modern synthesizers}. Simple FM/AM synthesizers have generally been the go-to methods of sound-matching. We believe that general statements regarding the effectiveness of an approach to iterative sound-matching (particularly in regards to a loss function) can only be reasonably made in the context of the subset of the tested methods of synthesis.
\textcolor{highlight}{The simplicity of parameter optimization approaches} is another area of weakness in past works. Classic genetic algorithms update the parameters randomly, while in recent differentiable settings, simple gradient descent methods are used. Past works have provided issues which arise in differentiable parameter estimation due to periodic loss functions. Perhaps the use of reinforcement learning (\gls{RL}), or other algorithms that continually learn from their environment, could help with this problem.

\subsection{Defining The Major Issues}
\label{sec:addressing_problems}
Based on the weaknesses noted above, we propose four problems that need to be addressed to improve the sound-matching field. 
\begin{enumerate}
    \item \LossSelect: Is there a \textit{best performing} loss function, or is the selection of the loss function dependent on factors such as the sound domain, synthesizer capability and the desired characteristics of the output? 
    \item \SynthSelect: There is a lack of diversity in the synthesis methods used in sound-matching. The use of different DSP functions for iterative sound-matching and their effects on the outcome or the interactions with the loss function have not been tested.
    \item \PeriodicLoss: The sinusoidal nature of loss function landscapes is a problem that appears frequently. In differentiable settings, this problem causes gradient descent updates to local minima. 
    \item \OutDomain: Sound-matching with out-of-domain sounds is an unexplored area, yet a necessary one for practical applications for sound designers.
\end{enumerate}

In Section~\ref{sec:what_is_done}, we discuss our work in simultaneously bridging the knowledge gap with regards to the \LossSelect~and \SynthSelect~problems. The latter two issues are left for future work; possible approaches are discussed in Sections~\ref{sec:summary_conclusion} and~\ref{sec:future}.


\section{Programs, Methodology, and Results}
\label{sec:what_is_done}
In this section we report the results of our efforts in addressing the first two problems. \textbf{We compare the performance of iterative sound-matching experiments, where four differentiable loss functions are applied to four differentiable synthesizers.} What we mean by performance is the similarity of the final output to the target, as measured by P-Loss, MSS, and manual listening.

\textbf{The hypothesis here is that the best performing loss function for sound-matching is influenced by the method of synthesis}. The results of these experiments show that there is no universally optimal loss function, strengthening the idea that the selection of the loss function and synthesis method is not an optimization problem, but a subjective choice depending on the needs of the sound designer. 

In Section~\ref{sec:methodology} we lay out the experiment methodology. Section~\ref{sec:loss_implementation} discusses the details of implementation for the loss functions. Section~\ref{sec:programs} covers the characteristics of the programs, and finally Section~\ref{sec:results_posthoc} shows that the effect of loss function choice on the success of iterative sound-matching is dependent on the signal process chain of interest. 

\subsection{Experiment Methodology}
\label{sec:methodology}
We have highlighted the lack of attention given to the selection of loss functions in sound-matching, and noted that the few existing works addressing this issue evaluate their proposed losses using only simplistic sound synthesis methods. \textcolor{highlight}{We hypothesize that the best performing loss function is influenced by the synthesis techniques utilized in the synthesizer}. This hypothesis targets the issues of \LossSelect~and \SynthSelect. For our experiments, we test four different loss functions and compare their success in sound-matching using four different synthesizers. We verify the success of the sound-matching process using two automatic measures of sound similarity (MSS and P-Loss) and a manual blinded hearing test conducted by two of the authors where likert scores~\cite{jebb2021review} of 1-5 are assigned to a subset of targets and outputs. 

An \textit{experiment} is one iteration of sound-matching, given a loss function, target sound,
and synthesizer program. When the final iteration is complete, a score is given (automatically with an algorithm or with manual listening) to an experiment that measures its success in replicating the target sound. Consider the following example: we have two loss functions, $L_1$ and $L_2$, and a single synthesizer program $P$ with a target parameter set $\theta^*$ where the sound output is $t$, or $P(\theta^*) = t$. We run a large number (in this case, 300 for each loss function) of experiments where we randomly set the initial ($\theta_0$) and target parameters ($\theta^*$) of $P$ and run an iterative sound-matching loop for each loss function, with max number of iterations set to 200. When the loop is terminated, we measure the similarity of the final output sound $x$ (where $P(\theta_{199}) = x$) to $t$, or if using P-Loss we compare $\theta_{199}$ to $\theta^*$. Since we ran 300 experiments for each loss, we can compare the bootstrapped distribution of losses or likert scores~\cite{jebb2021review} assigned by manual review to determine whether $L_1$ or $L_2$ led to better performance. This simplified example leads us to our methodolgy which has four loss functions, four synthesizer programs, and three different methods of scoring sound similarity.


\subsubsection{Determining Best Performers}
Determining the best performing loss function(s) might seem complicated since we have different synthesizer programs, loss functions, and evaluation metrics. Figure~\ref{fig:posthoc_evaluation} is a visualized summary of how the loss functions are ranked in three ways (MSS, P-Loss, Likert scores). 

We have four loss functions and four synthesizer programs. The maximum number of iterations set to 200, and 300 experiments for each loss/synthesizer combination. Each synthesizer program  is paired with four loss functions.  Under each of these loss functions, there are 300 experiments. Note that P-Loss and MSS are performance evaluation methods that measure similarity of final output and target sound; They are not one of the four loss functions. P-Loss and MSS are assigned automatically to each of the 300 pairs of target and output sounds. Blinded manual scoring using a Likert scale~\cite{jebb2021review} is assigned to 40 randomly selected sounds, where integers from 1 and 5 are assigned, with 1 meaning no similarity and 5 for near identical. 

For each program and loss function pair, we have three distributions that can be used to rank the loss functions. Two with 300 automatically assigned similarities and one with 80 likert scores. For consistency and statistical robustness, the distributions are upsampled to 1000 values using bootstrapping~\cite{tibshirani1993introduction}. Bootstrapping gives an estimation of the distribution for the mean performance of each experiment. $k$ (set to 1000) samples of $n$ values (set to 100\% of the values) are taken with replacement from the empirical distribution (list of 300 performance values for MSS and P-Loss or 80 for manual rankings). The mean performance is calculated for each of the $k$ samples. The $k$ estimates of the mean performance give us a bootstrapped distribution for each loss function. 

For each program, the loss functions are ranked using the non-parametric Scott-Knott (\gls{SNPK}) test~\cite{tantithamthavorn2017mvt,tantithamthavorn2018optimization}, which ranks the loss functions from rank 1 (best, or highest distribution values) to maximum rank of 4 (worst, with lowest distribution values). SNPK may determine that multiple functions belong to the same rank, in which case the maximum rank would be less than 4. Lower P-Loss and MSS values indicate higher similarity, therefore (for consistency with manually assigned ranks) the inverse of their output is used, where higher values indicate greater similarity. 

\input{extra_figures/evaluation_graph.tex}

\subsubsection{Training Parameters and Gradient Calculations}
 Previous work utilized gradient fields for analysis of the algorithm performance and gradient update~\cite{vahidi2023mesostructures}. Gradient fields are not the center of our analysis, but we include them in Appendix~\ref{appendix:gradient_fields}. During the experiments, the updates applied to the initial parameters are recorded for gradient field generation. For updating the weights, we use RMSProp, which operates similar to stochastic gradient descent (SGD)~\cite{goodfellow2016deep}, with the caveat that the gradients of each weight are scaled by the root-mean-square of past gradients of that weight. We used a fixed learning rate of 0.045 for all experiments. The maximum number of iterations is set to 200, where, based on our observations, the parameters have either irrecoverably diverged outside the acceptable ranges, or are stuck at a local minimum. At the maximum number of iterations the search is terminated and the P-Loss and MSS (as described in Section~\ref{sec:fourier_specs}) are calculated using the iteratively updated parameters, or the final sound output.

Additionally, the gradients are large, and calculated backwards sequentially through the signal processing chain. This backward calculation resembles the \textit{exploding gradients} problem in recurrent neural networks~\cite{gers2000learning}. For this reason, gradient clipping~\cite{goodfellow2016deep} is used as a preemptive measure to prevent large divergences in the parameter updates, but it is unclear if it facilitates learning in these experiments. The gradient clipping function~\cite{goodfellow2016deep} takes a set of gradients $\gamma_i$ and ensures that the $\ell_2$ norm of all gradients, or $||\gamma||_2:=\sqrt{\sum\limits_{i}(\gamma_{i}\cdot\gamma_{i})}$ does not exceed a clipping threshold of c (here set to 1). Clipped gradients $\gamma_i^{\text{clipped}}$ are calculated by multiplying them with a scale factor $\mathcal{S}$
\[
\mathbf{\gamma}_i^{\text{clipped}} = \mathbf{\gamma}_i \cdot \mathcal{S}
\]
Where $\mathcal{S}$ is a value less than or equal to 1, depending on whether the norm of the gradients exceeds the threshold $c$.  
\[
\mathcal{S} = \frac{c}{\max(\|\mathbf{\gamma}\|_2, c)}
\]


\subsection{Loss Function Implementation Details}
\label{sec:loss_implementation}
\subsubsection{STFT Losses}
Due to their ubiquity and lower cost of gradient calculation, we use STFT as the basis of two of the loss functions. As discussed in Section~\ref{sec:matching_types}, the L1 or L2 distances are the most common methods of comparing different spectrograms, however, there are situations where this might lead to shortcomings. An example given by Vahidi et  al.~\cite{vahidi2023mesostructures} is two chirplets (tones that increase in pitch) starting and ending at the same frequencies. If one of these tones is slightly shifted in time, then the spectrograms will no longer overlap at any point, despite their sonic similarity. In this example, we do not know if this is an issue of the spectrograms, or the L1 measure used for their comparison. In the computer vision field, Structural Similarity Index (SSIM)~\cite{wang2004imagesssim,wang2009mean} and Scale-Invariant Mean Squared Error (SIMSE)~\cite{barron2014shapessimse} have been used as alternatives that improved performance in various tasks relative to L1.

We define the \LoneSpec~and \SIMSESpec~functions as the application of L1 and SIMSE to the STFT spectrograms. The STFT spectrogram function uses 512 FFT bins, window size of 600 samples, and hop length (how many samples the window shifts) of 100 samples. The L1 and SIMSE implementations are differentiable. 

\subsubsection{JTFS Loss}
The \JTFS~loss function is the application of L1 difference to the JTFS representations of two sounds~\cite{vahidi2023mesostructures}. The code used for the JTFS transformation is the differentiable implementation of a 1-dimensional JTFS function provided by Andreux \textit{et al.}~\cite{kymatio}.

\subsubsection{Soft-DTW Loss}
We use the soft-DTW function, which is differentiable and --- depending on its parameters --- not shift-invariant~\cite{cuturi2017soft,janati2020spatio,tavenard.blog.softdtw}. The loss function \DTWEnv~is the application of the soft-DTW function to the envelope of the two sounds being compared~\cite{lyons1997understanding}. The envelope is calculated by creating the STFT spectrogram of a sound (the same process used in Section~\ref{sec:fourier_specs}) and summing the values at each timestep. 

\subsection{The Synthesizers}
\label{sec:programs}
The synthesizer programs are meant to be simple examples that test the building blocks of digital sound synthesis. Subtractive, additive, and FM/AM synthesis are three of the most common techniques in sound design~\cite{smith1991viewpoints}. In subtractive synthesis, frequencies are removed from a sound via digital filters. In additive synthesis, complex sounds are created via the linear combination of simpler sounds~\cite{lyons1997understanding,smith2007introduction}. As discussed previously, FM/AM synthesis refers to the general technique of modulating the frequency or amplitude of a waveform (the carrier) by another waveform (the modulator).

For each program, we provide the Faust code~\cite{orlarey2009faust}. Faust is a functional language for audio synthesis that can succinctly define signal processing chains. The publicly available Faust IDE\footnote{\url{https://faustide.grame.fr/}} is the simplest method for running the programs. A brief overview of each program is given in Table~\ref{tab:programs}. The programs are first defined in Faust, then converted (or \textit{transpiled}) to differentiable Jax functions~\cite{deepmind2020jax} using the DawDreamer library\footnote{\url{https://github.com/DBraun/DawDreamer}}. 

 Once the programs are transpiled to differentiable synthesizers, the iterative sound-matching procedure is:
 \begin{enumerate}
    \item Initialize $\theta^*$ and $\hat{\theta}$, i.e., random generation of target and initial parameters uniformly over a predefined range
    \item Generating the output of the synthesizer with $\hat{\theta}$ (the length of the output is set to 1 second with sample-rate of 44100 Hz) 
    \item Calculating the loss between the target and output
    \item Applying gradient updates to the synthesizer parameters
    \item Repeating the second step with the updated parameters $\hat{\theta}$, until maximum number of iterations has been reached
 \end{enumerate}

A general overview of the programs and the predefined parameter ranges is given in Table~\ref{tab:programs}. In the following subsections, we describe the programs and their parameters in more detail, followed by the methodology in Section~\ref{sec:methodology} and results in Section~\ref{sec:results_posthoc}.

\begin{table*}[htbp]
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|c|c|c|c|l|}
\hline
\textbf{ID} & \textbf{Parameters} & \makecell{\textbf{Param. 1}\\\textbf{Range}} & \makecell{\textbf{Param. 2}\\\textbf{Range}} & \textbf{Description} \\
\hline
\BPNoise & hp\_cut, lp\_cut & 50-1000 & 1-120 & Band-pass of Noise signal \\
\hline
\AddSineSaw & saw\_freq, sine\_freq & 20-1000 & 20-1000 & Addition of sine and saw waves \\
\hline
\AmpMod & amp, carrier & 1-20 & 20-1000 & LFO on amplitude of noise osc \\
\hline
\FMMod & amp, carrier & 0-5 & 0-4 & LFO on amplitude of saw osc \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Overview of Program Parameters and the range of values each parameter can take.}
\label{tab:programs}
\end{table*}

\subsubsection{\BPNoise}
\label{sec:program0}


\begin{lstlisting}[caption={\BPNoise}, label={lst:program0}, language=Faust,
                  float, floatplacement=htbp, xleftmargin=1em, xrightmargin=0.5em, firstnumber=0]
import("stdfaust.lib");
lp_cut = hslider("lp_cut",900,50,1000,1);
hp_cut = hslider("hp_cut",100,1,120,1);
process = no.noise:fi.lowpass(3,lp_cut):fi.highpass(10,hp_cut);
\end{lstlisting}

\BPNoise{} is a bare bones example of subtractive synthesis using digital filters~\cite{smith2007introduction}. Despite the ubiquity of subtractive synthesis in practical sound design, it has rarely been tested in differentiable sound-matching. In this program, a noise signal is fed through a band-pass (BP) filter. This removes the frequencies outside the low-pass (LP) and high-pass (HP) cutoffs. The noise generator produces all frequencies with random variation over time. The LP filter removes frequencies over its threshold frequency, and the HP removes frequencies lower than its threshold~\cite{smith2007introduction}. The search parameters are the cutoff thresholds for the LP and HP filters. Listing~\ref{lst:program0} shows the Faust code for \BPNoise.
\subsubsection{\AddSineSaw}
\label{sec:program1}

\begin{lstlisting}[caption={\AddSineSaw}, label={lst:program1},language=Faust,float,floatplacement=TBH,xleftmargin=1em,xrightmargin=0.5em,firstnumber=0]
import("stdfaust.lib");
saw_freq = hslider("saw_freq",800,20,1000,1);
sine_freq = hslider("sine_freq",300,20,1000,1);
sineOsc(f) = +(f/ma.SR) ~ ma.frac:*(2*ma.PI) : sin;
sawOsc(f) = +(f/ma.SR) ~ ma.frac;
process = sineOsc(sine_freq)+sawOsc(saw_freq);
\end{lstlisting}

\AddSineSaw{} is an additive program that combines a saw and a sine wave function. Like subtractive synthesis, additive synthesis is a common approach to sound design that has not been extensively tested as a benchmark in sound-matching. The search parameters here are the frequency of the sine and saw oscillators. Listing~\ref{lst:program1} is the Faust code for \AddSineSaw.

\subsubsection{\AmpMod}
\label{sec:program2}
\begin{lstlisting}[caption={\AmpMod}, label={lst:program2},language=Faust,float,floatplacement=TBH,xleftmargin=1em,xrightmargin=0.5em,firstnumber=0]
import("stdfaust.lib");
amp = hslider("amp",0.5,0,5,0.01);
modulator = hslider("modulator",0.5,0,4,0.01);
sineOsc(f) = +(f/ma.SR) ~ ma.frac:*(2*ma.PI) : sin;
process = no.noise*sineOsc(modulator)*amp;
\end{lstlisting}

\AmpMod{} involves amplitude modulation of a noise generator by an LFO with \textit{modulator} as its frequency and a global \textit{amp} value that does not change over time and applies to the entire signal. The search parameters for this program are the LFO frequency and the amp value. The amp value acts as a global volume control, and since the sounds are normalized before analysis, it is likely inconsequential to the performance. This program is meant to be a stepping stone to \FMMod, which utilizes proper AM synthesis. Listing~\ref{lst:program2} is the Faust code for \AmpMod. 

\subsubsection{\FMMod}

\label{sec:program3}
\begin{lstlisting}[caption={\FMMod}, label={lst:program3},language=Faust,float,floatplacement=TBH,xleftmargin=1em,xrightmargin=0.5em,firstnumber=0]
import("stdfaust.lib");
carrier = hslider("carrier",100,20,1000,1);
amp = hslider("amp",6,1,20,1);
sineOsc(f) = +(f/ma.SR) ~ ma.frac:*(2*ma.PI) : sin;
sawOsc(f) = +(f/ma.SR) ~ ma.frac;
process = sineOsc(amp)*sawOsc(carrier);
\end{lstlisting}
Relative to additive and subtractive synthesis, AM/FM synthesis are less common methods of sound design. Due to their frequent use in previous works, we also tested an AM synthesizer. The synthesizer program is the multiplication of a low frequency sine oscillator with frequency parameter \textit{amp}, and a saw oscillator with frequency parameter \textit{carrier}. Listing~\ref{lst:program3} is the Faust code for \FMMod. 



\subsection{Post hoc Analysis of Loss Performance}
\label{sec:results_posthoc}
As summarized in Figure~\ref{fig:posthoc_evaluation}, for each program and scoring method, there are four sets of empirical results (a set of 1000 bootstrapped values for each loss function). A post-hoc test is conducted in two stages in order to determine whether there is a difference in performance between loss functions for each program, and if so, which are the best performers. The first stage determines whether there is a difference in group means, with the null hypothesis being that all groups have similar mean ranks. If the null hypothesis is rejected, the second stage ranks the loss functions from best to worst using the non-parametric Scott-Knott algorithm (\gls{SNPK})~\cite{tantithamthavorn2017mvt,tantithamthavorn2018optimization}.

The Kruskal-Wallis test is used for the first stage~\cite{kruskal1952use}. This test works by pooling and ranking all performance evaluation measures for a program (either P-loss or MSS), then testing whether the differences in mean rank for all loss function groups is zero. The Kruskal-Wallis test calculates an H statistic that is compared to a chi-square distribution with k-1 degrees of freedom (k is the number of groups, or 4, and degrees of freedom is 3). Using the significance level of 0.05, if the H statistic is greater than the critical value of the chi-square distribution then the null hypothesis is rejected, meaning that at least one group has a mean rank significantly above others~\cite{kruskal1952use}. Table~\ref{tab:kruskal_auto} shows the Kruskal-Wallis results for every program and automatic evaluation method, and shows that significant differences exist in all cases except MSS for the \AmpMod{} program. The manual ranking results showed significant differences in all cases, as shown in table~\ref{tab:kruskal_manual}.

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Program} & \textbf{Eval. Method} & \textbf{H-Stat.} & \textbf{P-value} & \textbf{Reject} \\
\hline
\BPNoise & MSS      & 81.19  & $1.70 \times 10^{-17}$ & Yes \\
\BPNoise & P-Loss  & 163.60 & $3.07 \times 10^{-35}$ & Yes \\
\AddSineSaw & MSS      & 308.97 & $1.14 \times 10^{-66}$ & Yes \\
\AddSineSaw & P-Loss  & 348.42 & $3.28 \times 10^{-75}$ & Yes \\
\AmpMod & MSS      & 1.35   & $0.7171$               & No \\
\AmpMod & P-Loss  & 366.76 & $3.50 \times 10^{-79}$ & Yes \\
\FMMod & MSS      & 564.65 & $4.65 \times 10^{-122}$ & Yes \\
\FMMod & P-Loss  & 229.19 & $2.07 \times 10^{-49}$ & Yes \\
\hline
\end{tabular}
\caption{Kruskal-Wallis results by program and evaluation method. The null hypothesis is that no difference in performance is observed between loss functions; it is rejected for all programs and evaluation methods with the exception of MSS combined with \AmpMod.}
\label{tab:kruskal_auto}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Program} & \textbf{H-Stat.} & \textbf{P-value} & \textbf{Reject} \\
\hline
\BPNoise & 9.45  & $2.39 \times 10^{-2}$ & Yes \\
\AddSineSaw & 9.45  & $2.39 \times 10^{-2}$ & Yes \\
\AmpMod & 32.71 & $3.70 \times 10^{-7}$ & Yes \\
\FMMod & 207.58 & $9.69 \times 10^{-45}$ & Yes \\
\hline
\end{tabular}
\caption{Manual ranking Kruskal-Wallis results. Significant differences are observed for all programs.}
\label{tab:kruskal_manual}
\end{table}




\subsubsection{Best Performers For Each Program}
Based on the bootstrapped distribution of scores, the SNPK algorithm ranks the loss functions from 1 (best) to a maximum of 4 (worst). In cases where the distributions are similar, multiple loss functions can be clustered into the same rank. 

We use color-coded violin plots to visualize the best performers for each program. In the violin plots, the corresponding colors for each rank are
\colorbox{rank1}{\textcolor{black}{\textbf{1}}} \colorbox{rank2}{\textcolor{white}{\textbf{2}}} \colorbox{rank3}{\textcolor{white}{\textbf{3}}} \colorbox{rank4}{\textcolor{black}{\textcolor{white}{\textbf{4}}}}

\subsubsection{\BPNoise}
For this synthesizer program, the spectrogram-based models performed the best. This makes intuitive sense, as the visual effects of a band-pass filter on white noise are readily apparent in a spectrogram. As shown in Figure~\ref{fig:npsk_p0}, manual hearing test and MSS selected \SIMSESpec~as the best performer and \LoneSpec~as the second-best performer, while P-loss gave the reverse order. 

\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.03\textwidth}
        \footnotesize
        \raggedleft
        \vspace{-.2cm}
        SIMSE\\[0.6cm]
        L1\\[0.65cm]
        JTFS\\[0.65cm]
        DTW
    \end{minipage}%
    \begin{minipage}{0.95\textwidth}
        \centering
        \subfloat[Likert Scores]{
            \includegraphics[width=0.3\textwidth]{images/npsk_likert_0.png}
            \label{fig:p0_hearing}
        }\hspace{0.005\textwidth} % Reduced space between figures
        \subfloat[Inverse P-Loss]{
            \includegraphics[width=0.3\textwidth]{images/npsk_P-Loss_0.png}
            \label{fig:p0_P-Loss}
        }\hspace{0.005\textwidth} % Reduced space between figures
        \subfloat[Inverse MSS]{
            \includegraphics[width=0.3\textwidth]{images/npsk_MSS_0.png}
            \label{fig:p0_MSS}
        }
    \end{minipage}
    \vspace{-0.5em}
    \caption{\BPNoise~bootstrapped distributions and ranks given by NPSK. Ranks and their corresponding colors from best to worst are \colorbox{rank1}{\textcolor{black}{\textbf{1}}}, \colorbox{rank2}{\textcolor{white}{\textbf{2}}}, \colorbox{rank3}{\textcolor{white}{\textbf{3}}}, \colorbox{rank4}{\textcolor{white}{\textbf{4}}}.}
    \label{fig:npsk_p0}
\end{figure*}
\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.03\textwidth}
        \footnotesize
        \raggedleft
        \vspace{-.2cm}
        SIMSE\\[0.6cm]
        L1\\[0.65cm]
        JTFS\\[0.65cm]
        DTW
    \end{minipage}%
    \begin{minipage}{0.95\textwidth}
        \centering
        \subfloat[Likert Scores]{
            \includegraphics[width=0.3\textwidth]{images/npsk_likert_1.png}
            \label{fig:p1_hearing}
        }\hspace{0.005\textwidth} % Reduced space between figures
        \subfloat[Inverse P-Loss]{
            \includegraphics[width=0.3\textwidth]{images/npsk_P-Loss_1.png}
            \label{fig:p1_P-Loss}
        }\hspace{0.005\textwidth} % Reduced space between figures
        \subfloat[Inverse MSS]{
            \includegraphics[width=0.3\textwidth]{images/npsk_MSS_1.png}
            \label{fig:p1_MSS}
        }
    \end{minipage}
    \vspace{-0.5em}
    \caption{\AddSineSaw~bootstrapped distributions and ranks given by NPSK.}
    \label{fig:npsk_p1}
\end{figure*}
\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.03\textwidth}
        \footnotesize
        \raggedleft
        \vspace{-.2cm}
        SIMSE\\[0.6cm]
        L1\\[0.65cm]
        JTFS\\[0.65cm]
        DTW
    \end{minipage}%
    \begin{minipage}{0.95\textwidth}
        \centering
        \subfloat[Likert Scores]{
            \includegraphics[width=0.3\textwidth]{images/npsk_likert_2.png}
            \label{fig:p2_hearing}
        }\hspace{0.005\textwidth} % Reduced space between figures
        \subfloat[Inverse P-Loss]{
            \includegraphics[width=0.3\textwidth]{images/npsk_P-Loss_2.png}
            \label{fig:p2_P-Loss}
        }\hspace{0.005\textwidth} % Reduced space between figures
        \subfloat[Inverse MSS]{
            \includegraphics[width=0.3\textwidth]{images/npsk_MSS_2.png}
            \label{fig:p2_MSS}
        }
    \end{minipage}
    \vspace{-0.5em}
    \caption{\AmpMod~bootstrapped distributions and ranks given by NPSK.}
    \label{fig:npsk_p2}
\end{figure*}
\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.03\textwidth}
        \footnotesize
        \raggedleft
        \vspace{-.2cm}
        SIMSE\\[0.6cm]
        L1\\[0.65cm]
        JTFS\\[0.65cm]
        DTW
    \end{minipage}%
    \begin{minipage}{0.95\textwidth}
        \centering
        \subfloat[Likert Scores]{
            \includegraphics[width=0.3\textwidth]{images/npsk_likert_3.png}
            \label{fig:p3_hearing}
        }\hspace{0.005\textwidth} % Reduced space between figures
        \subfloat[Inverse P-Loss]{
            \includegraphics[width=0.3\textwidth]{images/npsk_P-Loss_3.png}
            \label{fig:p3_P-Loss}
        }\hspace{0.005\textwidth} % Reduced space between figures
        \subfloat[Inverse MSS]{
            \includegraphics[width=0.3\textwidth]{images/npsk_MSS_3.png}
            \label{fig:p3_MSS}
        }
    \end{minipage}
    \vspace{-0.5em}
    \caption{\FMMod~bootstrapped distributions and ranks given by NPSK.}
    \label{fig:npsk_p3}
\end{figure*}

\subsubsection{\AddSineSaw}
As shown in Figure~\ref{fig:npsk_p1}, JTFS was the best performing loss function in all evaluation methods. Moreover, all evaluation methods produced identical results. 

\subsubsection{\AmpMod}
IP-Loss and manual hearing tests yield identical rankings, and MSS results vary for for ranks 2 to 4.  It is not surprising that MSS results differ from the hearing tests, as we saw in Table~\ref{tab:kruskal_auto}, MSS showed no significant differences between groups. As shown in Figure~\ref{fig:npsk_p2}, all models selected \DTWEnv~as the best performer. This may be due to \DTWEnv's focus on periodic changes in loudness.

\subsubsection{\FMMod}
\DTWEnv~is again the best performer here. As shown in Figure~\ref{fig:npsk_p3}, all methods of analysis gave identical rankings to all programs.



\subsection{Consistency In Rankings}

As we've discussed, automatic sound evaluation methods are unreliable, as we do not know how they would align with the often subjective opinion of human listeners. Here, the manual blinded rankings given by the reviewers could be treated as a ground truth, particularly since the two reviewers gave very similar rankings to the experiment results.

To measure the agreement between the two listeners, Spearman's rank correlation is used. The Spearman test returns a p-value for the null hypothesis of no correlation, along with a $\rho$ value between -1 to 1; -1 for perfect negative correlation, 1 for perfect correlation, and 0 for no correlation~\cite{spearman1987proof,rebekic2015pearson}. For each program and loss function, the blind manual hearing test gives us 80 ranks, which are given by two of the authors to a set of 40 randomly selected experiment results. This gives 160 data points per program, and 640 data points for all four programs. We computed Spearman correlations between the two reviewers. Across all programs, the correlation was very strong ($\rho = 0.86$, $p < 10^{-180}$). Per-program correlations were similarly strong and statistically significant: $\rho = 0.71$ for \BPNoise{} ($p < 10^{-25}$), $\rho = 0.64$ for \AddSineSaw{} ($p < 10^{-19}$), $\rho = 0.84$ for \AmpMod{} ($p < 10^{-43}$), and $\rho = 0.85$ for \FMMod{} ($p < 10^{-44}$). 

Tables~\ref{tab:p0},~\ref{tab:p1},~\ref{tab:p2}, and~\ref{tab:p3} show the rankings for each program, with ranks different from the corresponding hearing test results marked with an asterisk. Both MSS and P-Loss gave a different rank from the hearing results in 3 out of 16 cases, which---somewhat surprisingly---shows consistency between automatic and manual hearing tests, at least for the simple programs used in this work. We also observe that identical top ranks were consistent across performance evaluation methodologies, with the exception of P-Loss in \BPNoise, which narrowly picks a different spectrogram-based loss function.
\input{extra_figures/ranking_tables}
\subsection{Takeaways}
Perhaps the most important takeaway from our analysis is that the loss function that yields the best sound-matching outcomes varies depending on the synthesizer program. This is perhaps due to the interaction between how the parameters of the synthesizer influence the sound, and the core sonic features of the loss function.

We also see that the novel \SIMSESpec{} and \DTWEnv{} loss functions (to our knowledge, not previously used in iterative sound-matching), lead to the best results in three of the four programs. This shows room for further creative approaches to differentiable sound-similarity measures, particularly since we observed that SOTA loss functions can be ineffective depending on the design of a synthesizer. 

Finally, we surprisingly see somewhat consistent results between the rankings assigned by manual hearing tests (which we take as the ground truth), and automatic measures of P-Loss and MSS. Due to the simplicity of our programs and the occasional deviations, we cannot conclude that these automatic measures are substitutes for actual hearing.


\section{Summary and Conclusion}
\label{sec:summary_conclusion}
 Sound-matching is an umbrella term for the algorithmic programming of audio synthesizers, often with the goal of assisting sound designers. Here we provided a history of sound-matching, and proposed ``differentiable iterative sound-matching'' as a natural extension of previous works and recent advances.
 
We exposed several areas of weakness in the field, for which we presented methods for bridging the knowledge gaps. These issues included loss and synthesizer selection, periodic loss landscapes, and verification of performance measures with in-domain and out-of-domain sounds. The first two issues being the main target of the experiments in this work. 

Using simple synthesizers, small sound domains, and a limited number of loss functions, previous works have been able to claim some loss functions or feature extractors for audio as ``better performing" for sound-matching (or other audio-related tasks)\cite{vahidi2023mesostructures,turian2020sorry,masuda2021soundmatch,engel2020ddsp,han2023perceptual}. Is the search for a universally best-performing loss function a worthy endeavor, or perhaps the choice of loss function is domain dependent?

The main hypothesis tested here is whether the performance of differentiable loss functions is influenced by the synthesis techniques used for sound-matching. We created iterative sound-matching experiments by combining four different loss functions with four different sound synthesis programs. We ranked the performance of the iterative sound-matching pipelines for every loss function and program, and observed that the success of the pipeline (that is, how closely the output sound matches the target sound) is program dependent. In other words, different synthesizer programs work best with different loss functions. 

 We considered P-Loss and MSS as performance measures, commonly used in other sound-matching works, and compared their rankings to those given by blind hearing tests. We observed that automatic performance measures and manual listening tests were often (though not always) in agreement. Manual verification of sound-matching results remains a necessity for future works.  

\section{Weaknesses}
Compared to previous work, we presented a relatively more cohesive approach to iterative sound-matching which utilizes and compares four loss functions and four synthesis methods. However, there are many other untested methods of synthesis that can be combined in practically infinite ways. 

To limit the scope of our experiments, we had no choice but to use arbitrary parameters for the various signal processing functions. We used bare-bones versions of STFT and JTFS with fixed parameters. We did not test complex synthesizers, which may use different signal generation functions in parallel or sequentially. Arbitrary hyperparameters such as learning rate and max number of iterations were selected for the DL pipeline, and only the RMSProp optimizer was tested. 

In Section~\ref{sec:in-domain}, we discussed the importance of sound domains. This work, much like previous work, only utilizes in-domain sounds. That is, the target sound is made by the synthesizer, and the parameters are already known. This simplifies the issues of measuring sound-similarity, but it is not a realistic scenario for practical sound-matching. We plan to address this issue in future works, as we will discuss in the next section.

\section{Future Work}
\label{sec:future}
The analysis of different loss functions sound-matching presented here shows that problem of periodic loss-landscapes is not limited to spectral representations using Fourier transforms~\cite{turian2020sorry,vahidi2023mesostructures,uzrad2024diffmoog,bruford2024synthesizer}. We can postulate that this problem emerges due to the periodic nature of sound. The current trend in contemporary work appears to be the application of domain specific and computationally expensive loss functions~\cite{han2023perceptual,uzrad2024diffmoog}, use of large neural networks~\cite{hershey2017cnn,cramer2019look}, or complex ensemble methods~\cite{turian2022hear}. Such models are useful, but often intractable; furthermore, training such models still requires the definition of simpler loss functions, which emphasizes the need for further work on differentiable and expressive loss function implementations. 

With regard to navigation of loss function landscapes, an optimizer that is more aware of fluctuations in gradients would perhaps lead to better solutions than simple gradient descent. Viewing the sound-matching problem as ``the navigation of an agent from an arbitrary point in a gradient field to a target'' closely resembles many classical problems in the field of reinforcement learning (RL)~\cite{sutton2018reinforcement}. Naturally, we can extend our work by applying RL and other heuristic search techniques to the problem of sound-matching, and measuring the performance changes (both quantitative and qualitative) relative to simple gradient descent updates. 

Like nearly all previous work in sound-matching, the main measure of success here was the accurate \textit{replication} of sounds (or synth parameters) rather than imitation of sounds outside of the synthesizer's domain. Replication of arbitrary features of sounds is an important component of sound-design, though it is much harder to define or measure. Future works could explore loss functions which only measure certain characteristics of sound (such as \DTWEnv), and whether they can pave the way for better imitation sound-matching.




\bibliographystyle{IEEEtran}
\bibliography{references}
\end{document}


