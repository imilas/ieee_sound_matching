\documentclass[lettersize,journal]{IEEEtran}
% ========================
% latexdiff underline/strike setup
% ========================
\usepackage[normalem]{ulem} % for \sout
\usepackage{xcolor}         % for colors

% Additions = blue underline
\renewcommand{\DIFadd}[1]{\textcolor{blue}{\uline{#1}}}

% Deletions = red strikeout
\renewcommand{\DIFdel}[1]{\textcolor{red}{\sout{#1}}}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL main.tex           Sun Sep 28 21:12:36 2025
%DIF ADD revised_main.tex   Sun Sep 28 21:12:36 2025
\usepackage{amsmath,amsfonts}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{csquotes}
\usepackage{makecell}
% my added packages and commands =======================
\usepackage[numbers]{natbib} %removed cite package
%DIF 18a18
 %DIF > 
%DIF -------
\usepackage{tikz}
\usetikzlibrary{positioning,shapes.geometric}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{caption}         

\newcommand{\highlight}[1]{\textcolor[RGB]{00,100,100}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}

%DIF 27a28-31
 %DIF > 
\newcommand{\xtheta}{x_{\theta}} % synthesizer output %DIF > 
\newcommand{\xzero}{x_{0}}       % target sound %DIF > 
 %DIF > 
%DIF -------
\captionsetup[lstlisting]{justification=centering, singlelinecheck=false}
\providecommand{\gls}[1]{#1}
\definecolor{rank1}{HTML}{70FF70}
\definecolor{rank2}{HTML}{858585}
\definecolor{rank3}{HTML}{454545}
\definecolor{rank4}{HTML}{000000}
\newcommand{\SIMSESpec}{\texttt{SIMSE\_Spec}}
\newcommand{\LoneSpec}{\texttt{L1\_Spec}}
\newcommand{\JTFS}{\texttt{JTFS}}
\newcommand{\DTWEnv}{\texttt{DTW\_Envelope}}
\newcommand{\LossSelect}{\textbf{Loss Selection}}
\newcommand{\SynthSelect}{\textbf{Synthesis Selection}}
%DIF 39c44
%DIF < \newcommand{\PeriodicLoss}{\textbf{Periodic Loss}}
%DIF -------
\newcommand{\PeriodicLoss}{\textbf{Loss Landscape Navigation}} %DIF > 
%DIF -------
\newcommand{\OutDomain}{\textbf{Out-Domain Generation}}
\newcommand{\BPNoise}{\textbf{BP-Noise}}  
\newcommand{\AddSineSaw}{\textbf{Add-SineSaw}}  
\newcommand{\AmpMod}{\textbf{Noise-AM}}  
\newcommand{\FMMod}{\textbf{SineSaw-AM}}  

% code_style.tex
\usepackage{xcolor}
\usepackage{tikz}

\newcommand{\greencircle}{%
    \begin{tikzpicture}
        \fill[green] (0,0) circle (4pt); % Adjust the size here if needed
    \end{tikzpicture}%
}

\newcommand{\greenstar}{%
    \begin{tikzpicture}[scale=0.22]
        \fill[green] 
            (0,1) 
            -- (0.2245,0.309) 
            -- (1,0.309) 
            -- (0.3633,-0.118) 
            -- (0.5878,-0.809) 
            -- (0,-0.382) 
            -- (-0.5878,-0.809) 
            -- (-0.3633,-0.118) 
            -- (-1,0.309) 
            -- (-0.2245,0.309) 
            -- cycle;
    \end{tikzpicture}%
}

\newcommand{\redcircles}{
    \begin{tikzpicture}[scale=0.75, line cap=round, line join=round]
        % Define coordinates for circles
        \coordinate (A) at (0,0);
        \coordinate (B) at (1,0);
        \coordinate (C) at (2,0);
        \coordinate (D) at (3,0);
        
        % Draw circles
        \foreach \point in {A,B} 
            \fill[red] (\point) circle (2pt);
        
        % Connect circles with lines
        \draw[red] (A) -- (B);
    \end{tikzpicture}%
}

\lstdefinelanguage{Faust}{
    morekeywords={import, process, environment, declare, with, if, else, while, for, int, float, true, false},
    sensitive=true,
    morecomment=[l]{//}, % Line comment
    morecomment=[s]{/*}{*/}, % Block comment
    morestring=[b]", % Strings
}

% Customize the appearance of the code
\lstset{
    language=Faust,
    backgroundcolor=\color{lightgray!20},
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    stringstyle=\color{orange},
    commentstyle=\color{green}\itshape,
    showstringspaces=false,
    numbers=left,
    numberstyle=\tiny,
    frame=single,
    breaklines=true
}

% Suppress the "Listing #" label and caption text
% \captionsetup[lstlisting]{
%     labelformat=empty,
%     labelsep=none,
%     font=normalfont,
%     skip=0pt
% }


%  ================================
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFadd}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{{\protect\color{red}\sout{#1}}} %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
\providecommand{\DIFmodbegin}{} %DIF PREAMBLE
\providecommand{\DIFmodend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
\providecommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\providecommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\providecommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
\begin{picture}(1,1)% %DIF PREAMBLE
\thicklines\linethickness{2pt} %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
\end{picture}% %DIF PREAMBLE
}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF AMSMATHULEM PREAMBLE %DIF PREAMBLE
\makeatletter %DIF PREAMBLE
\let\sout@orig\sout %DIF PREAMBLE
\renewcommand{\sout}[1]{\ifmmode\text{\sout@orig{\ensuremath{#1}}}\else\sout@orig{#1}\fi} %DIF PREAMBLE
\makeatother %DIF PREAMBLE
%DIF COLORLISTINGS PREAMBLE %DIF PREAMBLE
\RequirePackage{listings} %DIF PREAMBLE
\RequirePackage{color} %DIF PREAMBLE
\lstdefinelanguage{DIFcode}{ %DIF PREAMBLE
%DIF DIFCODE_UNDERLINE %DIF PREAMBLE
  moredelim=[il][\color{red}\sout]{\%DIF\ <\ }, %DIF PREAMBLE
  moredelim=[il][\color{blue}\uwave]{\%DIF\ >\ } %DIF PREAMBLE
} %DIF PREAMBLE
\lstdefinestyle{DIFverbatimstyle}{ %DIF PREAMBLE
	language=DIFcode, %DIF PREAMBLE
	basicstyle=\ttfamily, %DIF PREAMBLE
	columns=fullflexible, %DIF PREAMBLE
	keepspaces=true %DIF PREAMBLE
} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim}[1][]{\lstset{style=DIFverbatimstyle}}{} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim*}[1][]{\lstset{style=DIFverbatimstyle,showspaces=true}}{} %DIF PREAMBLE
\lstset{extendedchars=\true,inputencoding=utf8}

%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}
\DIFaddbegin \bibliographystyle{IEEEtran}


\DIFaddend \title{Evaluating Sound Similarity Metrics for Differentiable, Iterative Sound-Matching}

\author{Amir Salimi, Abram Hindle, Osmar R. Za{\"i}ane}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  \markboth{IEEE TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL -, 2025}%
%DIF <  {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  \markboth{%
%DIF <    \parbox{\textwidth}{%
%DIF <      This work has been submitted to the IEEE for possible publication. \\
%DIF <      Copyright may be transferred without notice, after which this version may no longer be accessible.%
%DIF <    }%
%DIF <  }{}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \maketitle

 
\begin{abstract}
Manual sound design with a synthesizer is inherently iterative: an artist compares the synthesized output to a mental target, adjusts parameters, and repeats until satisfied. Iterative sound-matching automates this workflow by continually programming a synthesizer under the guidance of a loss function (or similarity measure) towards a target sound. Prior comparisons of loss functions have typically favored one metric over another, but only within narrow settings: limited synthesis methods, few loss types, often without blind listening tests. This leaves open the question of whether a universally optimal loss exists, or the choice of loss remains a creative decision conditioned on the synthesis method and the sound designer's preference. We propose differentiable iterative sound-matching as the natural extension of the available literature, since it combines the manual approach to sound design with modern advances in machine learning. To analyze the variability of loss function performance across synthesizers, we implemented a mix of four novel and established differentiable loss functions, and paired them with differentiable subtractive, additive, and AM synthesizers. For each of the sixteen synthesizer–loss combinations, we ran 300 randomized sound-matching trials. Performance was measured using parameter differences, spectrogram-distance metrics, and manually assigned listening scores. We observed a moderate level of consistency among the three performance measures. Our post-hoc analysis shows that the loss function performance is highly dependent on the synthesizer. These findings underscore the value of expanding the scope of sound-matching experiments, and developing new similarity metrics tailored to specific synthesis techniques, rather than pursuing one-size-fits-all solutions.
\end{abstract}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  the automatic performance measures have not been compared to human hearing
%DIF <  lack of creativity in loss function definitions 
\DIFdelend \begin{IEEEkeywords}
Audio synthesis, differentiable digital signal processing, music information retrieval, sound-matching
\end{IEEEkeywords}
%DIF >  \hphantom{\vphantom{\begin{minipage}{\textwidth}\bibliography{references}\end{minipage}}}


% need to simplify the intro, and expand the hypothesis
\section{Introduction}
\DIFdelbegin \DIFdel{Since the 1970s, artists such as Suzanne Ciani~\mbox{%DIFAUXCMD
\cite{ciani_life_in_waves} }\hskip0pt%DIFAUXCMD
have explored the use of audio synthesizers to replicate natural sounds—such as a bottle cap popping off—in ways that preserve the essential qualities of the original while showcasing the unique characteristics of the synthesizer~\mbox{%DIFAUXCMD
\cite{creativecherep2024}}\hskip0pt%DIFAUXCMD
. Rather than merely replicating sounds, this approach uses synthesizers for artistic reinterpretation. However, the automation of this interpretive approach to sound design using digital audio synthesizers has rarely been explored, and significant knowledge gaps remain even in the simpler problem of replicating sounds. One such gap is the lack of diversity and depth in the analysis of sound-similarity measures and their interaction with various synthesis methods. As a stepping stone toward automated interpretive sound design, we use automatic and manual sound-evaluations to investigate whether the optimal choice of a sound-similarity measure is dependent on the synthesis method.
}\DIFdelend 

%DIF >  Since the 1970s, artists such as Suzanne Ciani~\cite{ciani_life_in_waves} have explored the use of audio synthesizers to replicate natural sounds—such as a bottle cap popping off—in ways that preserve the essential qualities of the original while showcasing the unique characteristics of the synthesizer~\cite{creativecherep2024}. Rather than merely replicating sounds, this approach uses synthesizers for artistic reinterpretation. However, the automation of this interpretive approach to sound design using digital audio synthesizers remains impractical for sound-designers, and significant knowledge gaps remain even in the simpler problem of replicating sounds. One such gap is the lack of diversity and depth in the analysis of sound-similarity measures and their interaction with various synthesis methods. As a stepping stone toward automated interpretive sound design, we use automatic and manual sound-evaluations to investigate whether the optimal choice of a sound-similarity measure is dependent on the synthesis method.
\DIFaddbegin 

\DIFaddend A digital audio synthesizer is any software used for the creation and manipulation of audio. \DIFdelbegin \DIFdel{Since the advent of digital signal processing (\gls{DSP}) in the 1960s~\mbox{%DIFAUXCMD
\cite{stranneby2004digital}}\hskip0pt%DIFAUXCMD
, a wide variety of parametric audio generation functions have been proposed~\mbox{%DIFAUXCMD
\cite{lyons1997understanding,russ1999sound,shier2020spiegelib}}\hskip0pt%DIFAUXCMD
. Manual sound design with a synthesizer is an iterative process of trial and error; when the artist has a desired (or target) sound in mind, sound design involves listening to the output, determining whether the output is closer or farther from the target, and adjusting the parameters of the synthesizer accordingly until the target is reached~\mbox{%DIFAUXCMD
\cite{russ1999sound}}\hskip0pt%DIFAUXCMD
. This paper explores the }\DIFdelend \DIFaddbegin \DIFadd{The possibilities afforded by these synthesizers are infinite, leading to their widespread adaption by artists and sound-designers~\mbox{%DIFAUXCMD
\cite{lyons1997understanding,russ1999sound,stranneby2004digital}}\hskip0pt%DIFAUXCMD
. A typical synthesizer has a number of parameterizable functions which affect the sound output in various ways, and manual sound-design often involves modifying the parameters until a conceptualized target sound is reached. The }\DIFaddend automation of this \DIFdelbegin \DIFdel{process, which we call }\textit{\DIFdel{iterative sound-matching}}%DIFAUXCMD
\DIFdel{. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Automatic sound design has been a subject of research for decades}\DIFdelend \DIFaddbegin \DIFadd{approach has commonly been referred to as "sound-matching"}\DIFaddend , with reduction of tinkering time and creation of ``interesting" sounds as the main motivators \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{krekovic2019insights,turian2020sorry,horner1993machine,salimi2020make,esling2019flow,engel2020ddsp,mitchell2007evolutionary,shier2020spiegelib,masuda2021soundmatch,masuda2023improving}}\hskip0pt%DIFAUXCMD
. }\textit{\DIFdel{Sound-matching}} %DIFAUXCMD
\DIFdel{is a common form of automatic sounddesign explored in literature, where }\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{krekovic2019insights,turian2020sorry,horner1993machine,salimi2020make,esling2019flow,engel2020ddsp,mitchell2007evolutionary,shier2020spiegelib,masuda2021soundmatch,masuda2023improving}}\hskip0pt%DIFAUXCMD
. The main requirements of sound-matching are a target sound, }\DIFaddend a similarity metric (or loss function)\DIFdelbegin \DIFdel{is given, and the goal is }\DIFdelend \DIFaddbegin \DIFadd{, and a heuristic }\DIFaddend to find parameters for a synthesizer that replicate \textit{all} or \textit{some} of the characteristics of \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend target sound as best as possible~\cite{horner1993machine,mitchell2007evolutionary,masuda2023improving}. \DIFdelbegin \DIFdel{Our analysis of past works shows that this general goal of ``}\DIFdelend \DIFaddbegin \DIFadd{However, despite the seeming simplicity of the problem and decades of research, }\DIFaddend sound-matching \DIFdelbegin \DIFdel{'' can be broken down in to very different subtypes, many of them rarely or never explored, despite their relevance to practical applications. This leads us to a typology of sound-matching tasks, and the importance of }\textit{\DIFdel{differentiable, iterative sound-matching}}%DIFAUXCMD
\DIFdel{, rarely explored in previous literature~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures}}\hskip0pt%DIFAUXCMD
.
}\DIFdelend \DIFaddbegin \DIFadd{is not yet a practical solution for sound-designers. Here we are motivated to understand why this is the case, and what future research should prioritize in order to improve the field. 
}\DIFaddend 

\DIFdelbegin \textit{\DIFdel{Iterative sound-matching}} %DIFAUXCMD
\DIFdel{refers to works that mimic the back-and-forth process of listening to a synthesizer's output and updating the parameters until the termination conditions are met. The two necessary components are the }\textit{\DIFdel{loss function}}%DIFAUXCMD
\DIFdel{, which determines how close two sounds are, and the }\textit{\DIFdel{parametric synthesizer}} %DIFAUXCMD
\DIFdel{that outputs sounds based on input parameters. In the iterative workflow, a target sound is selected, the synthesizer outputs sounds based on its parameters, and the loss function determines the proximity of the output to the target. From there, the synthesizer parameters are updated, the output and target are compared, and so on. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Despite the early popularity of the iterative approach~\mbox{%DIFAUXCMD
\cite{justice1979analytic,horner1993machine,mitchell2007evolutionary}}\hskip0pt%DIFAUXCMD
, recent works in sound-matching have favored supervised learning using inference models, deep neural-networks (\gls{DNNs}), and large datasets. Rather than continually interacting with the environment, these models learn from a dataset of inputs (synthesizer parameters) and outputs (corresponding sound), and predict the synthesizer parameters for new sounds~\mbox{%DIFAUXCMD
\cite{yee2018automatic,han2023perceptual,han2024learning,masuda2021soundmatch,masuda2023improving}}\hskip0pt%DIFAUXCMD
. }%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Iterative sound-matching is interesting for multiple reasons. First, it better resembles the human approach to sound-design, which requires continually interacting with the environment and making decisions according to a feedback signal~\mbox{%DIFAUXCMD
\cite{sutton2018reinforcement}}\hskip0pt%DIFAUXCMD
. Second, the trial-and-error approach leaves a trail of parameter sets from the starting point to the end goal, which could be of use to sound designers. 
Third, relative to the DNN-based inference models, it provides a more tractable environment with room for creative approaches to sound-similarity definitions, and more amenable to deployment on different synthesizers as it does not require pre-training on large datasets. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{This work takes a }\textit{\DIFdel{differentiable}} %DIFAUXCMD
\DIFdel{approach to iterative sound-matching, a natural extension of early works in iterative sound-matching combined with recent advances in Machine Learning (\gls{ML}) software~\mbox{%DIFAUXCMD
\cite{braun2024dac,paszke2019pytorch} }\hskip0pt%DIFAUXCMD
and increase in computational power~\mbox{%DIFAUXCMD
\cite{moore2012introduction,schaller1997moore}}\hskip0pt%DIFAUXCMD
. A differentiable approach requires differentiable loss and synthesizer functions, allowing the synthesizer parameters to be updated using gradient descent~\mbox{%DIFAUXCMD
\cite{goodfellow2016deep,boyd2004convex}}\hskip0pt%DIFAUXCMD
. }%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{We discuss the history of sound-matching, and provide a nomenclature for different subtypes of sound-matching tasks. A number of weaknesses in current literature (see Section~\ref{sec:lacking}) are identified. The experiments in this work focus on two of these core issues , referred to as }\DIFdelend \DIFaddbegin \DIFadd{We provide a review of past works and identify major perennial issues in the field. In particular, we are concerned with the issues of }\DIFaddend \LossSelect~and \SynthSelect. The former refers to the search for an optimal loss function, lack of novel algorithms, and the---perhaps needless---focus on outperforming state of the art (\gls{SOTA}), while the latter refers to the lack of diversity in synthesis methods. \DIFaddbegin \DIFadd{As a consequence of these problems, we find that not much is known about the interaction between different methods of synthesis and different loss functions. Is there a universally best performing loss function for audio? Is there a need for further development of bespoke loss functions?
}\DIFaddend 

\DIFdelbegin \DIFdel{In the case of \LossSelect, there }\DIFdelend \DIFaddbegin \DIFadd{The main hypothesis here is that }\textit{\DIFadd{the performance of a similarity measure (or loss function) is influenced by other factors in the environment, particularly the method of synthesis}}\DIFadd{. This hypothesis is proven false if a one loss function consistently proves most effective in a series of different sound-matching experiments. While there }\DIFaddend have been many works and experiments comparing the accuracy of loss functions\DIFdelbegin \DIFdel{and choosing the best performers~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures,turian2020sorry,engel2020ddsp,uzrad2024diffmoog,han2023perceptual,masuda2021soundmatch,turian2020sorry,bruford2024synthesizer}}\hskip0pt%DIFAUXCMD
. We note that these }\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures,turian2020sorry,engel2020ddsp,uzrad2024diffmoog,han2023perceptual,masuda2021soundmatch,turian2020sorry,bruford2024synthesizer}}\hskip0pt%DIFAUXCMD
, we note that }\DIFaddend claims regarding the effectiveness of one function versus another have \DIFdelbegin \DIFdel{consistently }\DIFdelend \DIFaddbegin \DIFadd{often }\DIFaddend been made in limited contexts that may not generalize to other settings. \DIFdelbegin \DIFdel{One such limitation is the synthesizers used, and ties into the problem of \SynthSelect, as typically only a single (and often, simple) synthesizer has been used for testing the effectiveness of loss functions. Furthermore, there is a lack of diversity in the underlying functions used by the synthesizers, with simple Frequency Modulation (\gls{FM}) being the most common method of synthesis~\mbox{%DIFAUXCMD
\cite{horner1993machine,vahidi2023mesostructures,mitchell2007evolutionary,caspe2023envelope}}\hskip0pt%DIFAUXCMD
. 
}\DIFdelend \DIFaddbegin \DIFadd{Beyond this central hypothesis, we also investigate three additional research questions: (Q1) To what extent do automatic evaluation metrics agree with manual listening tests? (Q2) Can loss functions based on Dynamic Time Warping (DTW) and Scale-Invariant Mean Squared Error (SIMSE) provide advantages over SOTA loss functions, and under what conditions?
(Q3) Is iterative differentiable optimization a viable strategy for design of sound-matching experiments?
}\DIFaddend 

 \DIFdelbegin \DIFdel{Considering the interplay between the two problems, we design experimentsthat address them simultaneously. Testing the hypothesis that the performance of a similarity measure (or loss function ) is influenced by other factors in the environment, particularly the }\DIFdelend \DIFaddbegin \DIFadd{We adopt a lesser used }\textit{\DIFadd{iterative}} \DIFadd{and }\textit{\DIFadd{differentiable}} \DIFadd{approach to defining sound-matching experiments. The iterative approach better mimics the manual process of recursive listening and parameter adjustments towards sound design, while a differentiable environment allows access to the loss function gradients that can be used to better understand the nature of the problem. We define four differentiable synthesizers (each showcasing a fundamental }\DIFaddend method of synthesis \DIFdelbegin \DIFdel{. We combine 4 loss functions with 4 differentiable synthesizers, and compare their performance as measured by the similarity of the final output to the target}\DIFdelend \DIFaddbegin \DIFadd{used in modern synthesizers) and pair them with four different loss functions (two established methods, one utilizing DTW, and one utilizing SIMSE)}\DIFaddend . We evaluate the final similarity with two different automatic methods, as well as manual listening tests conducted by two of the authors. We \DIFdelbegin \DIFdel{compare the distributions of the evaluation scores in order to rank the loss functions from best to worst. The results of these experiments show that when accounting for different synthesizers, there is no consistent choice of sound similarity measure for optimal performance in sound-matching}\DIFdelend \DIFaddbegin \DIFadd{apply statistical ranking applied to the evaluation scores to compare outcomes across synthesizer–loss combinations and measure the similarity of different evaluation measures}\DIFaddend . 

\DIFdelbegin \DIFdel{We note that for some synthesizers, our novel use of ``dynamic time warping of envelopes'' as a loss function for differentiable }\DIFdelend \DIFaddbegin \textbf{\DIFadd{Contributions.}} \DIFadd{The contributions of this paper include (1) an evaluation of multiple differentiable losses across multiple synthesis methods, (2) the introduction and justification of loss functions utilizing DTW and SIMSE (3) a discussion on the utility and agreement between manual and automatic evaluation metrics (4) a nomenclature of different }\DIFaddend sound-matching \DIFdelbegin \DIFdel{outperforms the standard methodsof audio comparison. These results imply that rather than aiming to outperform the SOTA and finding a universal best performer, more effort could be placed towards diversifying the existing similarity measuresof sound as well as conducting experiments using a variety of synthesis methods. Sound design is an inherently creative endeavor, and would likely benefit from a diversified range of solutions for different approaches}\DIFdelend \DIFaddbegin \DIFadd{approaches and unsolved issues in the field. 
}


%DIF >  Here, several common characteristics of previous work are avoided in order to gain fundamental, practical takeaways from the experiments. In order to directly analyze the interaction between the loss and method of synthesis, the synthesizer here are implemented with simple, functions that are used as the building blocks of complex synthesizers, and the gradients flow from the loss function to the synthesizer parameters with no extra layers between the two. Furthermore, since the true target parameters are unknown in any realistic sound-design scenario, parameter loss (P-Loss) is only used as a secondary measure of performance, and not used as a loss function.

%DIF >  The simple synthesizers tested here represent fundimental building blocks of more complex implementations, and direct use of loss function gradients assuming 
%DIF >    A primary conclusion of the work is that there is no one best sound similarity measure capable of working across all synthesizers.
%DIF >  The core problem that no single loss function performs optimally across synthesizer types

\section{\DIFadd{Background and Related Work}}
\label{sec:background_related}
 \DIFadd{Sound-matching sits at the intersection of digital signal processing, audio representation, and optimization. Here we first formalize the sound-matching task, then review core synthesis methods, similarity measures, and optimization approaches. 
We conclude with a survey of related work and a discussion of gaps in the field}\DIFaddend . 

\DIFdelbegin \section{\DIFdel{Background}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
%DIFDELCMD < \label{sec:background}
%DIFDELCMD < %%%
\DIFdel{In this Section, we give a formal definition of }\DIFdelend \DIFaddbegin \subsection{\DIFadd{Formalization of Sound-Matching}}
\label{sec:sound_matching_definition}
\DIFadd{Following prior works~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures,han2023perceptual}}\hskip0pt%DIFAUXCMD
, we define }\DIFaddend sound-matching \DIFaddbegin \DIFadd{in terms of a parametric synthesizer, a target sound, a representation function, and a similarity measure. 
The key components are:
}\begin{itemize}
    \item \DIFadd{$g(\theta)$: A parametric audio synthesizer $g$ with parameters $\theta$.
    }\item \DIFadd{$x_\theta$: The synthesizer output, $x_\theta = g(\theta)$.”.
    }\item \DIFadd{$\xzero$: The target sound to be replicated or imitated. 
    }\item \DIFadd{$\phi(\cdot)$: A representation (feature extraction) function mapping signals into a comparison space.
    }\item \DIFadd{$L$: A loss function that measures distance between $\xtheta$ }\DIFaddend and \DIFdelbegin \DIFdel{the necessary components }\DIFdelend \DIFaddbegin \DIFadd{$\xzero$, typically via $L(\theta,\xzero) = d(\phi(\xtheta), \phi(\xzero))$ for some metric $d$}\DIFaddend .
\DIFdelbegin \DIFdel{These discussions lead to a nomenclature for types }\DIFdelend \DIFaddbegin \end{itemize}

\DIFadd{This formalization highlights that sound-matching is not a single well-defined optimization problem, but rather depends on design choices for $g$, $\phi$, and $L$. 
Moreover, the ``correct'' solution may not exist in a strict sense: depending on the artistic goal, multiple parameter settings may yield acceptable or even preferable outputs.  In the following sections we will discuss the various components }\DIFaddend of sound-matching, \DIFdelbegin \DIFdel{as well as major perennial }\DIFdelend \DIFaddbegin \DIFadd{the subjectivity of ``correct'' solutions, and important }\DIFaddend issues in the field.

 \DIFdelbegin \subsection{\DIFdel{Digital Audio Synthesis}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < \label{sec:diffSynth}
%DIFDELCMD < %%%
\DIFdel{A digital audio synthesizer is any software used for the creation and manipulation of }\DIFdelend \DIFaddbegin \DIFadd{Figure~\ref{fig:sound_design_loop_iterative} shows a general model for iterative sound-matching. To begin the process, the parameters are arbitrarily initialized (usually random generation), the similarity of the target and output is measured, and the parameter is }\textit{\DIFadd{optimized}} \DIFadd{with the goal of increasing the similarity, or alternatively, reducing the loss. This process repeats until termination. 
}\begin{figure}[ht]
    \centering
\begin{tikzpicture}[node distance=2cm, auto]

% Nodes
\node (start) [text centered] {\( g(\hat{\theta}) = x_{\theta} \)};
\node (L) [above of=start, right of=start, text centered] 
    {\( L(\hat{\theta}, x_{0}) \propto \phi(x_{\theta}) - \phi(x_{0}) \)};
\node (optimize) [below of=L, right of=L, text centered] {Optimize $\hat{\theta}$};
\node (new_theta) [below of=optimize, right of=start, text centered] {New \( \hat{\theta} \)};

% Highlight and arrow for the start node
\draw[->, very thick, red] (start) ++(-2,1.5) -- (start)
    node[midway, below, align=center, sloped, color=red] {Start}
    node[midway, above, align=center, sloped, color=red] {Random $\hat{\theta}$};

% Arrows with multi-line labels
\draw[->, bend left] (start) to node[midway, right, align=center] {} (L);
\draw[->, bend left] (L) to node[midway, right, align=center] {} (optimize);
\draw[->, bend left] (optimize) to node[midway, below, align=center] {} (new_theta);
\draw[->, bend left] (new_theta) to node[midway, left, align=center] {} (start);

\end{tikzpicture}
    %DIF >  \caption{ Iterative approach to sound design. Synthesizer $g$ with arbitrary initialized parameters $\hat{\theta}$, creates sound $x$. The target sound $t$ is used as the goal. Parameters $\hat{\theta}$ are adjusted to minimize error (or loss) $L(\hat{\theta},t)$, where $L$ is proportional to the difference between the representations of $x$ and $t$. $\phi$ is the audio feature extractor, or representation function.}
    \caption{ \DIFaddFL{Iterative approach to sound design.}}
    \label{fig:sound_design_loop_iterative}
\end{figure}


\subsection{\DIFadd{Digital Signal Processing and Synthesis}}
\label{sec:dsp}

\DIFadd{A digital audio synthesizer generates or processes audio by chaining }\DIFaddend digital \DIFdelbegin \DIFdel{audio. Digital synthesizers use signal processing chains with a variety of digital }\DIFdelend signal processing (\gls{DSP}) functions\DIFdelbegin \DIFdel{to create sounds. 
These functions are often parametric}\DIFdelend \DIFaddbegin \DIFadd{. 
Each function is parameterized}\DIFaddend , and the set of parameters \DIFdelbegin \DIFdel{given to the synthesizer's }\DIFdelend \DIFaddbegin \DIFadd{defining a }\DIFaddend chain of DSP functions is called a synthesizer program. 

The simplest \DIFdelbegin \DIFdel{form of }\DIFdelend DSP function is a sinusoidal \DIFdelbegin \DIFdel{tone. For example, consider: 
}\DIFdelend \DIFaddbegin \DIFadd{oscillator: 
}\DIFaddend \[
x\DIFdelbegin \DIFdel{(}\DIFdelend \DIFaddbegin [\DIFaddend n\DIFdelbegin \DIFdel{) }\DIFdelend \DIFaddbegin ] \DIFaddend = \DIFdelbegin \DIFdel{sin( }\DIFdelend \DIFaddbegin \DIFadd{\sin\!}\left(\DIFaddend 2 \pi \DIFdelbegin \DIFdel{n T)}\DIFdelend \DIFaddbegin \DIFadd{f \frac{n}{SR}}\right)\DIFadd{,
}\DIFaddend \]
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{where $T$ represents time in seconds, }\DIFdelend \DIFaddbegin \DIFadd{where $f$ is frequency in hertz, $SR$ is the sampling rate, }\DIFaddend and $n$ is the \DIFdelbegin \DIFdel{frequency parameter. 
If $n$ is }\DIFdelend \DIFaddbegin \DIFadd{discrete time index. 
At a sampling rate of $SR$ samples per second, a }\DIFaddend 1\DIFdelbegin \DIFdel{, then $x(n)$ would be a waveform with a frequency of 1 hertz, meaning that the value of $x(n)$ oscillates to the same point once every second. 
Waveform generators are called oscillators for this reason. 
}\DIFdelend \DIFaddbegin \DIFadd{\,Hz oscillator completes one cycle per second. 
Such computations form the foundation of digital audio synthesis~\mbox{%DIFAUXCMD
\cite{lyons1997understanding}}\hskip0pt%DIFAUXCMD
. 
}\DIFaddend 

%DIF <  If $n$ is in the range of 30 to 20,000 (the upper limit is often lower for some), $x(n)$ could be played as an audible tone~\cite{smith2007mathematics,jilek2014reference}.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Since the advent of \DIFdelbegin \DIFdel{digital signal processing }\DIFdelend \DIFaddbegin \DIFadd{DSP }\DIFaddend in the 1960s~\cite{stranneby2004digital}, a wide variety of parametric \DIFdelbegin \DIFdel{audio generation }\DIFdelend functions have been proposed\DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{lyons1997understanding,russ1999sound,shier2020spiegelib}}\hskip0pt%DIFAUXCMD
. This includes }\DIFdelend \DIFaddbegin \DIFadd{, including }\DIFaddend oscillators, filters, equalizers, and envelopes\DIFdelbegin \DIFdel{, which can be used sequentially or in parallel}\DIFdelend ~\cite{lyons1997understanding,russ1999sound}. 
Sound design \DIFdelbegin \DIFdel{with a synthesizer is done by modifying the parameters of the functions until reaching }\DIFdelend \DIFaddbegin \DIFadd{involves modifying these parameters until }\DIFaddend a desired output \DIFaddbegin \DIFadd{is reached}\DIFaddend ~\cite{roads1996computer,pinch2004analog}. 

\DIFdelbegin \DIFdel{A relatively small subset of these synthesis methods have been used for automatic }\DIFdelend \DIFaddbegin \DIFadd{Early }\DIFaddend sound-matching \DIFdelbegin \DIFdel{, notably additive and }\DIFdelend \DIFaddbegin \DIFadd{research focused heavily on frequency and amplitude modulation (\gls{FM}}\DIFaddend /\DIFdelbegin \DIFdel{or subtractive synthesizers~\mbox{%DIFAUXCMD
\cite{esling2019flow,yee2018automatic,mitchell2007evolutionary,salimi2020make}}\hskip0pt%DIFAUXCMD
, }\DIFdelend \DIFaddbegin \DIFadd{\gls{AM}) synthesis~\mbox{%DIFAUXCMD
\cite{horner1993machine,mitchell2007evolutionary,vahidi2023mesostructures}}\hskip0pt%DIFAUXCMD
, which is simple to implement yet expressive~\mbox{%DIFAUXCMD
\cite{chowning1973synthesis}}\hskip0pt%DIFAUXCMD
. 
Other synthesis methods studied in }\textit{\DIFadd{isolation}} \DIFadd{include additive and subtractive synthesis~\mbox{%DIFAUXCMD
\cite{engel2020ddsp,masuda2023improving,salimi2020make} }\hskip0pt%DIFAUXCMD
and }\DIFaddend physical modeling~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{riionheimo2003parameter}}\hskip0pt%DIFAUXCMD
, and frequency/amplitude modulation (\gls{FM}/\gls{AM}) ~\mbox{%DIFAUXCMD
\cite{horner1993machine,vahidi2023mesostructures}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{riionheimo2003parameter,han2024learning}}\hskip0pt%DIFAUXCMD
. By }\textit{\DIFadd{isolation}}\DIFadd{, we mean settings where the effect of individual parameters on the output sound remains tractable. For example, we exclude studies using commercial Virtual Studio Technology (VST) which can obscure the interactions between modules, losses, and outputs}\DIFaddend . 

\DIFdelbegin \DIFdel{Differential }\DIFdelend \DIFaddbegin \DIFadd{Finally, recent years have seen a growing interest in differentiable }\DIFaddend DSP (\gls{DDSP})\DIFdelbegin \DIFdel{is a general term for applications that combine machine learning techniques such as gradient descent~}\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{engel2020ddsp}}\hskip0pt%DIFAUXCMD
, which integrates gradient-based optimization~}\DIFaddend \cite{goodfellow2016deep,boyd2004convex} with DSP \DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{engel2020ddsp}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{building blocks}\DIFaddend . 
Implementing complex DSP functions in a differentiable manner \DIFdelbegin \DIFdel{can be }\DIFdelend \DIFaddbegin \DIFadd{remains }\DIFaddend challenging, and \DIFdelbegin \DIFdel{effective }\DIFdelend \DIFaddbegin \DIFadd{robust }\DIFaddend differentiable audio similarity measures \DIFdelbegin \DIFdel{require careful mathematical expression of the desired attributes of sound. These issues have likely contributed to the limited exploration of this domain. Perhaps the most relevant recent work is by Vahidi }\textit{\DIFdel{et al.}}%DIFAUXCMD
\DIFdel{, featuring the comparison of two differentiable loss functions with a simple FM synthesizer~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{are still under active investigation~\mbox{%DIFAUXCMD
\cite{masuda2021soundmatch,vahidi2023mesostructures,uzrad2024diffmoog}}\hskip0pt%DIFAUXCMD
}\DIFaddend .


\subsection{Sound Representation and Loss Functions}
\label{sec:loss_funcs}
A digital sound (or an audio signal) is a series of numbers~\cite{smith1991viewpoints,smith2007mathematics}. To compare two digital sounds, the two corresponding series are passed to a function that measures their similarity. Two signals can sound identical to our ears, without having any values in common~\cite{moore2012introduction}. This necessitates the use of proxy representations (or feature extractors) when comparing sounds automatically. Similarity between the target sound and the synthesizer output is then measured by some form of subtraction and summation of the proxy representations.

In sound-matching, particularly in a Deep Learning (\gls{DL}) context~\cite{goodfellow2016deep}, the similarity function can also be called a \textit{loss} function, where the emphasis is on measurement and reduction of the distance between target and output. It is important to note that there is a close relationship between the loss function $L$ and the sound representation function $\phi$. $L$ is the result of a distance measure $d$ applied to the features extracted by $\phi$. 

\[
L(\theta, \DIFdelbegin \DIFdel{t}\DIFdelend \DIFaddbegin \DIFadd{\xzero}\DIFaddend ) = d\DIFdelbegin %DIFDELCMD < \langle%%%
\DIFdelend \DIFaddbegin \DIFadd{(}\DIFaddend \phi(\DIFdelbegin \DIFdel{t}\DIFdelend \DIFaddbegin \DIFadd{\xtheta}\DIFaddend ), \phi(\DIFdelbegin \DIFdel{x}\DIFdelend \DIFaddbegin \DIFadd{\xzero}\DIFaddend )\DIFdelbegin %DIFDELCMD < \rangle
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{)
}\DIFaddend \]

\noindent

A proxy representation is the output of the function \( \phi \), which can be thought of as a feature extraction function that maps the sounds  \DIFdelbegin \DIFdel{\( t \) and \( x \) }\DIFdelend \DIFaddbegin \DIFadd{$\xzero$ and $\xtheta$  }\DIFaddend to their respective representations. 
The proportionality or distance metric $d$ has typically been the L1 \DIFaddbegin \DIFadd{or L2 }\DIFaddend distance~\cite{turian2020sorry,richard2025model}, \DIFaddbegin \DIFadd{with L1 being }\DIFaddend calculated as the mean of the absolute difference between every point in the proxy representation\DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{engel2020ddsp,vahidi2023mesostructures}}\hskip0pt%DIFAUXCMD
:
}\DIFdelend \DIFaddbegin \DIFadd{:
}\DIFaddend \[
L(\theta, \DIFdelbegin \DIFdel{t}\DIFdelend \DIFaddbegin \DIFadd{\xzero}\DIFaddend ) = \left\| \phi(\DIFdelbegin \DIFdel{t}\DIFdelend \DIFaddbegin \DIFadd{\xzero}\DIFaddend ) - \phi(\DIFdelbegin \DIFdel{x}\DIFdelend \DIFaddbegin \DIFadd{\xtheta}\DIFaddend ) \right\|_1
\]

Here we discuss four methods of audio representation and the corresponding loss functions\DIFdelbegin \DIFdel{. Three of which have been used in previous works, and a novel method not previously used in iterative sound-matching}\DIFdelend \DIFaddbegin \DIFadd{, including technical justifications for our novel methods}\DIFaddend .

\subsubsection{Parameter Loss}
A common measure of similarity in sound-matching is the distance between synthesizer parameter sets, referred to as ``P-Loss"~\cite{han2023perceptual}. Typically, for the implementation of P-Loss the parameter sets are treated as vectors in space, and L1 or L2 distance is applied. There are two major limitations to this approach: First, the target and output sound must be made by the same synthesizer; otherwise the parameter sets cannot be compared (see Section~\DIFdelbegin \DIFdel{\ref{sec:matching_types}}\DIFdelend \DIFaddbegin \DIFadd{\ref{sec:in-domain}}\DIFaddend ). Second, the relationship between synthesizer parameters and the audio output is not linear~\cite{shier2020spiegelib,han2023perceptual,esling2019flow}. 

\subsubsection{Fourier Spectrograms}
\label{sec:fourier_specs}
Fourier-based transformations such as short-time Fourier transforms (\gls{STFT}), Mel-spectrograms, and Mel-frequency cepstral coefficients have been viewed as the de facto and state-of-the-art representation of audio~\cite{beauchamp2003error,mitchell2007evolutionary,yee2018automatic}, however, there are many issues associated with their use in sound-matching~\cite{turian2020sorry,vahidi2023mesostructures,han2023perceptual,uzrad2024diffmoog}. Fourier transformations allow for the conversion of a signal from the time-domain to the frequency domain. Audio spectrograms can be generated by segmentation of a piece of audio into overlapping windows followed by the application of Fourier transforms to each window. They are costly to compute, but provide a better temporal view of changes in frequency content~\cite{muller2007dynamic,smith2007mathematics}. There are different types of spectrograms that have a basis in Fourier transformations, but the most notable and commonly used is the STFT.  
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend What we call \textit{Fourier-based Spectrograms} are variations on the STFT approach. For example, Mel-Spectrograms \textit{bin} frequencies on a near-logarithmic scale to better match human perception of frequencies~\cite{muller2007dynamic}. Multi-scale spectrograms (\gls{MSS}) used in recent works are a simple weighted average of multiple spectrograms with different parameters such as window size, number of frequency bins, and hop size~\cite{engel2020ddsp,vahidi2023mesostructures}; this may provide some improvements at a higher computational cost~\cite{turian2020sorry,engel2020ddsp}.

\DIFaddbegin \DIFadd{A common limitation of spectrogram-based losses is their sensitivity to global gain: two signals with identical spectral shape but different overall amplitude can yield large errors under L1/L2 norms. 
Here we propose and test Scale-Invariant Mean Squared Error (SIMSE) as an alternative to L1/L2 norms in spectrogram comparisons. SIMSE normalizes the amplitudes before comparison and emphasizes proportional differences in spectral envelopes~\mbox{%DIFAUXCMD
\cite{barron2014shapessimse}}\hskip0pt%DIFAUXCMD
. 
Perceptually, listeners are often tolerant of loudness changes while remaining sensitive to timbral shape, therefore this property could be advantageous for subtractive synthesis, where filter cutoffs reshape spectral balance without predictable changes in total energy. Although SIMSE has been applied in other contexts such as audio reconstruction, to our knowledge its use as a differentiable spectrogram loss in iterative sound-matching is novel.
}

 %DIF >  \todo{(this is not addressed by SIMSE so why is it here?) An example given by Vahidi \textit{et al.}~\cite{vahidi2023mesostructures} is two chirplets (tones that increase in pitch) starting and ending at the same frequencies. If one of these tones is slightly shifted in time, then the spectrograms will no longer overlap at any point, despite their sonic similarity (similar to the idea that a 2D plot of say, $f(x) = ax$, can never overlap with a slightly vertically shifted version of itself, or $f(x) = ax +\epsilon$). In this example, we do not know if this is an issue of the spectrograms, or the L1 measure used for their comparison. In fields such as computer vision, Structural Similarity Index (SSIM)~\cite{wang2004imagesssim,wang2009mean} and Scale-Invariant Mean Squared Error (SIMSE)~\cite{barron2014shapessimse} have been used as alternatives that improved performance in various tasks relative to L1.}

\DIFaddend \subsubsection{Joint-Time Frequency Spectrum}
Recent works have focused on the limitations of parameter and spectral loss functions in sound-matching~\cite{vahidi2023mesostructures,uzrad2024diffmoog}, seeking to create more effective general solutions for the comparison of audio. 
Noting the \DIFdelbegin \DIFdel{aforementioned }\DIFdelend weaknesses of comparing STFT spectrograms \DIFaddbegin \DIFadd{(such as alignment and loudness distances)}\DIFaddend , Vahidi \textit{et al.} proposed differentiable Joint-Time Frequency Scattering (\gls{JTFS})~\cite{anden2015joint} as an alternative to spectrogram loss in sound-matching, and showed improved performance in sound-matching with differentiable chirplet synthesizers~\cite{vahidi2023mesostructures}. JTFS is the result of the application of a 2D wavelet transformation to the time-frequency representation of a signal~\cite{anden2015joint} \DIFaddbegin \DIFadd{and has been reported as more sensitive to }\textit{\DIFadd{mesostructural}} \DIFadd{features such as melody, syncopation, and textural
contrast~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures}}\hskip0pt%DIFAUXCMD
}\DIFaddend .


\subsubsection{Dynamic Envelope Warping}
Dynamic Time Warping (DTW) is a method for measuring similarity between multi-dimensional time-series~\cite{rabiner1993fundamentals,muller2007dynamic,giorgino2009computing}. Given any two time-series $X = \{x_1,x_2,...,x_m\}$ and $Y = \{y_1,y_2,...,y_n\}$, we have indices $i\in\{1...m\}$ and $j\in\{1...n\}$ defining $X$ and $Y$. When the series are \textit{warped}, these indices change to expand or contract different portions of the series. To borrow the notation given by Muller~\cite{muller2007dynamic}, warped indices are a sequence $p=(p_1,...,p_L)$, where \(p_\ell = (m_\ell, n_\ell) \in [1 : m] \times [1 : n] \text{ for } \ell \in [1 : L]\), meaning that the indices for $X$ and $Y$ are reorganized under special conditions. In classical DTW, these conditions are \textit{monotonicity}, \textit{boundary matching}, and \textit{single step-size}. \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend DTW measures the distance between the time-series \textit{after} alignments, typically using Euclidean distance\DIFdelbegin \DIFdel{. The }\DIFdelend \DIFaddbegin \DIFadd{, such that the }\DIFaddend distance between a time-series and shifted versions of itself would be 0, regardless of shift amount~\cite{tavenard.blog.dtw}. Additional rules can be imposed to keep alignments locally constrained~\cite{itakura1975minimum,sakoe1978dynamic}.
%DIF <  \textcolor{red}{DTW was originally used for signal comparison but using it the way we do is novel}

\DIFdelbegin \subsection{\DIFdel{Automatic Sound Matching}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < \label{sec:sound_matching_definition}
%DIFDELCMD < %%%
\DIFdel{The problem of }\DIFdelend \DIFaddbegin \DIFadd{DTW provides robustness to local temporal shifts, making it well suited for comparing modulated signals where the perceptual similarity lies in envelope dynamics rather than precise onset alignment. 
For example, two tremolo signals with slightly different phase are perceptually similar but would appear distant under spectrogram L1. 
A possible use case to consider is the application of DTW to amplitude envelopes, which may be able to match two sounds with similar rates of loudness modulation regardless of alignments. 
To our knowledge, DTW applied in this way has not been used as a loss in }\DIFaddend sound-matching \DIFdelbegin \DIFdel{has been tackled from a variety of perspectives as it involves a number of modular parts: the choice of synthesizer $g$, the target sounds of interest, the representation function $\phi$, and the heuristic for finding the optimal $\theta$. 
To give a formal definition of sound-matching, we expand the definition given by recent works~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures,han2023perceptual}}\hskip0pt%DIFAUXCMD
. The major components are as follows: 
}%DIFDELCMD < \begin{itemize}
\begin{itemize}%DIFAUXCMD
%DIFDELCMD <     \item %%%
\item%DIFAUXCMD
\DIFdel{$g(\theta)$: Parametric audio synthesizer $g$ which takes on parameters $\theta$ 
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{$x$: The output of $g$, given a set of parameters $\theta$ or $g(\theta) = x$ 
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{$t$: The target sound which we want to replicate or imitate. 
}%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{$\phi$: Representation function or feature extractor. $\phi$ is applied to $x$ and $t$ to facilitate their comparison by the loss function.
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{$L$: Loss or error function. $L(\theta,t)$ is a measure of distance between $x$ and $t$. Often proportional to the subtraction of their representations, or $ \phi(x) - \phi(t)$
}
\end{itemize}%DIFAUXCMD
%DIFDELCMD < \end{itemize}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{(whether differentiable and iterative or not).
}\DIFaddend 


\DIFdelbegin \DIFdel{\highlight{It could be argued that the implementation of these components is not an optimization problem with a correct answer, but a \textit{creative} endeavor depending on the artist's needs}. Consider the hypothetical case where the goal might be to create an 8-bit~\mbox{%DIFAUXCMD
\cite{collins2007loop} }\hskip0pt%DIFAUXCMD
version of a high quality, organic snare drum: in such a case, there is no optimal or ``correct" version of an 8-bit snare sound. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\subsection{\DIFdel{Sound Matching Types}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < \label{sec:matching_types}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Sound-matching tasks can be categorized based on the choices of target sound $t$, the loss $L$ and representation function $\phi$, and the heuristic used for minimizing $\theta$.
We will discuss the subtypes of sound-matching tasks in the upcoming sub-sections.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\subsubsection{\DIFdel{In-Domain vs Out-of-Domain}}
%DIFAUXCMD
\addtocounter{subsubsection}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \subsection{\DIFadd{In-Domain Versus Out-of-Domain}}
\DIFaddend \label{sec:in-domain}
The choice of domain depends on whether we want to use the same synthesizer for the target and output sounds, \DIFdelbegin \DIFdel{which we call }\DIFdelend \DIFaddbegin \DIFadd{the scenario that is called }\DIFaddend \textit{in-domain}, or have target sounds that came from sources other than the synthesizer, or \textit{out-of-domain}. To paraphrase the description given by Masuda \textit{et al.}~\cite{masuda2021soundmatch}, if $g$, the synthesizer of choice, can accurately replicate the target sound \DIFdelbegin \DIFdel{$t$}\DIFdelend \DIFaddbegin \DIFadd{$\xzero$}\DIFaddend , or put differently, if \DIFdelbegin \DIFdel{$t$ }\DIFdelend \DIFaddbegin \DIFadd{$\xzero$ }\DIFaddend itself is an output of $g$, then the sound-matching task is \textit{in-domain}. If \DIFdelbegin \DIFdel{$t$ }\DIFdelend \DIFaddbegin \DIFadd{$\xzero$ }\DIFaddend is not an output of $g$, then the sound-matching task is \textit{out-of-domain}. In-domain tasks in general are simpler, and often there is a guarantee that there is a correct answer to the sound-matching problem, particularly if the goal is accurate replication of the sound. If the target sound is out-of-domain, replication is not guaranteed, and the goal becomes the \textit{imitation} of some aspect of sound. 

\DIFdelbegin \subsubsection{\DIFdel{Replication vs Imitation}}
%DIFAUXCMD
\addtocounter{subsubsection}{-1}%DIFAUXCMD
\DIFdelend Regardless of the domain, the generation goal can be \textit{replication} or \textit{imitation} of the target sound. In replication, the goal is to make an identical copy of the target sound. Imitation is an artistic pursuit and harder to define, since the goal is to make new sounds that only retain a subset of the target's sonic features. While closely related to the in-domain versus out-of-domain problem, the choice of replication versus imitation is more dependent on how the loss and representation functions are defined. 

%DIF <  From a signal perspective, replication is a scenario where the goal is to find a $\theta$ such that $t-x=0$ where $x=g(\theta)$. From a sound perspective, this definition of replication is unnecessarily restrictive, as $t$ and $x$ can sound identical but contain very different values. For example, if $t$ and $x$ were periodic waveforms at an audible frequency $f$ with different phase values, their subtraction would not be 0, yet they would sound identical to our ears, as our ears are insensitive to phase values~\cite{moore2012introduction}. Representation functions such as STFT spectrograms or Mel-frequency cepstral coefficients (\gls{MFCC}) become viable in these cases, as they allow for separation and removal of the phase information within an audio signal. The \textit{sound replication} goal can be defined, in layman's terms, as ``$x$ sounds exactly like $t$". More formally, the goal is to find parameters such that $\phi(t)-\phi(x)=0$, where $\phi$ transforms an audio into a spectrogram that removes the phase information from sounds, but otherwise represents the sonic information as accurately as possible.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  With \textit{sound imitation}, the goal is the replication of some aspect of the target sound. Instrument conversion is a common example of imitation. As an example, a note played by a violin is converted to the same note played by a piano. Conversion of even longer pieces has been made possible with large enough datasets~\cite{engel2020ddsp}. Imitation of sound envelopes (how the loudness changes over time) is another interesting idea~\cite{caspe2023envelope}. Transfer of other sound properties such as reverb and delay has also been a subject of research~\cite{engel2020ddsp}. 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  A target sound can have many properties of interest, and the imitation task is the transfer of a subset of those properties to other sounds. Relative to replication, imitation is a very open ended goal. From a research point of view, success in the replication task can be measured more reliably, often without the need for manual evaluation. With imitation, listening tests are needed to ensure quality of outputs, particularly in a supervised learning setting where learning is done by examples~\cite{salimi2020make,masuda2021soundmatch}. The need for subjective evaluations has perhaps contributed to the lack of research in imitation and out-of-domain sound-matching.
%DIFDELCMD < 

%DIFDELCMD < %%%
\subsubsection{\DIFdel{Supervised Versus Direct Optimization}}
%DIFAUXCMD
\addtocounter{subsubsection}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \subsection{\DIFadd{Supervised Versus Direct Optimization}}
\DIFaddend \label{sec:optimization}

The \DIFaddbegin \DIFadd{choice of }\textit{\DIFadd{heuristics}} \DIFadd{is yet another important attribute sound-matching. The }\DIFaddend goal of sound-matching is to find the optimal parameters $\theta^*$ that minimize the loss between the synthesizer output and the target sound. 
\[
\theta^* = \arg\min_{\theta} L(\theta,\DIFdelbegin \DIFdel{t}\DIFdelend \DIFaddbegin \DIFadd{\xzero}\DIFaddend )
\]

The heuristics \DIFaddbegin \DIFadd{(i.e., how $\theta^*$ is approximated) used }\DIFaddend in past works can be broadly split into \DIFdelbegin \DIFdel{two categories:
}%DIFDELCMD < \begin{enumerate}
%DIFDELCMD <     \item %%%
\DIFdel{Supervised }\DIFdelend \DIFaddbegin \DIFadd{the two categories of }\textit{\DIFadd{direct optimization}} \DIFadd{and }\textit{\DIFadd{supervised}}  \DIFaddend (or inference) methods\DIFdelbegin \DIFdel{which use datasets of sound and parameter pairs to describe }\DIFdelend \DIFaddbegin \DIFadd{. Direct optimization refers to the iterative generation of a sound output, measurement of similarity between target and output, and application of updates to the parameters to maximize similarity (or minimize loss)~\mbox{%DIFAUXCMD
\cite{horner1993machine,mitchell2007evolutionary,yee2018automatic,vahidi2023mesostructures}}\hskip0pt%DIFAUXCMD
; while supervised methods use large datasets of synthesizer sounds and corresponding parameters to learn }\DIFaddend the generation objective, \DIFdelbegin \DIFdel{often with a large number of input and output examples~\mbox{%DIFAUXCMD
\cite{engel2020ddsp,salimi2020make,yee2018automatic,esling2019flow}}\hskip0pt%DIFAUXCMD
}%DIFDELCMD < \item %%%
\DIFdel{Unsupervised or }\textit{\DIFdel{direct sound optimization}} %DIFAUXCMD
\DIFdel{methods~\mbox{%DIFAUXCMD
\cite{horner1993machine,mitchell2007evolutionary,yee2018automatic,vahidi2023mesostructures}}\hskip0pt%DIFAUXCMD
. Usually require at most one target example, and a loss function that measures the difference between the target and synthesizer outputs. 
}%DIFDELCMD < \end{enumerate}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{commonly with the use of DNNs~\mbox{%DIFAUXCMD
\cite{engel2020ddsp,salimi2020make,yee2018automatic,esling2019flow}}\hskip0pt%DIFAUXCMD
. These models often make their parameter predictions in a single step (or 1-shot). 
}\DIFaddend 

Genetic algorithms (\gls{GA})~\cite{holland1992genetic} \DIFdelbegin \DIFdel{can be used for direct sound optimization, and }\DIFdelend have been the earliest and most common heuristic \DIFdelbegin \DIFdel{in }\DIFdelend \DIFaddbegin \DIFadd{for direct }\DIFaddend sound-matching~\cite{horner1993machine,mitchell2007evolutionary,yee2018automatic}. These algorithms start with arbitrary parameter sets that can be treated as an evolving population where the genomes are the parameter values. The most fit members of the group are the parameters which perform the best in the loss function, and create a new generation of parameters via mutation (random change in subset of parameters) and crossovers (combination of parameter sets); this process repeats until the goal or a maximum number of generations is reached. \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD <  %%%
\DIFdel{Differentiable settings have the benefit of making }\DIFdelend \DIFaddbegin \DIFadd{Rather than using random mutations, differentiable approaches allow }\DIFaddend goal-oriented updates to the synthesizer parameters (the goal being the minimization of loss)\DIFdelbegin \DIFdel{. There are also }\DIFdelend \DIFaddbegin \DIFadd{, but with some }\DIFaddend drawbacks: other than requiring more computation power, differentiable functions require careful implementation of operations that are continuous and numerically stable; this makes the implementation of signal processing functions quite difficult, contributing to the scarcity of works in this domain.


%DIF <  One important aspect of the heuristics is whether they imitate the iterative nature of manual sound design---that is, listening to the output and adjusting the synthesizer parameters recursively until the desired goal is reached. Supervised methods generate their final output in a single step, while direct sound optimization methods refine their outputs iteratively until termination, which resembles manual sound design more closely.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\subsection{\DIFdel{Iterative Sound Matching}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < \label{sec:iterative_sound_matching}
%DIFDELCMD <  %%%
\DIFdel{\todo{Figure~\ref{fig:sound_design_loop_iterative} shows a general model for iterative sound-matching. To begin the process, the parameters are arbitrarily initialized (usually random generation), the similarity of the target and output is measured, and the parameter is \textit{optimized} with the goal of increasing the similarity, or alternatively, reducing the loss. This process repeats until termination. 
}
}%DIFDELCMD < \begin{figure}[ht]
%DIFDELCMD <     \centering
%DIFDELCMD < \begin{tikzpicture}[node distance=2cm, auto]
%DIFDELCMD < 

%DIFDELCMD <     % Nodes
%DIFDELCMD <     \node (start) [text centered] {\( g (\hat{\theta}) = x \)};
%DIFDELCMD <     \node (L) [above of=start,right of=start, text centered] {\( L(\hat{\theta},t) {\ \propto \ } \phi(x) - \phi(t) \)};
%DIFDELCMD <     \node (optimize) [below of=L,right of=L, text centered] {Optimize $\hat{\theta}$};
%DIFDELCMD <     \node (new_theta) [below of=optimize,right of = start, text centered] {New \( \hat{\theta} \)};
%DIFDELCMD < 

%DIFDELCMD <     % Highlight and arrow for the start node
%DIFDELCMD <     \draw[->, very thick, red] (start) ++(-2,1.5) -- (start)
%DIFDELCMD <         node[midway, below, align=center, sloped, color=red] {Start}
%DIFDELCMD <         node[midway, above, align=center, sloped, color=red] {Random $\hat{\theta}$};
%DIFDELCMD <         

%DIFDELCMD <     % Arrows with multi-line labels
%DIFDELCMD <     \draw[->, bend left] (start) to node[midway, right, align=center] {} (L);
%DIFDELCMD <     \draw[->, bend left] (L) to node[midway, right, align=center] {} (optimize);
%DIFDELCMD <     \draw[->, bend left] (optimize) to node[midway, below, align=center] {} (new_theta);
%DIFDELCMD <     \draw[->, bend left] (new_theta) to node[midway, left, align=center] {} (start);
%DIFDELCMD < 

%DIFDELCMD < \end{tikzpicture}
%DIFDELCMD <     %%%
%DIF <  \caption{ Iterative approach to sound design. Synthesizer $g$ with arbitrary initialized parameters $\hat{\theta}$, creates sound $x$. The target sound $t$ is used as the goal. Parameters $\hat{\theta}$ are adjusted to minimize error (or loss) $L(\hat{\theta},t)$, where $L$ is proportional to the difference between the representations of $x$ and $t$. $\phi$ is the audio feature extractor, or representation function.}
    %DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Iterative approach to sound design.}}
    %DIFAUXCMD
%DIFDELCMD < \label{fig:sound_design_loop_iterative}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{In the upcoming ``Past Works'' section, we will discuss the evolution of prior sound-matching research, with }\textit{\DIFdel{differentiable iterative sound-matching}} %DIFAUXCMD
\DIFdel{being the latest extension of the field.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \begin{table*}[t]
\centering
\caption{Summary of select works in sound-matching. Gen. refers to whether the generations are done in 1-shot (generally, output by a neural network) or a search is conducted with the goal of iteratively getting closer to the target sound.}
\setlength{\tabcolsep}{3pt} % Reduce horizontal spacing
\renewcommand{\arraystretch}{1.05} % Reduce vertical spacing
\scriptsize % Reduce font size
\resizebox{\textwidth}{!}{
\begin{tabular}{|>{\raggedright\arraybackslash}m{2cm}|>{\raggedright\arraybackslash}m{2cm}|>{\raggedright\arraybackslash}m{2cm}|>{\raggedright\arraybackslash}m{2cm}|>{\raggedright\arraybackslash}m{1.2cm}|>{\raggedright\arraybackslash}m{1.2cm}|>{\raggedright\arraybackslash}m{1cm}|>{\raggedright\arraybackslash}m{1cm}|}
\hline
\textbf{Work} & \textbf{Synthesis} & \textbf{Loss} & \textbf{Heuristics} & \textbf{Goal} & \textbf{Domain} & \textbf{Year} & \textbf{Gen. } \\
\hline
Horner \textit{\textit{et al.}}~\cite{horner1993machine} & Non-differentiable FM & STFT (McAulay-Quatieri) & GA & Replicate & In & 1991 & Iter \\
\hline
Mitchel \textit{\textit{et al.}}~\cite{mitchell2007evolutionary} & Non-differentiable FM & Spectral Relative & Evolutionary & Replicate & In (Contrived) & 2007 & Iter \\
\hline
Yee-King \textit{\textit{et al.}}~\cite{yee2018automatic} & Non-differentiable VST & P-Loss (for Supervised), Spectral (MFCC) & Supervised \& Evolutionary & Replicate & In & 2018 & Both \\
\hline
Esling \textit{\textit{et al.}}~\cite{esling2019flow} & Non-differentiable VST & Spec MSS/SC P-Loss & Supervised \& Modeling & Replicate & In* \& Out & 2019 & 1-shot \\
\hline
Shier \textit{\textit{et al.}}~\cite{shier2020spiegelib} & Custom, non-diff & FFT \& Spectral & Supervised \& Direct & Replicate & In & 2020 & Both \\
\hline
Salimi \textit{\textit{et al.}}~\cite{salimi2020make} & DSP & STFT and Envelope & Supervised & Imitate & Out & 2020 & 1-shot \\
\hline
Masuda \textit{\textit{et al.}}~\cite{masuda2021soundmatch} & DDSP & P-Loss initially, fine-tune with Spec. & Supervised & Both & In \& Out & 2021 & 1-shot \\
\hline
Han \textit{\textit{et al.}}~\cite{han2023perceptual} & NN-encoder $\rightarrow$ physical model & PNP (approx. of STFT) & Supervised & Replicate & In & 2023 & 1-shot \\
\hline
Vahidi \textit{\textit{et al.}}~\cite{vahidi2023mesostructures} & Differentiable FM & JTFS/MSS & Gradient desc. & Replicate & In & 2023 & Iter \\
\hline
Barkan \textit{et al.}~\cite{barkan2023inversynthII}& Diff. Synthesizer Proxy& P-Loss + L1 Spec (weighted) &  Supervised (init.), Semi-Supervised & Replicate& In & 2023 & Both
\\
\hline
Uzrad \textit{\textit{et al.}}~\cite{uzrad2024diffmoog} & DDSP & Synth. Chain and P-Loss & Supervised & Replicate & In \& Out & 2024 & 1-shot \\
\hline
Cherep \textit{et al.}~\cite{creativecherep2024} & DDSP & CLAP & Evolutionary & Imitate & Out & 2024 & Iter \\
\hline
\end{tabular}
}

\label{tab:summary}
\end{table*}


\DIFdelbegin \section{\DIFdel{Past Works}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
%DIFDELCMD < \label{sec:related_works}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \subsection{\DIFadd{Historical Framing of Sound-Matching}}
\DIFadd{Given the attributes discussed above, }\DIFaddend Table~\ref{tab:summary} \DIFdelbegin \DIFdel{contains an overview of relevant literature, covering important works in sound-matching. adsfEarlier works in this domain focused on FM synthesis and genetic algorithms (or direct optimization methods)\mbox{%DIFAUXCMD
\cite{justice1979analytic,horner1993machine,mitchell2007evolutionary}}\hskip0pt%DIFAUXCMD
. Along with the exponential increase in computation power, later works utilized a wider range of approaches to parameter search, estimation, and audio synthesis; particularly with supervised learning and neural networks (\gls{NN})~\mbox{%DIFAUXCMD
\cite{yee2018automatic,engel2020ddsp,esling2019flow}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{summarizes relevant literature}\DIFaddend . \DIFaddbegin \DIFadd{Here we expand with a historical analysis of past sound-matching work.
}\DIFaddend 

\DIFdelbegin \subsection{\DIFdel{Earlier Works}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdel{FM synthesizers are simple to implement, yet very expressive~\mbox{%DIFAUXCMD
\cite{chowning1973synthesis}}\hskip0pt%DIFAUXCMD
. }\DIFdelend Perhaps the earliest foundational \DIFdelbegin \DIFdel{work in sound-matching is the analytical approach by }\DIFdelend \DIFaddbegin \DIFadd{study is }\DIFaddend Justice~\cite{justice1979analytic}\DIFdelbegin \DIFdel{toward the decomposition and recreation of sounds with FM synthesis using one carrier, one modulator, and the corresponding envelopes:
}\[ \DIFdel{f(t) = I_c(t) \cos(\omega_c t + I_m(t) \cos(\omega_m t))
}\]%DIFAUXCMD
\DIFdel{Where $f(t)$ is the output signal, $I_c(t)$ is the carrier amplitude, $\omega_c$ is the carrier frequency, and $I_m(t)$ and $\omega_m$ are the envelope and frequency for the modulator~\mbox{%DIFAUXCMD
\cite{justice1979analytic}}\hskip0pt%DIFAUXCMD
.  
}%DIFDELCMD < 

%DIFDELCMD <  %%%
\DIFdel{The sound-matching works that followed used other heuristics for finding the synthesizer parameters, yet the }\DIFdelend \DIFaddbegin \DIFadd{, who analytically decomposed and recreated sounds using a }\DIFaddend simple FM synthesizer \DIFdelbegin \DIFdel{structure remained unchanged. 
}\DIFdelend \DIFaddbegin \DIFadd{(see Section~\ref{sec:dsp}). 
Subsequent work retained this FM structure but explored new heuristics for parameter search. 
For example, }\DIFaddend Horner \textit{et al.}~\cite{horner1993machine} \DIFdelbegin \DIFdel{used GAsfor re-synthesis of sounds with an FM synthesizer using }\DIFdelend \DIFaddbegin \DIFadd{applied genetic algorithms (GAs) to resynthesize sounds with }\DIFaddend one modulator and one carrier\DIFdelbegin \DIFdel{oscillator, and the McAulay-Quatieri }\DIFdelend \DIFaddbegin \DIFadd{, measuring similarity via the McAulay–Quatieri }\DIFaddend method~\cite{mcaulay1986speech}\DIFdelbegin \DIFdel{for measuring loss}\DIFdelend . 


%DIF <  MQ calculates the STFT and marks the start and end of \textit{trends} in a sound with the assumption that the harmonic content belongs to trends that have a beginning and an end. 
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Introduction of more complex models of synthesis }\DIFdelend \DIFaddbegin \DIFadd{Later studies introduced more complex synthesis models, }\DIFaddend such as wavetables~\cite{horner2003auto} and physical modeling~\cite{riionheimo2003parameter}\DIFdelbegin \DIFdel{rendered interesting results, but led to further questions about the nature and goals of sound-matching}\DIFdelend . 
Mitchell and Creasy\DIFdelbegin \DIFdel{noted the difficulty in distinguishing between ``inefficiency of the optimization engine'' and ``synthesizer limitations '' as the cause of failure or success in sound-matching~\mbox{%DIFAUXCMD
\cite{mitchell2007evolutionary}}\hskip0pt%DIFAUXCMD
. 
They offered }\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{mitchell2007evolutionary} }\hskip0pt%DIFAUXCMD
highlighted the difficulty of disentangling synthesizer limitations from optimization inefficiency. 
They proposed }\DIFaddend a \textit{contrived methodology} \DIFdelbegin \DIFdel{for finding the best evolutionary method for sound-matching using their FM synthesizer. The methodology postulates that the best search }\DIFdelend \DIFaddbegin \DIFadd{in which the best }\DIFaddend heuristic for in-domain \DIFdelbegin \DIFdel{search (where an exact target exists, and a wide range of targets can be produced by sampling from different points in the parameter space) would also be the best-performing heuristic for }\DIFdelend \DIFaddbegin \DIFadd{FM resynthesis should also generalize to }\DIFaddend out-of-domain \DIFdelbegin \DIFdel{search on a dataset of }\DIFdelend \DIFaddbegin \DIFadd{targets (e.g., }\DIFaddend muted trumpet tones~\cite{opolko1989mcgill}\DIFdelbegin \DIFdel{; however, testing was not extensive and yielded some contradictory results. As they noted, modifications }\DIFdelend \DIFaddbegin \DIFadd{). 
However, limited testing produced contradictory results, and they concluded that changes }\DIFaddend to the synthesizer, loss\DIFdelbegin \DIFdel{function, and sound domain would lead to a problem with an entirely new }\DIFdelend \DIFaddbegin \DIFadd{, or sound domain effectively redefine the }\DIFaddend search space~\cite{mitchell2007evolutionary}.

%DIF <  Testing sound-matching experiments can be difficult, in part due to the large number of parametric components and navigation of complex non-linear systems. The issue with finding the best approaches to sound-matching is exacerbated when slight changes to the pipeline render previous results moot. Perhaps due to these reasons, the contrived methodology was not extensively tested by Mitchell and Creasy, who only provided a single experiment where the best evolutionary strategy found by in-domain search would also perform well in out-of-domain synthesis.
\DIFdelbegin \subsection{\DIFdel{Introduction of Deep-Learning}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdelend Recent years have seen more works in sound-matching using \DIFaddbegin \DIFadd{supervised }\DIFaddend machine learning techniques. In 2018, Yee-King \textit{et al.} rendered 60,000 audio-parameter pairs from the \textit{Dexed} \gls{VST} synthesizer~\footnote{https://asb2m10.github.io/dexed/}, and showed that NNs trained on this dataset can outperform GA and hill-climber (\gls{HC})~\cite{hoffmann2000heuristic} methods in rendering speed, with slight improvements in MFCC error (used as an objective performance test). The speed improvement appears trivial, considering the iterative nature of GAs when compared to offline training of supervised models. For GAs and HC optimizer, MFCCs were used as a measure of performance as well as a loss function. For training the networks, P-Loss was used, since differentiable MFCCs were not possible in their pipeline. Importantly, informal \DIFdelbegin \DIFdel{hearing tests revealed that the performance of even the best NN model was unsatisfactory, possibly }\DIFdelend \DIFaddbegin \DIFadd{listening tests found the results unsatisfactory, likely }\DIFaddend due to the \DIFdelbegin \DIFdel{complex nature of the synthesizer, which features 155 parameters}\DIFdelend \DIFaddbegin \DIFadd{synthesizer’s 155-parameter complexity}\DIFaddend . 

%DIF <  Programming synthesizers and querying sounds by imitating human vocals has also been a subject of interest~\cite{Cartwright2014SynthAssistQA,siamesevocalimitation2018}. For example, Cartwright and Pardo created a framework for searching pre-made patches for a synthesizer to best imitate vocals using audio features such as pitch, spectral centroid, and loudness~\cite{Cartwright2014SynthAssistQA}. 
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Esling }\textit{\DIFdel{et al.}} %DIFAUXCMD
\DIFdel{noted the issue of using P-Loss in sound-matching~\mbox{%DIFAUXCMD
\cite{esling2019flow}}\hskip0pt%DIFAUXCMD
, considering the non-linear mapping between parameters of a synth and the audio output. In their approach, dubbed as ``flowsynth'', they used a large parameter-to-audio dataset for a commercial \gls{VST} synthesizer. In flowsynth, audio is encoded to an auditory space, using a NN autoencoding approach~\mbox{%DIFAUXCMD
\cite{hinton2006reducing}}\hskip0pt%DIFAUXCMD
. The encodings in the auditory space can then be converted to the parameter space, but only for that particular synthesizer. Using P-Loss and Spectral Convergence (Frobenius norm of the spectrograms) as metrics, flowsynth outperformed other NN approaches that did not use an auditory to parameter space conversion, but formal hearing tests were not conducted.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Masuda \textit{et al.} also applied supervised learning\DIFdelbegin \DIFdel{to synthesizer parameter estimation}\DIFdelend ~\cite{masuda2021soundmatch}. Their work highlights the issue of non-linearity in parameter-to-synthesizer outputs and out-of-domain search. This work uses a differentiable subtractive synthesizer (two oscillators and an LP filter) \DIFdelbegin \DIFdel{. A NN model was }\DIFdelend \DIFaddbegin \DIFadd{alongside a NN model }\DIFaddend pre-trained \DIFdelbegin \DIFdel{using }\DIFdelend \DIFaddbegin \DIFadd{with P-Loss on }\DIFaddend an in-domain dataset of randomly selected parameters\DIFdelbegin \DIFdel{and P-Loss}\DIFdelend . After training, the model was fine-tuned using 20,000 out-of-domain sounds from the NSynth dataset~\cite{engel2017neural} and multi-scale spectrogram loss~\cite{engel2020ddsp}. This approach proved more effective---i.e\DIFaddbegin \DIFadd{.}\DIFaddend , achieved lower multi-level spectral difference in out-of-domain tests---than baseline models, which were either not exposed to out-of-domain sounds or trained exclusively with P-Loss. Subjective hearing tests were conducted, showing a preference for the fine-tuned model~\cite{masuda2021soundmatch}. Masuda \textit{et al.} later extended this work with semi-supervised learning, highlighting significant gaps in in-domain and out-of-domain performance~\cite{masuda2023improving}.

Rather than focusing on a particular implementation, Shier \textit{et al.} presented Spieglib, a library for implementation of sound-matching pipelines~\cite{shier2020spiegelib}. This library provides different choices for DNNs, GAs, synthesizers, and feature extractors. Shier \textit{et al.} presented an experiment with a similar setup to Yee-King \textit{et al.}~\cite{yee2018automatic}, however, they found a genetic algorithm \DIFdelbegin \DIFdel{to be }\DIFdelend \DIFaddbegin \DIFadd{as }\DIFaddend the best performing.

%DIF <  Rather than focusing on a particular implementation, Shier \textit{et al.} presented Spieglib, a library for implementation of sound-matching pipelines (which Shier \textit{et al.} referred to as ``automatic parameter search'')~\cite{shier2020spiegelib}. This library provides extensible choices of deep-learning estimators with \textit{tensorflow}~\cite{abadi2016tensorflow}, genetic estimators with \textit{DEAP}~\cite{DEAP_JMLR2012}, feature extractors with \textit{librosa}~\cite{mcfee2015librosa}, and different choices for synthesizers, evaluators, dataloaders. Shier \textit{et al.} provide an example experiment using this library. This example has a setup similar to Yee-King \textit{et al.}~\cite{yee2018automatic}, where the \textit{Dexed} VST was used and different genetic and NN estimators were compared by using MFCC difference as the minimization objective. Depending on the NN architecture, MFCC or STFT spectrograms were used as the input. Unlike previous comparisons of NN and genetic algorithms, the best estimator was the multi-objective non-dominated sorting genetic algorithm III (NSGA-III). This algorithm had multiple minimization objectives: 13-band MFCC, STFT, and five spectral features. Shier \textit{et al.} report that all models performed relatively well at imitation of harmonic distributions, but the multi-objective function was the only optimizer which did \textit{not} struggle with matching temporal envelopes of the audio spectrum~\cite{shier2020spiegelib}.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Differentiable loss functions that use spectrogram differences can be computationally expensive. To mitigate this, Han \textit{et al.}~\cite{han2023perceptual} introduced ``perceptual-neural-physical loss'' (PNP). PNP is an approximation of loss functions; specifically, a loss function that uses the L2 norm of the difference between the features of two sounds, or $||\phi(t) - \phi(x)||^2_2$, where $\phi$ could be a spectrogram or JTFS function. PNP loss functions are fast and differentiable, but require training and parameter estimation. A Riemannian metric M needs to be calculated for the minimization of locally linear approximation of the ``true" spectral loss function~\cite{han2023perceptual}. 
\DIFaddbegin 

\DIFaddend \[
\| \DIFdelbegin \DIFdel{\phi}\DIFdelend \DIFaddbegin \DIFadd{\varphi}\DIFaddend (\DIFdelbegin \DIFdel{t}\DIFdelend \DIFaddbegin \DIFadd{\xzero}\DIFaddend ) - \DIFdelbegin \DIFdel{\phi}\DIFdelend \DIFaddbegin \DIFadd{\varphi}\DIFaddend (\DIFdelbegin \DIFdel{x}\DIFdelend \DIFaddbegin \DIFadd{\xtheta}\DIFaddend ) \|_2^2
= \langle \tilde{\theta} - \theta \DIFaddbegin \DIFadd{\,}\DIFaddend |\DIFdelbegin \DIFdel{\mathbf{M}}\DIFdelend \DIFaddbegin \DIFadd{\, M}\DIFaddend (\theta) \DIFaddbegin \DIFadd{\,}\DIFaddend |\DIFaddbegin \DIFadd{\, }\DIFaddend \tilde{\theta} - \theta \rangle
+ O(\|\tilde{\theta} - \theta\|\DIFdelbegin \DIFdel{_2^3}\DIFdelend \DIFaddbegin \DIFadd{_3^2}\DIFaddend ).
\DIFdelbegin %DIFDELCMD < \tag{4}
%DIFDELCMD < %%%
\DIFdelend \]

\DIFdelbegin \DIFdel{During the training phase, this }\DIFdelend \DIFaddbegin \DIFadd{This }\DIFaddend metric is calculated alongside the neural parameter estimator. \DIFdelbegin \DIFdel{After training, PNP can be used as a fast approximation of computationally demanding }\DIFdelend \DIFaddbegin \DIFadd{Once trained, PNP enabled the fast differentiable optimization of computationally expensive }\DIFaddend loss functions such as JTFS \DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{han2023perceptual,han2024learning}}\hskip0pt%DIFAUXCMD
, paired with FM synthesizers and a differentiable physical model of a drum head~\mbox{%DIFAUXCMD
\cite{smith2010physical}}\hskip0pt%DIFAUXCMD
. }\DIFdelend \DIFaddbegin \DIFadd{with FM and physical models~\mbox{%DIFAUXCMD
\cite{han2023perceptual,han2024learning}}\hskip0pt%DIFAUXCMD
.
}\DIFaddend 

\DIFaddbegin \DIFadd{A neural approximation for another part of the supervised sound-matching chain---this time the synthesizer---was proposed by Barkan }\textit{\DIFadd{et al.}}\DIFadd{~\mbox{%DIFAUXCMD
\cite{barkan2023inversynthII}}\hskip0pt%DIFAUXCMD
. As they noted, without a differentiable synthesizer, ``model-based'' (or what we call supervised) approaches cannot directly compare $\xtheta$ to $\xzero$, often opting for P-Loss, which may not correctly map the parameters of sound to the output audio~\mbox{%DIFAUXCMD
\cite{esling2019flow,han2023perceptual,masuda2023improving}}\hskip0pt%DIFAUXCMD
. This approach used a model mapping sounds to parameters, another approximating the synthesizer, and a loss function which combines P-Loss with STFT differences. This ``Inversynth II'' (IS2) approach yielded significant improvements to previous works which did not use the STFT approximations for loss~\mbox{%DIFAUXCMD
\cite{esling2019flow,barkan2019inversynth}}\hskip0pt%DIFAUXCMD
. Barkan }\textit{\DIFadd{et al.}} \DIFadd{then attempted to improve the IS2 model with Inference-Time
Fine-tuning (ITF). For ITF, the synthesizer approximation is frozen and the encoder is iteratively fine-tuned for a particular sample. However, ITF often degraded performance, possibly due to proxy–synthesizer mismatch or overfitting.
}

\DIFaddend Uzrad \textit{et al.}~\cite{uzrad2024diffmoog} took another unique approach to sound-matching: using a differentiable \textit{synthesis chain} of DSP generators and effects and a loss function that combines P-Loss with a \textit{signal-chain loss}. The synthesizer is a customizable chain of effects, which feed one output as input to the next step of the chain; signal-chain loss compares the parameter and output difference at every output step in the chain~\cite{uzrad2024diffmoog}. Possible chain functionalities are FM/AM, Low-Frequency Oscillators (\gls{LFO}), filters, and envelopes. Like the results shown by Masuda \textit{et al.}~\cite{masuda2021soundmatch}, better out-of-domain results were achieved when pre-trained on in-domain data and fine-tuned using out-of-domain NSynth data~\cite{engel2017neural}.


 %DIF <  Uzrad \textit{et al.}\cite{uzrad2024diffmoog} used log-spectral distance (LSD) as their loss function, where   
%DIF <  $\text{LSD} = \| \log(S(t)) - \log(S(x)) \|_F$, S is the mel-spectrogram of the sounds and F is the Frobenius norm~\cite{golub2013matrix}. The Frobenius norm of a matrix $A$ is defined as
%DIF <  $ \|A\|_F = \sqrt{\sum_{i,j} |a_{ij}|^2}$. Here, $A$ would be the 2D matrix resulting from subtracting two log-spectrograms. Along with other works, \textbf{non-convexity of loss surfaces when applying gradient optimization} is a noted issue~\cite{turian2020sorry,vahidi2023mesostructures,uzrad2024diffmoog}. 
\DIFaddbegin \DIFadd{Audio embeddings can also be used as a similarity metric. In a recent work, Cherep }\textit{\DIFadd{et al.}} \DIFadd{used latent representations from the CLAP model~\mbox{%DIFAUXCMD
\cite{wu2023large} }\hskip0pt%DIFAUXCMD
along with a differentiable synthesizer~\mbox{%DIFAUXCMD
\cite{synthhaxcherep2023} }\hskip0pt%DIFAUXCMD
to create creative interpretations of sound effects. In their approach, a desired sound-effect is described in text and embedded using CLAP, followed by iterative updates from a gradient-free optimizer~\mbox{%DIFAUXCMD
\cite{evosax2022github} }\hskip0pt%DIFAUXCMD
to the synthesizer's parameters to minimize embedding differences. Based on manual hearing tests, this approach did not produce the ``correct'' sounds more frequently than previous works~\mbox{%DIFAUXCMD
\cite{kreuk2022audiogen}}\hskip0pt%DIFAUXCMD
. However, it did yield better scores for ``artistic-interpretation'' (or what this works refers to as ``imitation'').
}\DIFaddend 

%DIF >  masuda and esling both noted out-of-domain drop in performance
%DIF >  yee king noted performance drop in manual surveys 
%DIF > p-loss bad : masuda-improving, han2023, esling-flowsynth
%DIF >  manual performance measure: barkan, masuda2021soundmatch, yee-king 
\DIFaddbegin 

\DIFaddend \subsection{What is Lacking In the Field}
\label{sec:lacking}
%DIF <    % First, we need to clearly define the characteristics we are looking for with non-generic loss functions, otherwise the chances of finding the sound(s) of interest is slim. Second, we need to move beyond direct replication, and focus on imitating target sounds that are outside the synthesizers capabilities. Third, more complex methods of synthesis need to be explored for iterative sound-matching. Fourth, we need to find optimization methods that can effectively navigate the the sinusoidal nature of loss function landscapes. 
%DIF > -----> much of the original text (available in older version) is removed for brevity)

%DIF <  Based on the literature reviewed, we find several gaps in past research that makes the application of sound-matching for sound-designers difficult. Thus far, replication of in-domain sounds has been the main subject of research. With in-domain replication, we already have the target sound \textit{and} the program that makes it. This simplifies the problem of measuring performance and is the reason why this type of sound-matching research is by far the most common. However, practical application of sound-matching requires answers to much harder problems. Sound-matching would be an interesting creative tool if we could use it to either:
%DIF <  \begin{enumerate}
%DIF <   \item \textbf{Replicate out-of-domain sounds}: this would give us the parameters that could closely approximate a target sound, and we can modify the sound further by modulating the synthesizer. 
%DIF <  \item \textbf{Imitate in-domain or out-of-domain sounds}: This would allow for applying the characteristics of one sound to another. 
%DIF <  \end{enumerate}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  We need to be aware of the ``open set recognition'' (\gls{OSR}) problem here \cite{mundt2019open,gers2000learning}. OSR is a well documented issue in DL that manifests when the set or category that the network is supposed to reject or accept cannot be explained via examples. With imitation or replication of out-of-domain sounds, we can expect issues related to OSR to occur, since target and output sounds would be from different domains~\cite{salimi2021percussive}. 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  Perhaps one method of addressing the OSR problem is the use of representations (or feature extraction) methods that reduce sounds to a feature vector in the same domain. For example, a representation that only looks at the envelope of sounds is far less likely to be subject to the OSR problem. \textcolor{highlight}{We highlight the lack of non-generic loss functions as a major weakness in previous works and an interesting area to explore}. 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  Another weakness \textcolor{highlight}{is that the synthesis methods in past works are not representative of the common methods used by sound designers and modern synthesizers}. Simple FM/AM synthesizers have generally been the go-to methods of sound-matching. We believe that general statements regarding the effectiveness of an approach to iterative sound-matching (particularly in regards to a loss function) can only be reasonably made in the context of the subset of the tested methods of synthesis.
%DIF <  \textcolor{highlight}{The simplicity of parameter optimization approaches} is another area of weakness in past works. Classic genetic algorithms update the parameters randomly, while in recent differentiable settings, simple gradient descent methods are used. Past works have provided issues which arise in differentiable parameter estimation due to periodic loss functions. Perhaps the use of reinforcement learning (\gls{RL}), or other algorithms that continually learn from their environment, could help with this problem.
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  \subsection{Defining The Major Issues}
%DIF <  \label{sec:addressing_problems}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Having looked at the current \DIFdelbegin \DIFdel{available }\DIFdelend literature, we \DIFdelbegin \DIFdel{note }\DIFdelend \DIFaddbegin \DIFadd{identify }\DIFaddend four major areas of weakness in sound-matching. We target the first two issues in this work, while the latter two issues are left for future work.
\begin{enumerate}
    \item \LossSelect: \DIFdelbegin \DIFdel{Is there a }\textit{\DIFdel{best performing}} %DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{It remains unclear whether there exists a universally best-performing }\DIFaddend loss function, 
    or \DIFdelbegin \DIFdel{is the selection of the loss function dependent }\DIFdelend \DIFaddbegin \DIFadd{whether performance depends }\DIFaddend on factors such as the sound domain, synthesizer \DIFdelbegin \DIFdel{capability and the desired characteristics of the output ? 
    }\DIFdelend \DIFaddbegin \DIFadd{architecture, 
    and desired output characteristics. Existing studies typically evaluate losses only with simple synthesis 
    setups~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures}}\hskip0pt%DIFAUXCMD
, leaving open questions about their generality. 
    }\DIFaddend \item \SynthSelect: There is a lack of diversity in the synthesis methods used in sound-matching. The \DIFdelbegin \DIFdel{use of different DSP functions for }\DIFdelend \DIFaddbegin \DIFadd{effects of using different isolated DSP functions in }\DIFaddend iterative sound-matching and \DIFdelbegin \DIFdel{their effects on the outcome or the interactions with the loss function have not been tested}\DIFdelend \DIFaddbegin \DIFadd{how these interact with loss functions remains largely untested}\DIFaddend .
    \item \PeriodicLoss: \DIFdelbegin \DIFdel{The sinusoidal nature of loss function landscapes is a problem that appears frequently}\DIFdelend \DIFaddbegin \DIFadd{Loss function landscapes are not easy to navigate, as there can be many local minima, or large flat areas~\mbox{%DIFAUXCMD
\cite{turian2020sorry,vahidi2023mesostructures}}\hskip0pt%DIFAUXCMD
}\DIFaddend .  In differentiable settings, this \DIFdelbegin \DIFdel{problem causes }\DIFdelend \DIFaddbegin \DIFadd{can cause }\DIFaddend gradient descent updates to \DIFdelbegin \DIFdel{local }\DIFdelend \DIFaddbegin \DIFadd{not reach the global }\DIFaddend minima. 
    \item \OutDomain: Sound-matching with out-of-domain sounds is \DIFdelbegin \DIFdel{an }\DIFdelend \DIFaddbegin \DIFadd{a largely }\DIFaddend unexplored area, yet a necessary one for practical applications for sound designers.
\end{enumerate}

\DIFdelbegin \section{\DIFdel{Programs, Methodology, and Results}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
%DIFDELCMD < \label{sec:progs_methodology_results}
%DIFDELCMD < %%%
\DIFdel{Towards addressing }\DIFdelend \DIFaddbegin \subsection{\DIFadd{What Approach Should We Take?}}
\DIFadd{We argue that isolated benchmarking is a necessary precursor to building robust, generalizable sound-matching systems. To address }\DIFaddend the problems of \LossSelect{} and \SynthSelect{}, we \DIFdelbegin \DIFdel{compare the performance of iterative sound-matching experiments, where four differentiable loss functions are applied to four differentiable synthesizers. What we mean by ``performance'' is the similarity of the final output to the target, as measured by P-Loss, MSS, and---most importantly---manual listening tests.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\textbf{\DIFdel{We hypothesize that the relative performance of loss functions for sound-matching is influenced by the method of synthesis}}%DIFAUXCMD
\DIFdel{. If true, this would strengthen the idea that the selection of the loss function and synthesis method is not an optimization problem, but a subjective choice depending on the needs of the sound designer}\DIFdelend \DIFaddbegin \DIFadd{adopt a methodology with three key characteristics that have not previously appeared together}\DIFaddend .

%DIF <  In Section~\ref{sec:methodology} we lay out the experiment methodology. Section~\ref{sec:loss_implementation} discusses the details of implementation for the loss functions. Section~\ref{sec:programs} covers the characteristics of the programs, and finally Section~\ref{sec:results_posthoc} shows that the effect of loss function choice on the success of iterative sound-matching is dependent on the signal process chain of interest. 
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\subsection{\DIFdel{Experiment Methodology}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < \label{sec:methodology}
%DIFDELCMD < %%%
%DIF <  We have highlighted the lack of attention given to the selection of loss functions in sound-matching, and noted that the few existing works addressing this issue evaluate their proposed losses using only simplistic sound synthesis methods. \textcolor{highlight}{We hypothesize that the best performing loss function is influenced by the synthesis techniques utilized in the synthesizer}. This hypothesis targets the issues of \LossSelect~and \SynthSelect. 
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{We test four different loss functions and compare their success in sound-matching using four different synthesizers . We verify the success of the sound-matching process using two automatic measures of sound similarity (MSS and P-Loss) and a manual blinded hearing test conducted by two of the authors where Likert scores~\mbox{%DIFAUXCMD
\cite{jebb2021review} }\hskip0pt%DIFAUXCMD
of 1-5 are assigned to a subset of final outputs and the corresponding targets.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{An }\textit{\DIFdel{experiment}} %DIFAUXCMD
\DIFdel{is one iteration of sound-matching given a loss function, target sound, and synthesizer program. When the final iteration is complete, a score is given (automatically with an algorithm or with manual listening) to an experiment that measures its success in replicating the target sound. Consider the following simplified scenario: we have two loss functions, $L_1$ and $L_2$, and a single synthesizer program $P$ with a target parameter set $\theta^*$ where the target sound is $t$, or $P(\theta^*) = t$. We run a large number (in this case, 300 for each loss function) of }\DIFdelend \DIFaddbegin \begin{enumerate}
    \item \DIFadd{Use simple differentiable synthesizers built from common DSP functions (oscillators, filters, envelopes) to address the \SynthSelect{} problem. Restriction to small-scale, low-parameter models avoids the confounding effects of neural proxies or embedding models, enabling direct analysis of how synthesis method interacts with loss functions. 
    }\item \DIFadd{Evaluate experiments under multiple loss functions to address the \LossSelect{} problem. Prior work has typically tested losses in isolation, making direct comparisons of loss functions difficult~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures,han2023perceptual,uzrad2024diffmoog}}\hskip0pt%DIFAUXCMD
. Particularly rare are comparisons which are done in the context of different methods of synthesis.  
    }\item \DIFadd{Optimize synthesizer parameters directly with gradient descent, without neural approximations. This complements prior proxy-based methods by providing the missing “low-level }\DIFaddend experiments\DIFdelbegin \DIFdel{where we randomly set the initial ($\theta_0$) and target parameters ($\theta^*$) of $P$ and run an iterative sound-matching loop for each loss function, with maximum number of iterations set to 200. When the loop is terminated, we measure the similarity of the final output sound $x$ (where $P(\theta_{199}) = x$) to $t$, or if using P-Loss we compare $\theta_{199}$ to $\theta^*$. Having 300 experiments gives us a distribution of losses to determine whether $L_1$ or $L_2$ led to better performance. This simplified example leads us to our methodology which has four loss functions, four synthesizer programs, and three different evaluation methods of scoring sound similarity.
}\DIFdelend \DIFaddbegin \DIFadd{” that clarify fundamental interactions between DSP functions and similarity measures. 
}\end{enumerate}
\DIFaddend 

\DIFdelbegin \subsubsection{\DIFdel{Automatic and Manual Evaluation}}
%DIFAUXCMD
\addtocounter{subsubsection}{-1}%DIFAUXCMD
%DIFDELCMD < \label{sec:evaluation_manual_auto}
%DIFDELCMD < %%%
\DIFdel{The two automatic evaluation methods used here are P-Loss and MSS, as described in Section~\ref{sec:loss_funcs}. For P-Loss, the parameters are normalized between 0-1 based on the valid ranges defined in the Faust program, and the L1 distance between the normalized parameters is calculated. MSS is computed using a hop size of $100$ samples, and FFT window lengths of $(512, 1024, 2048, 4096)$}\DIFdelend \DIFaddbegin \DIFadd{Together, these design choices supply the controlled experimental evidence missing from the literature, 
clarifying how losses and synthesis methods interact in differentiable, iterative settings, 
and offering insights likely to generalize to more complex domains}\DIFaddend .




\DIFdelbegin \DIFdel{Automatic sound evaluation methods are often unreliable, as their alignment with the subjective judgments of human listeners is uncertain.  
    Therefore, listening tests are conducted by randomly sampling 40 experiment results for each program and loss function pair. Two of the authors then assign blinded similarity scores to the outputs and targets using a 5-point Likert scale (1 = no similarity , 5 = near identical)~\mbox{%DIFAUXCMD
\cite{jebb2021review}}\hskip0pt%DIFAUXCMD
. 
Using Spearman's rank correlation~\mbox{%DIFAUXCMD
\cite{spearman1987proof,rebekic2015pearson}}\hskip0pt%DIFAUXCMD
, we found a very strong rate of agreement between the human listeners (see Section~\ref{sec:consistency_in_rankings}), a good indicator that the scores assigned by manual listeners can be treated as the ground truth. We then combine the ranks assigned by both authors, 
giving us 80 ranks per program and loss function pair. 
}\DIFdelend %DIF >  4. Past synths are either simple differentiable functions (commonly FM), or complex VSTs where we can't isolate the interactions between loss and functions, here we use isolated building blocks of DSP functions in a differentiable environment. 


%DIF <  Agreement between the assigned scores is measured using Spearman’s rank correlation, which provides both a correlation coefficient ($\rho$) ranging from –1 (perfect negative correlation) to 1 (perfect positive correlation), and a p-value testing the null hypothesis of no correlation~\cite{spearman1987proof,rebekic2015pearson}. Across all programs, the correlation was very strong ($\rho = 0.86$, $p < 10^{-180}$). Per-program correlations were also very strong: $\rho = 0.71$ for \BPNoise{} ($p < 10^{-25}$), $\rho = 0.64$ for \AddSineSaw{} ($p < 10^{-19}$), $\rho = 0.84$ for \AmpMod{} ($p < 10^{-43}$), and $\rho = 0.85$ for \FMMod{} ($p < 10^{-44}$). This high rate of agreement is a good indicator that the scores assigned by manual listeners can be treated as the ground truth. 
%DIF >   % We have highlighted the lack of attention given to the selection of loss functions in sound-matching, and noted that the few existing works addressing this issue evaluate their proposed losses using only simplistic sound synthesis methods. We hypothesize that the best performing loss function is influenced by the synthesis techniques utilized in the synthesizer. This hypothesis targets the issues of \LossSelect~and \SynthSelect.

%DIF <  For each program and loss function, the blind manual hearing test gives us 80 ranks, which are given by two of the authors to a set of 40 randomly selected experiment results. This gives 160 pairs of ranks per program, and 640 for all programs. We computed Spearman correlations between the two reviewers.
%DIF >  Section IV titled "Programs, Methodology, and Results" is overloaded and blends experimental setup with results and interpretation. For clear clarity and alignment with structural norms, we recommend splitting this into three sections: (1) Experimental Setup (including synthesizer programs, loss definitions, optimization procedure); (2) Experimental Results (performance metrics, statistical tests, and rankings); and (3) Discussion (qualitative interpretation, synthesis-specific observations, and practical implications).

\DIFdelbegin \subsubsection{\DIFdel{Determining Best Performers}}
 %DIFAUXCMD
\addtocounter{subsubsection}{-1}%DIFAUXCMD
\DIFdel{Figure~\ref{fig:posthoc_evaluation} is a visualized summary of how the loss functions are ranked. We have four loss functions and four synthesizer programs. The maximum number of iterations set to 200, and 300 experiments for each loss}\DIFdelend \DIFaddbegin \section{\DIFadd{Experimental Setup and Results}}
\label{sec:experiment_setup}
\DIFadd{At a glance, our methodology is to conduct controlled, low-level differentiable experiments without the use of proxy networks. We pair four differentiable losses with four differentiable DSP-based synthesizers: BP-Noise (band-pass noise with LP}\DIFaddend /\DIFdelbegin \DIFdel{synthesizer combination. Each synthesizer program is paired with four loss functions and 300 experiments are conducted and evaluated automatically with }\DIFdelend \DIFaddbegin \DIFadd{HP filters), Add-SineSaw (additive sine and saw oscillators), Noise-AM (noise modulated by an LFO), and SineSaw-AM (sine-modulated saw oscillator). We optimize parameters directly via gradients in order to isolate loss–synthesizer interactions. This controlled setup systematically evaluates the performance of iterative sound-matching pipelines, where “performance” is defined as the similarity of the synthesized output to a target. Similarity is assessed with }\DIFaddend P-Loss\DIFdelbegin \DIFdel{and MSS, and manually with Likert scores.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{For each program and loss function pair, we have three distributions that can be used to rank the loss functions. Two with 300 automatically assigned similarities (P-Loss and MSS)and one with 80 Likert scores (combining the 40 ranks assigned by each author). For consistency and statistical robustness, the distributions are upsampled to 1000 values using bootstrapping~\mbox{%DIFAUXCMD
\cite{tibshirani1993introduction}}\hskip0pt%DIFAUXCMD
. Bootstrapping gives an estimation of the distribution for the mean performance of each experiment. $k$ (set to 1000) samples of $n$ values (set to 100\% of the values) are taken with replacement from the empirical distribution (list of 300 performance values for MSSand P-Loss or 80 for manual rankings). The mean performance is calculated for each of the $k$ samples. The $k$ estimates of the mean performance give us a bootstrapped distribution for each loss function }\DIFdelend \DIFaddbegin \DIFadd{, MSS, and—most importantly—manual listening tests. This design allows us to directly test our central hypothesis: that the effectiveness of a loss function depends on the synthesis method, and that no universal “best” similarity measure exists}\DIFaddend .

\DIFdelbegin \DIFdel{For each program, the loss functions are ranked using the non-parametric Scott-Knott (\gls{NPSK}) test~\mbox{%DIFAUXCMD
\cite{tantithamthavorn2017mvt,tantithamthavorn2018optimization}}\hskip0pt%DIFAUXCMD
, which ranks the loss functions from rank 1 (best, or highest distribution values)
to maximum rank of 4 (worst, with lowest distribution values). NPSK may determine that multiple functions belong to the same rank, in which case the maximum rank would be less than 4. As mentioned before, lower P-Loss and MSS values indicate higher similarity, therefore (for consistency with manually assigned ranks) the reciprocal of their output is used ($x^{-1}$, if $x$ is the P-Loss or MSS value), where higher values indicate greater similarity. 
}\DIFdelend \DIFaddbegin \DIFadd{Here we discuss the implementation of the four loss functions (Section~\ref{sec:loss_implementation})
across four differentiable synthesizers (Section~\ref{sec:programs}). For
each loss–synthesizer pair, we run a large number of trials and evaluate
their outcomes automatically and manually
(Section~\ref{sec:evaluation_manual_auto}). ``Experiments'' in this context
refer to one complete iterative sound-matching run, from random parameter
initialization through gradient-based optimization until termination. Scores
from these experiments are then aggregated and statistically compared to
determine best-performing losses and synth–loss pairings.
}\DIFaddend 



\DIFdelbegin %DIFDELCMD < \begin{figure*}
%DIFDELCMD < \resizebox{\linewidth}{!}{ % Automatically scales diagram to fit page width
%DIFDELCMD < \begin{tikzpicture}[
%DIFDELCMD <     function/.style={rectangle, draw, rounded corners, minimum width=2cm, minimum height=1cm, text centered, node distance=1cm},
%DIFDELCMD <     distribution/.style={ellipse, draw, minimum width=1cm, minimum height=0cm, text centered, node distance=0.1cm},
%DIFDELCMD <     gaussian/.style={draw, smooth, samples=100, domain=-2.5:2.5},
%DIFDELCMD <     NPSK/.style={rectangle, draw, rounded corners, minimum width=2.5cm, minimum height=1cm, text centered, node distance=1.5cm, fill=blue!20},
%DIFDELCMD <     level 1/.style={sibling distance=6cm, level distance=1cm},
%DIFDELCMD <     level 2/.style={sibling distance=2cm, level distance=1.2cm},
%DIFDELCMD <     mss/.style={fill=red!50, draw=black},
%DIFDELCMD <     ploss/.style={fill=blue!50, draw=black},
%DIFDELCMD <     likert/.style={fill=green!40, draw=black}
%DIFDELCMD < ]
%DIFDELCMD < 

%DIFDELCMD < % Existing functions and distributions diagram
%DIFDELCMD < \node[function] {Synthesizer Program}
%DIFDELCMD <     child { node[function] {DTW\_Onset}
%DIFDELCMD <         child { node[distribution, mss] {MSS}
%DIFDELCMD <             child[grow=down, level distance=1cm] {
%DIFDELCMD <                 node[draw=none] {
%DIFDELCMD <                     \begin{tikzpicture}[scale=0.7]
%DIFDELCMD <                         \draw[red] plot (\x/3, {exp(-\x*\x)});  % Red Gaussian
%DIFDELCMD <                     }
%DIFDELCMD <                 }
%DIFDELCMD <             }
%DIFDELCMD <         }
%DIFDELCMD <         child { node[distribution, ploss] {P-Loss}
%DIFDELCMD <             child[grow=down, level distance=1cm] {
%DIFDELCMD <                 node[draw=none] {
%DIFDELCMD <                     \PICTUREBLOCKtikzpicture{[scale=0.7]
%DIFDELCMD <                         \draw[blue] plot (\x/3, {exp(-\x*\x)});  % Blue Gaussian
%DIFDELCMD <                     }
%DIFDELCMD <                 }
%DIFDELCMD <             }
%DIFDELCMD <         }
%DIFDELCMD <         child { node[distribution, likert] {Likert}
%DIFDELCMD <             child[grow=down, level distance=1cm] {
%DIFDELCMD <                 node[draw=none] {
%DIFDELCMD <                     \PICTUREBLOCKtikzpicture{[scale=0.7]
%DIFDELCMD <                         \draw[green] plot (\x/3, {exp(-\x*\x)});  % Green Gaussian
%DIFDELCMD <                     }
%DIFDELCMD <                 }
%DIFDELCMD <             }
%DIFDELCMD <         }
%DIFDELCMD <     \end{tikzpicture}
%DIFDELCMD <     child { node[function] {L1\_Spec}
%DIFDELCMD <         child { node[distribution, mss] {MSS} }
%DIFDELCMD <         child { node[distribution, ploss] {P-Loss} }
%DIFDELCMD <         child { node[distribution, likert] {Likert} }
%DIFDELCMD <     }
%DIFDELCMD <     child { node[function] {SIMSE\_Spec}
%DIFDELCMD <         child { node[distribution, mss] {MSS} }
%DIFDELCMD <         child { node[distribution, ploss] {P-Loss} }
%DIFDELCMD <         child { node[distribution, likert] {Likert} }
%DIFDELCMD <     }
%DIFDELCMD <     child { node[function] {JTFS}
%DIFDELCMD <         child { node[distribution, mss] {MSS}
%DIFDELCMD <             child[grow=down, level distance=1cm] {
%DIFDELCMD <                 node[draw=none] {
%DIFDELCMD <                     \begin{tikzpicture}[scale=0.7]
%DIFDELCMD <                         \draw[red] plot (\x/3, {exp(-\x*\x)});  % Red Gaussian
%DIFDELCMD <                     \end{tikzpicture}
%DIFDELCMD <                 }
%DIFDELCMD <             }
%DIFDELCMD <         }
%DIFDELCMD <         child { node[distribution, ploss] {P-Loss}
%DIFDELCMD <             child[grow=down, level distance=1cm] {
%DIFDELCMD <                 node[draw=none] {
%DIFDELCMD <                     \begin{tikzpicture}[scale=0.7]
%DIFDELCMD <                         \draw[blue] plot (\x/3, {exp(-\x*\x)});  % Blue Gaussian
%DIFDELCMD <                     \end{tikzpicture}
%DIFDELCMD <                 }
%DIFDELCMD <             }
%DIFDELCMD <         }
%DIFDELCMD <         child { node[distribution, likert] {Likert}
%DIFDELCMD <             child[grow=down, level distance=1cm] {
%DIFDELCMD <                 node[draw=none] {
%DIFDELCMD <                     \begin{tikzpicture}[scale=0.7]
%DIFDELCMD <                         \draw[green] plot (\x/3, {exp(-\x*\x)});  % Green Gaussian
%DIFDELCMD <                     \end{tikzpicture}
%DIFDELCMD <                 }
%DIFDELCMD <             }
%DIFDELCMD <         }
%DIFDELCMD <     };
%DIFDELCMD < 

%DIFDELCMD < % Label for the second row (Distributions)
%DIFDELCMD < \node at (0,-3.25) {\textbf{$\longleftarrow$\ \ \large Bootstrapped Distributions\ \ $\longrightarrow$}};
%DIFDELCMD < % Left side: NPSK with MSS
%DIFDELCMD < 

%DIFDELCMD < \def\xoffsetL{-0.5}
%DIFDELCMD < \def\xoffsetM{0}
%DIFDELCMD < \def\xoffsetR{0.5}
%DIFDELCMD < \def\yoffset{1}
%DIFDELCMD < 

%DIFDELCMD < % Left: NPSK with MSS
%DIFDELCMD < \node[NPSK, text height=5ex, align=center, fill=red!30, draw=black] at ({-6+\xoffsetL},{-6+\yoffset}) {NPSK with 4 \\ MSS distributions};
%DIFDELCMD < \draw[->] ({-6+\xoffsetL},{-6.5+\yoffset}) -- ({-6+\xoffsetL},{-6.6+\yoffset}) node[below, below] {\textbf{Ranks 1-4}};
%DIFDELCMD < \draw[red!40, thick] ({-5+\xoffsetL}, {-5.4+\yoffset}) -- ({-2.95+\xoffsetL}, {-5.15+\yoffset});  % Line 1
%DIFDELCMD < \draw[red!40, thick] ({-5.5+\xoffsetL}, {-5.4+\yoffset}) -- ({-3.5+\xoffsetL}, {-4.75+\yoffset});  % Line 2
%DIFDELCMD < \draw[red!40, thick] ({-6+\xoffsetL}, {-5.4+\yoffset}) -- ({-5.5+\xoffsetL}, {-4.75+\yoffset});  % Line 3
%DIFDELCMD < \draw[red!40, thick] ({-6.5+\xoffsetL}, {-5.4+\yoffset}) -- ({-9+\xoffsetL}, {-4.75+\yoffset});  % Line 4
%DIFDELCMD < 

%DIFDELCMD < % Middle: NPSK with P-Loss
%DIFDELCMD < \node[NPSK, text height=5ex, align=center, fill=blue!40, draw=black] at ({0+\xoffsetM},{-6+\yoffset}) {NPSK with 4 \\ P-Loss distributions};
%DIFDELCMD < \draw[->] ({0+\xoffsetM},{-6.5+\yoffset}) -- ({0+\xoffsetM},{-6.6+\yoffset}) node[below, below] {\textbf{Ranks 1-4}};
%DIFDELCMD < \draw[blue!40, thick] ({1.5+\xoffsetM}, {-5.4+\yoffset}) -- ({3.25+\xoffsetM}, {-4.75+\yoffset});  % Line 1
%DIFDELCMD < \draw[blue!40, thick] ({0.75+\xoffsetM}, {-5.4+\yoffset}) -- ({1.5+\xoffsetM}, {-4.75+\yoffset});  % Line 2
%DIFDELCMD < \draw[blue!40, thick] ({-0.25+\xoffsetM}, {-5.4+\yoffset}) -- ({-1+\xoffsetM}, {-4.75+\yoffset});  % Line 3
%DIFDELCMD < \draw[blue!40, thick] ({-0.75+\xoffsetM}, {-5.4+\yoffset}) -- ({-3+\xoffsetM}, {-4.75+\yoffset});  % Line 4
%DIFDELCMD < 

%DIFDELCMD < % Right: NPSK with Likert
%DIFDELCMD < \node[NPSK, text height=5ex, align=center, fill=green!30, draw=black] at ({6+\xoffsetR},{-6+\yoffset}) {NPSK with 4 \\ Likert distributions};
%DIFDELCMD < \draw[->] ({6+\xoffsetR},{-6.5+\yoffset}) -- ({6+\xoffsetR},{-6.6+\yoffset}) node[below, below] {\textbf{Ranks 1-4}};
%DIFDELCMD < \draw[green!50, thick] ({7.5+\xoffsetR}, {-5.4+\yoffset}) -- ({8.75+\xoffsetR}, {-5+\yoffset});  % Line 1
%DIFDELCMD < \draw[green!50, thick] ({6.75+\xoffsetR}, {-5.4+\yoffset}) -- ({6+\xoffsetR}, {-4.75+\yoffset});    % Line 2
%DIFDELCMD < \draw[green!50, thick] ({5.75+\xoffsetR}, {-5.4+\yoffset}) -- ({4+\xoffsetR}, {-4.85+\yoffset});    % Line 3
%DIFDELCMD < \draw[green!50, thick] ({4.95+\xoffsetR}, {-5.4+\yoffset}) -- ({3.25+\xoffsetR}, {-5.1+\yoffset}); % Line 4
%DIFDELCMD < 

%DIFDELCMD < \end{tikzpicture}
%DIFDELCMD < }
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{For each synthesizer program, we assign ranks to the four loss functions  (}%DIFDELCMD < \DTWEnv%%%
\DIFdelFL{, }%DIFDELCMD < \LoneSpec%%%
\DIFdelFL{, }%DIFDELCMD < \SIMSESpec%%%
\DIFdelFL{, \JTFS) in three different ways, using MSS, P-Loss, or manually assigned Likert scores.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:posthoc_evaluation}
%DIFDELCMD < \end{figure*}
%DIFDELCMD < 

%DIFDELCMD < %%%
\subsubsection{\DIFdel{Training Parameters and Gradient Calculations}}
 %DIFAUXCMD
\addtocounter{subsubsection}{-1}%DIFAUXCMD
%DIF <  Previous work utilized gradient fields for analysis of the algorithm performance~\cite{vahidi2023mesostructures}. Gradient fields are not the center of our analysis, but we include them in Appendix~\ref{appendix:gradient_fields}.
 \DIFdel{For
updating the synthesizer parameters (or weights), we use RMSProp, which operates similar to stochastic gradient descent (SGD)~\mbox{%DIFAUXCMD
\cite{goodfellow2016deep}}\hskip0pt%DIFAUXCMD
, with the caveat that the gradients of each weight are scaled by the root-mean-square of past gradients of that weight. Based on some initial test runs, we used an arbitrary fixed learning rate of 0.045 for all experiments (learning rates used in Vahidi }\textit{\DIFdel{et al.}}%DIFAUXCMD
\DIFdel{~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures} }\hskip0pt%DIFAUXCMD
is unknown). The maximum number of iterations is set to 200, where, based on our observations, the parameters have either irrecoverably diverged outside the acceptable ranges, or are stuck at a local minimum. Additionally, the gradients are large, and calculated backwards sequentially through the signal processing chain. This backward calculation resembles the }\textit{\DIFdel{exploding gradients}} %DIFAUXCMD
\DIFdel{problem in recurrent neural networks~\mbox{%DIFAUXCMD
\cite{gers2000learning}}\hskip0pt%DIFAUXCMD
. Gradient clipping~\mbox{%DIFAUXCMD
\cite{goodfellow2016deep} }\hskip0pt%DIFAUXCMD
is preemptively used to
ensure that the  $\ell_2$ norm of all gradients does not exceed the threshold of 1. 
%DIF <  \mathbf{\gamma}_i^{\text{clipped}} = \mathbf{\gamma}_i \cdot \mathcal{S}
%DIF <  \]
%DIF <  Where $\mathcal{S}$ is a value less than or equal to 1, depending on whether the norm of the gradients exceeds the threshold $c$.  
%DIF <  \[
%DIF <  \mathcal{S} = \frac{c}{\max(\|\mathbf{\gamma}\|_2, c)}
%DIF <  \]
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \subsection{Loss Function Implementation Details}
\label{sec:loss_implementation}
\subsubsection{STFT Losses}
Due to their ubiquity and lower cost of gradient calculation, we use STFT as the basis of two loss functions. 
\DIFdelbegin \DIFdel{As discussed in Section~\ref{sec:matching_types}, the }\DIFdelend L1 or L2 distances are the most common methods of comparing \DIFdelbegin \DIFdel{different spectrograms, however, there are situations where this might lead to shortcomings. An example given by Vahidi }\textit{\DIFdel{et al.}}%DIFAUXCMD
\DIFdel{~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures} }\hskip0pt%DIFAUXCMD
is two chirplets (tones that increase in pitch) starting and ending at the same frequencies. 
If one of these tones is slightly shifted in time, then the spectrograms will no longer overlap at any point, despite their sonic similarity (similar to the idea that a linear equation $f(x) = ax$ can never overlap with a slightly shifted version of itself, or $f(x) = ax +\epsilon$). 
In thisexample, we do not know if this is an issue of the spectrograms, or the L1 measure used for their comparison. In fields such as computer vision, Structural Similarity Index (SSIM)~\mbox{%DIFAUXCMD
\cite{wang2004imagesssim,wang2009mean} }\hskip0pt%DIFAUXCMD
and }\DIFdelend \DIFaddbegin \DIFadd{spectrograms~\mbox{%DIFAUXCMD
\cite{turian2020sorry,richard2025model}}\hskip0pt%DIFAUXCMD
. 
However, these measures can be overly sensitive to global gain or minor misalignments, potentially overstating perceptual differences. 
To address this, we also test }\DIFaddend Scale-Invariant Mean Squared Error (SIMSE) \DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{barron2014shapessimse} }\hskip0pt%DIFAUXCMD
have been used as alternatives that improved performance in various tasks relative }\DIFdelend \DIFaddbegin \DIFadd{as an alternative }\DIFaddend to L1 \DIFaddbegin \DIFadd{for comparing STFT spectrograms}\DIFaddend .


We define the \LoneSpec~and \SIMSESpec~functions as the application of L1 and SIMSE to the STFT spectrograms. The STFT spectrogram function uses 512 FFT bins, window size of 600 samples, and hop length (how many samples the window shifts) of 100 samples. The L1 and SIMSE implementations are differentiable. 

\subsubsection{JTFS Loss}
The \JTFS~loss function is the application of L1 difference to the JTFS representations of two sounds~\cite{vahidi2023mesostructures}. The code used for the JTFS transformation is the differentiable implementation of a 1-dimensional JTFS function provided by Andreux \textit{et al.}~\cite{kymatio}. 

\subsubsection{Soft-DTW Loss}
We use the soft-DTW function, which is differentiable \DIFdelbegin \DIFdel{and --- depending on its parameters --- not }\DIFdelend \DIFaddbegin \DIFadd{and---depending on its parameters---not }\DIFaddend shift-invariant~\cite{cuturi2017soft,janati2020spatio,tavenard.blog.softdtw}. The loss function \DTWEnv~is the application of the soft-DTW function to the envelope of the two sounds being compared~\cite{lyons1997understanding}. \DIFaddbegin \DIFadd{This loss uses the similarity of amplitude modulation patterns, which are perceptually important in many sound-design scenarios. }\DIFaddend The envelope is calculated by creating the STFT spectrogram of a sound (the same process used in Section~\ref{sec:fourier_specs}) and summing the values at each timestep. 

\subsection{The Synthesizers}
\label{sec:programs}
The synthesizer programs are meant to be simple examples that test the building blocks of digital sound synthesis. Subtractive, additive, and FM/AM synthesis are three of the most common techniques in sound design~\cite{smith1991viewpoints}. In subtractive synthesis, frequencies are removed from a sound via digital filters. In additive synthesis, complex sounds are created via the linear combination of simpler sounds~\cite{lyons1997understanding,smith2007introduction}. As discussed previously, FM/AM synthesis refers to the general technique of modulating the frequency or amplitude of a waveform (the carrier) by another waveform (the modulator).

For each program, we provide the Faust code~\cite{orlarey2009faust}, which can be run in the online IDE~\footnote{\url{https://faustide.grame.fr/}}. Faust is a functional language for audio synthesis that can succinctly define signal processing chains. Following Braun's methodology~\cite{braun2024dac}, programs are first defined in Faust, then converted (or \textit{transpiled}) to differentiable \DIFdelbegin \DIFdel{Jax }\DIFdelend \DIFaddbegin \DIFadd{JAX }\DIFaddend functions using the DawDreamer library\footnote{\url{https://github.com/DBraun/DawDreamer}}. 
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD <  %%%
\DIFdel{Once the programs are transpiled to differentiable synthesizers, the iterative sound-matching procedure is:
 }%DIFDELCMD < \begin{enumerate}
\begin{enumerate}%DIFAUXCMD
%DIFDELCMD <     \item %%%
\item%DIFAUXCMD
\DIFdel{Initialize $\theta^*$ and $\hat{\theta}$, i.e., random generation of target and initial parameters uniformly over a predefined range
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{Generating the output of the synthesizer with $\hat{\theta}$ (the length of the output is set to 1 second with sample-rate of 44100 Hz) 
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{Calculating the loss between the target and output
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{Applying gradient updates to the synthesizer parameters
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{Repeating the second step with the updated parameters $\hat{\theta}$, until maximum number of iterations has been reached
 }
\end{enumerate}%DIFAUXCMD
%DIFDELCMD < \end{enumerate}
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  A general overview of the programs and the predefined parameter ranges is given in Table~\ref{tab:programs}.
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{In the following subsections, we describe the programs and their parameters in more detail, followed by the methodology in Section~\ref{sec:methodology} and results in Section~\ref{sec:results_posthoc}.
}%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  \begin{table*}[htbp]
%DIF <  \centering
%DIF <  \begin{adjustbox}{max width=\textwidth}
%DIF <  \begin{tabular}{|c|c|c|c|l|}
%DIF <  \hline
%DIF <  \textbf{ID} & \textbf{Parameters} & \makecell{\textbf{Param. 1}\\\textbf{Range}} & \makecell{\textbf{Param. 2}\\\textbf{Range}} & \textbf{Description} \\
%DIF <  \hline
%DIF <  \BPNoise & hp\_cut, lp\_cut & 50-1000 & 1-120 & Band-pass of Noise signal \\
%DIF <  \hline
%DIF <  \AddSineSaw & saw\_freq, sine\_freq & 20-1000 & 20-1000 & Addition of sine and saw waves \\
%DIF <  \hline
%DIF <  \AmpMod & amp, carrier & 1-20 & 20-1000 & LFO on amplitude of noise osc \\
%DIF <  \hline
%DIF <  \FMMod & amp, carrier & 0-5 & 0-4 & LFO on amplitude of saw osc \\
%DIF <  \hline
%DIF <  \end{tabular}
%DIF <  \end{adjustbox}
%DIF <  \caption{Overview of Program Parameters and the range of values each parameter can take.}
%DIF <  \label{tab:programs}
%DIF <  \end{table*}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \begin{lstlisting}[caption={\BPNoise}, label={lst:program0}, language=Faust,
                  float, floatplacement=!H, xleftmargin=1em, xrightmargin=0.5em, firstnumber=0, aboveskip=0em, belowskip=-1em]
import("stdfaust.lib");
lp_cut = hslider("lp_cut",900,50,1000,1);
hp_cut = hslider("hp_cut",100,1,120,1);
process = no.noise:fi.lowpass(3,lp_cut):fi.highpass(10,hp_cut);
\end{lstlisting}

\begin{lstlisting}[caption={\AddSineSaw}, label={lst:program1},language=Faust,float,floatplacement=!H,xleftmargin=1em,xrightmargin=0.5em,firstnumber=0,aboveskip=0em, belowskip=-1em]
import("stdfaust.lib");
saw_freq = hslider("saw_freq",800,20,1000,1);
sine_freq = hslider("sine_freq",300,20,1000,1);
sineOsc(f) = +(f/ma.SR) ~ ma.frac:*(2*ma.PI) : sin;
sawOsc(f) = +(f/ma.SR) ~ ma.frac;
process = sineOsc(sine_freq)+sawOsc(saw_freq);
\end{lstlisting}

\begin{lstlisting}[caption={\AmpMod}, label={lst:program2},language=Faust,float,floatplacement=!H,xleftmargin=1em,xrightmargin=0.5em,firstnumber=0,aboveskip=0em, belowskip=-1em]
import("stdfaust.lib");
amp = hslider("amp",0.5,0,5,0.01);
modulator = hslider("modulator",0.5,0,4,0.01);
sineOsc(f) = +(f/ma.SR) ~ ma.frac:*(2*ma.PI) : sin;
process = no.noise*sineOsc(modulator)*amp;
\end{lstlisting}

\begin{lstlisting}[caption={\FMMod}, label={lst:program3},language=Faust,float,floatplacement=!H,xleftmargin=1em,xrightmargin=0.5em,firstnumber=0,aboveskip=0em, belowskip=-1em]
import("stdfaust.lib");
carrier = hslider("carrier",100,20,1000,1);
amp = hslider("amp",6,1,20,1);
sineOsc(f) = +(f/ma.SR) ~ ma.frac:*(2*ma.PI) : sin;
sawOsc(f) = +(f/ma.SR) ~ ma.frac;
process = sineOsc(amp)*sawOsc(carrier);
\end{lstlisting}

\subsubsection{\BPNoise}
\label{sec:program0}
\BPNoise{} is a bare bones example of subtractive synthesis using digital filters~\cite{smith2007introduction}. Despite the ubiquity of subtractive synthesis in practical sound design, it has rarely been tested in differentiable sound-matching. In this program, a noise signal is fed through a band-pass (BP) filter. This removes the frequencies outside the low-pass (LP) and high-pass (HP) cutoffs. The noise generator produces all frequencies with random variation over time. The LP filter removes frequencies over its threshold frequency, and the HP removes frequencies lower than its threshold~\cite{smith2007introduction}. The search parameters are the cutoff thresholds for the LP and HP filters. Listing~\ref{lst:program0} shows the Faust code for \BPNoise.

\subsubsection{\AddSineSaw}
\label{sec:program1}
\AddSineSaw{} is an additive program that combines a saw and a sine wave function. Like subtractive synthesis, additive synthesis is a common approach to sound design that has not been extensively tested as a benchmark in sound-matching. The search parameters here are the frequency of the sine and saw oscillators. Listing~\ref{lst:program1} is the Faust code for \AddSineSaw.

\subsubsection{\AmpMod}
\label{sec:program2}
\AmpMod{} involves amplitude modulation of a noise generator by an LFO with \textit{modulator} as its frequency and a global \textit{amp} value that does not change over time and applies to the entire signal. The search parameters for this program are the LFO frequency and the amp value. The amp value acts as a global volume control, and since the sounds are normalized before analysis, it is likely inconsequential to the performance. This program is meant to be a stepping stone to \FMMod, which utilizes proper AM synthesis. Listing~\ref{lst:program2} is the Faust code for \AmpMod. 

\subsubsection{\FMMod}
\label{sec:program3}
Relative to additive and subtractive synthesis, AM/FM synthesis are less common methods of sound design. Due to their frequent use in previous works, we also tested an AM synthesizer. The synthesizer program is the multiplication of a low frequency sine oscillator with frequency parameter \textit{amp}, and a saw oscillator with frequency parameter \textit{carrier}. Listing~\ref{lst:program3} is the Faust code for \FMMod. 




\subsection{\DIFdelbegin \DIFdel{Post hoc Analysis of Loss Performance}\DIFdelend \DIFaddbegin \DIFadd{Evaluation Methods}\DIFaddend }
\DIFdelbegin %DIFDELCMD < \label{sec:results_posthoc}
%DIFDELCMD < %%%
\DIFdel{As summarized in Figure~\ref{fig:posthoc_evaluation}, }\DIFdelend \DIFaddbegin \label{sec:evaluation_manual_auto}
\DIFadd{The two automatic evaluation methods used here are P-Loss and MSS, as described in Section~\ref{sec:loss_funcs}. For P-Loss, the parameters are normalized between 0-1 based on the valid ranges defined in the Faust program, and the L1 distance between the normalized parameters is calculated. MSS is computed using a hop size of $100$ samples, and FFT window lengths of $(512, 1024, 2048, 4096)$. 
}

\DIFadd{Automatic sound evaluation methods are often unreliable, as their alignment with the subjective judgments of human listeners is uncertain. Therefore, listening tests are conducted by randomly sampling 40 experiment results }\DIFaddend for each program and \DIFdelbegin \DIFdel{evaluation (or scoring)method, there are four sets of 1000 bootstrapped values, corresponding to the }\DIFdelend \DIFaddbegin \DIFadd{loss function pair. Two of the authors then assign blinded similarity scores to the outputs and targets using a 5-point Likert scale (1 = no similarity, 5 = near identical)~\mbox{%DIFAUXCMD
\cite{jebb2021review}}\hskip0pt%DIFAUXCMD
. Using Spearman's rank correlation~\mbox{%DIFAUXCMD
\cite{spearman1987proof,rebekic2015pearson}}\hskip0pt%DIFAUXCMD
, we found a very strong rate of agreement between the human listeners (see Section~\ref{sec:consistency_in_rankings}), a good indicator that the scores assigned by manual listeners can be treated as the ground truth. We then combine the ranks assigned by both authors, giving us 80 ranks per program and loss function pair. 
}



%DIF >  Agreement between the assigned scores is measured using Spearman’s rank correlation, which provides both a correlation coefficient ($\rho$) ranging from –1 (perfect negative correlation) to 1 (perfect positive correlation), and a p-value testing the null hypothesis of no correlation~\cite{spearman1987proof,rebekic2015pearson}. Across all programs, the correlation was very strong ($\rho = 0.86$, $p < 10^{-180}$). Per-program correlations were also very strong: $\rho = 0.71$ for \BPNoise{} ($p < 10^{-25}$), $\rho = 0.64$ for \AddSineSaw{} ($p < 10^{-19}$), $\rho = 0.84$ for \AmpMod{} ($p < 10^{-43}$), and $\rho = 0.85$ for \FMMod{} ($p < 10^{-44}$). This high rate of agreement is a good indicator that the scores assigned by manual listeners can be treated as the ground truth. 

%DIF >  For each program and loss function, the blind manual hearing test gives us 80 ranks, which are given by two of the authors to a set of 40 randomly selected experiment results. This gives 160 pairs of ranks per program, and 640 for all programs. We computed Spearman correlations between the two reviewers.

\subsection{\DIFadd{Training Loop and Gradient Calculations}}
\DIFadd{Given a differentiable loss and synthesizer, the iterative sound-matching procedure is as follows:
 }\begin{enumerate}
    \item \DIFadd{Initialize $\theta^*$ and $\hat{\theta}$, i.e., random generation of target and initial parameters uniformly over a predefined range
    }\item \DIFadd{Generating the output of the synthesizer with $\hat{\theta}$ (the length of the output is set to 1 second with sample-rate of 44100 Hz) 
    }\item \DIFadd{Calculating the loss between the target and output
    }\item \DIFadd{Applying gradient updates to the synthesizer parameters
    }\item \DIFadd{Repeating the second step with the updated parameters $\hat{\theta}$, until maximum number of iterations has been reached
 }\end{enumerate}

 \DIFadd{For updating the synthesizer parameters (or weights), we use RMSProp, which operates similar to stochastic gradient descent (SGD)~\mbox{%DIFAUXCMD
\cite{goodfellow2016deep}}\hskip0pt%DIFAUXCMD
, with the caveat that the gradients of each weight are scaled by the root-mean-square of past gradients of that weight. Based on some initial test runs, we used an arbitrary fixed learning rate of 0.045 for all experiments (learning rates used in Vahidi }\textit{\DIFadd{et al.}}\DIFadd{~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures} }\hskip0pt%DIFAUXCMD
is unknown). The maximum number of iterations is set to 200, where, based on our observations, the parameters have either irrecoverably diverged outside the acceptable ranges, or are stuck at a local minimum. Additionally, the gradients are large, and calculated backwards sequentially through the signal processing chain. This backward calculation resembles the }\textit{\DIFadd{exploding gradients}} \DIFadd{problem in recurrent neural networks~\mbox{%DIFAUXCMD
\cite{gers2000learning}}\hskip0pt%DIFAUXCMD
. Gradient clipping~\mbox{%DIFAUXCMD
\cite{goodfellow2016deep} }\hskip0pt%DIFAUXCMD
is preemptively used to ensure that the  $\ell_2$ norm of all gradients does not exceed the threshold of 1. 
%DIF > 
}\subsection{\DIFadd{Ranking Loss Functions}}

 %DIF >  As an example, consider the following simplified scenario: we have two loss functions, $L_1$ and $L_2$, and a single synthesizer program $P$ with a target parameter set $\theta^*$ where the target sound is $t$, or $P(\theta^*) = t$. We run a large number (in this case, 300 for each loss function) of experiments where we randomly set the initial ($\theta_0$) and target parameters ($\theta^*$) of $P$ and run an iterative sound-matching loop for each loss function, with maximum number of iterations set to 200. When the loop is terminated, we measure the similarity of the final output sound $x$ (where $P(\theta_{199}) = x$) to $t$, or if using P-Loss we compare $\theta_{199}$ to $\theta^*$. Having 300 experiments gives us a distribution of losses to determine whether $L_1$ or $L_2$ led to better performance. This simplified example leads us to our methodology which has four loss functions, four synthesizer programs, and three different evaluation methods of scoring sound similarity.

 \DIFadd{Figure~\ref{fig:posthoc_evaluation} is a visualized summary of how the loss functions are ranked. We have }\DIFaddend four loss functions \DIFdelbegin \DIFdel{. }\DIFdelend \DIFaddbegin \DIFadd{and four synthesizer programs. The maximum number of iterations set to 200, and 300 experiments for each loss/synthesizer combination. Each synthesizer program is paired with four loss functions and 300 experiments are conducted and evaluated automatically with P-Loss and MSS, and manually with Likert scores. With this approach, for each program and loss function pair, there are three distributions that can be used to rank the loss functions. Two distributions with 300 automatically assigned similarities (P-Loss and MSS) and one distribution with 80 Likert scores (combining the 40 ranks assigned by each author).
}

\DIFadd{For consistency and statistical robustness, the distributions are upsampled to 1000 values using bootstrapping~\mbox{%DIFAUXCMD
\cite{tibshirani1993introduction}}\hskip0pt%DIFAUXCMD
. Bootstrapping gives an estimation of the distribution for the mean performance of each experiment. $k$ (set to 1000) samples of $n$ values (set to 100\% of the values) are taken with replacement from the empirical distribution (list of 300 performance values for MSS and P-Loss or 80 for manual rankings). The mean performance is calculated for each of the $k$ samples. The $k$ estimates of the mean performance give us a bootstrapped distribution for each loss function. 
}



\begin{figure*}
\resizebox{\linewidth}{!}{ % Automatically scales diagram to fit page width
\begin{tikzpicture}[
    function/.style={rectangle, draw, rounded corners, minimum width=2cm, minimum height=1cm, text centered, node distance=1cm},
    distribution/.style={ellipse, draw, minimum width=1cm, minimum height=0cm, text centered, node distance=0.1cm},
    gaussian/.style={draw, smooth, samples=100, domain=-2.5:2.5},
    NPSK/.style={rectangle, draw, rounded corners, minimum width=2.5cm, minimum height=1cm, text centered, node distance=1.5cm, fill=blue!20},
    level 1/.style={sibling distance=6cm, level distance=1cm},
    level 2/.style={sibling distance=2cm, level distance=1.2cm},
    mss/.style={fill=red!50, draw=black},
    ploss/.style={fill=blue!50, draw=black},
    likert/.style={fill=green!40, draw=black}
]

% Existing functions and distributions diagram
\node[function] {Synthesizer Program}
    child { node[function] {DTW\_Onset}
        child { node[distribution, mss] {MSS}
            child[grow=down, level distance=1cm] {
                node[draw=none] {
                    \begin{tikzpicture}[scale=0.7]
                        \draw[red] plot (\x/3, {exp(-\x*\x)});  % Red Gaussian
                    }
                }
            }
        }
        child { node[distribution, ploss] {P-Loss}
            child[grow=down, level distance=1cm] {
                node[draw=none] {
                    \PICTUREBLOCKtikzpicture{[scale=0.7]
                        \draw[blue] plot (\x/3, {exp(-\x*\x)});  % Blue Gaussian
                    }
                }
            }
        }
        child { node[distribution, likert] {Likert}
            child[grow=down, level distance=1cm] {
                node[draw=none] {
                    \PICTUREBLOCKtikzpicture{[scale=0.7]
                        \draw[green] plot (\x/3, {exp(-\x*\x)});  % Green Gaussian
                    }
                }
            }
        }
    \end{tikzpicture}
    child { node[function] {L1\_Spec}
        child { node[distribution, mss] {MSS} }
        child { node[distribution, ploss] {P-Loss} }
        child { node[distribution, likert] {Likert} }
    }
    child { node[function] {SIMSE\_Spec}
        child { node[distribution, mss] {MSS} }
        child { node[distribution, ploss] {P-Loss} }
        child { node[distribution, likert] {Likert} }
    }
    child { node[function] {JTFS}
        child { node[distribution, mss] {MSS}
            child[grow=down, level distance=1cm] {
                node[draw=none] {
                    \begin{tikzpicture}[scale=0.7]
                        \draw[red] plot (\x/3, {exp(-\x*\x)});  % Red Gaussian
                    \end{tikzpicture}
                }
            }
        }
        child { node[distribution, ploss] {P-Loss}
            child[grow=down, level distance=1cm] {
                node[draw=none] {
                    \begin{tikzpicture}[scale=0.7]
                        \draw[blue] plot (\x/3, {exp(-\x*\x)});  % Blue Gaussian
                    \end{tikzpicture}
                }
            }
        }
        child { node[distribution, likert] {Likert}
            child[grow=down, level distance=1cm] {
                node[draw=none] {
                    \begin{tikzpicture}[scale=0.7]
                        \draw[green] plot (\x/3, {exp(-\x*\x)});  % Green Gaussian
                    \end{tikzpicture}
                }
            }
        }
    };

% Label for the second row (Distributions)
\node at (0,-3.25) {\textbf{$\longleftarrow$\ \ \large Bootstrapped Distributions\ \ $\longrightarrow$}};
% Left side: NPSK with MSS

\def\xoffsetL{-0.5}
\def\xoffsetM{0}
\def\xoffsetR{0.5}
\def\yoffset{1}

% Left: NPSK with MSS
\node[NPSK, text height=5ex, align=center, fill=red!30, draw=black] at ({-6+\xoffsetL},{-6+\yoffset}) {NPSK with 4 \\ MSS distributions};
\draw[->] ({-6+\xoffsetL},{-6.5+\yoffset}) -- ({-6+\xoffsetL},{-6.6+\yoffset}) node[below, below] {\textbf{Ranks 1-4}};
\draw[red!40, thick] ({-5+\xoffsetL}, {-5.4+\yoffset}) -- ({-2.95+\xoffsetL}, {-5.15+\yoffset});  % Line 1
\draw[red!40, thick] ({-5.5+\xoffsetL}, {-5.4+\yoffset}) -- ({-3.5+\xoffsetL}, {-4.75+\yoffset});  % Line 2
\draw[red!40, thick] ({-6+\xoffsetL}, {-5.4+\yoffset}) -- ({-5.5+\xoffsetL}, {-4.75+\yoffset});  % Line 3
\draw[red!40, thick] ({-6.5+\xoffsetL}, {-5.4+\yoffset}) -- ({-9+\xoffsetL}, {-4.75+\yoffset});  % Line 4

% Middle: NPSK with P-Loss
\node[NPSK, text height=5ex, align=center, fill=blue!40, draw=black] at ({0+\xoffsetM},{-6+\yoffset}) {NPSK with 4 \\ P-Loss distributions};
\draw[->] ({0+\xoffsetM},{-6.5+\yoffset}) -- ({0+\xoffsetM},{-6.6+\yoffset}) node[below, below] {\textbf{Ranks 1-4}};
\draw[blue!40, thick] ({1.5+\xoffsetM}, {-5.4+\yoffset}) -- ({3.25+\xoffsetM}, {-4.75+\yoffset});  % Line 1
\draw[blue!40, thick] ({0.75+\xoffsetM}, {-5.4+\yoffset}) -- ({1.5+\xoffsetM}, {-4.75+\yoffset});  % Line 2
\draw[blue!40, thick] ({-0.25+\xoffsetM}, {-5.4+\yoffset}) -- ({-1+\xoffsetM}, {-4.75+\yoffset});  % Line 3
\draw[blue!40, thick] ({-0.75+\xoffsetM}, {-5.4+\yoffset}) -- ({-3+\xoffsetM}, {-4.75+\yoffset});  % Line 4

% Right: NPSK with Likert
\node[NPSK, text height=5ex, align=center, fill=green!30, draw=black] at ({6+\xoffsetR},{-6+\yoffset}) {NPSK with 4 \\ Likert distributions};
\draw[->] ({6+\xoffsetR},{-6.5+\yoffset}) -- ({6+\xoffsetR},{-6.6+\yoffset}) node[below, below] {\textbf{Ranks 1-4}};
\draw[green!50, thick] ({7.5+\xoffsetR}, {-5.4+\yoffset}) -- ({8.75+\xoffsetR}, {-5+\yoffset});  % Line 1
\draw[green!50, thick] ({6.75+\xoffsetR}, {-5.4+\yoffset}) -- ({6+\xoffsetR}, {-4.75+\yoffset});    % Line 2
\draw[green!50, thick] ({5.75+\xoffsetR}, {-5.4+\yoffset}) -- ({4+\xoffsetR}, {-4.85+\yoffset});    % Line 3
\draw[green!50, thick] ({4.95+\xoffsetR}, {-5.4+\yoffset}) -- ({3.25+\xoffsetR}, {-5.1+\yoffset}); % Line 4



\end{tikzpicture}
}
\caption{\DIFaddFL{For each synthesizer program, we assign ranks to the four loss functions  (}\DTWEnv\DIFaddFL{, }\LoneSpec\DIFaddFL{, }\SIMSESpec\DIFaddFL{, \JTFS) in three different ways, using MSS, P-Loss, or manually assigned Likert scores.}}
\label{fig:posthoc_evaluation}
\end{figure*}
 \DIFaddend A post-hoc test is conducted in two stages in order to determine whether there is a difference in performance between loss functions for each program, and if so, which are the best performers. The first stage determines whether there is a difference in group means, with the null hypothesis being that all groups have similar mean ranks. The second stage ranks the loss functions from best to worst using the non-parametric Scott-Knott test (\gls{NPSK})~\cite{tantithamthavorn2017mvt,tantithamthavorn2018optimization}.
The Kruskal-Wallis test is used for the first stage~\cite{kruskal1952use}; this test pools and ranks all evaluation measures for a program, then tests whether the differences in mean rank for all loss function groups is zero. The Kruskal-Wallis test calculates an H statistic that is compared to a chi-square distribution with k-1 degrees of freedom (k is the number of groups, or 4, and degrees of freedom is 3). Using the significance level of 0.05, if the H statistic is greater than the critical value of the chi-square distribution then the null hypothesis is rejected, meaning that at least one group has a mean rank significantly above others~\cite{kruskal1952use}. 
\DIFdelbegin \DIFdel{Table~\ref{tab:kruskal_auto} shows the Kruskal-Wallis results for every program and evaluation method, and shows that significant differences between loss function performance measures (i.e., the bootstrapped distributions of the evaluation for each loss function) exist in all cases except MSS for the \AmpMod{} program. 
}\DIFdelend 

\begin{table}[ht]
\centering
\caption{Do loss functions have different median performance? Kruskal-Wallis results by program and bootstrapped evaluation results.}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Program} & \textbf{Eval. Method} & \textbf{H-Stat.} & \textbf{P-value} & \textbf{Reject} \\
\hline
\BPNoise & MSS      & 81.19  & $1.70 \times 10^{-17}$ & Yes \\
\BPNoise & P-Loss  & 163.60 & $3.07 \times 10^{-35}$ & Yes \\
\BPNoise & Manual & 9.45  & $2.39 \times 10^{-2}$ & Yes \\
\AddSineSaw & MSS      & 308.97 & $1.14 \times 10^{-66}$ & Yes \\
\AddSineSaw & P-Loss  & 348.42 & $3.28 \times 10^{-75}$ & Yes \\
\AddSineSaw & Manual & 9.45  & $2.39 \times 10^{-2}$ & Yes \\
\AmpMod & MSS      & 1.35   & $0.7171$               & No \\
\AmpMod & P-Loss  & 366.76 & $3.50 \times 10^{-79}$ & Yes \\
\AmpMod & Manual & 32.71 & $3.70 \times 10^{-7}$ & Yes \\
\FMMod & MSS      & 564.65 & $4.65 \times 10^{-122}$ & Yes \\
\FMMod & P-Loss  & 229.19 & $2.07 \times 10^{-49}$ & Yes \\
\FMMod & Manual &  207.58 & $9.69 \times 10^{-45}$ & Yes \\
\hline
\end{tabular}
\label{tab:kruskal_auto}
\end{table}

% \begin{table}[ht]
% \centering
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Program} & \textbf{H-Stat.} & \textbf{P-value} & \textbf{Reject} \\
% \hline
% \BPNoise & 9.45  & $2.39 \times 10^{-2}$ & Yes \\
% \AddSineSaw & 9.45  & $2.39 \times 10^{-2}$ & Yes \\
% \AmpMod & 32.71 & $3.70 \times 10^{-7}$ & Yes \\
% \FMMod & 207.58 & $9.69 \times 10^{-45}$ & Yes \\
% \hline
% \end{tabular}
% \caption{Manual ranking Kruskal-Wallis results. Significant differences are observed for all programs.}
% \label{tab:kruskal_manual}
% \end{table}

\DIFdelbegin \subsubsection{\DIFdel{Best Performers For Each Program}}
%DIFAUXCMD
\addtocounter{subsubsection}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \subsection{\DIFadd{Best Performers For Each Program}}
\DIFadd{Table~\ref{tab:kruskal_auto} shows the Kruskal-Wallis results for every program and evaluation method, we see significant differences between loss function performance measures (i.e., the bootstrapped distributions of the evaluation for each loss function) in 11 of the 12 cases, with the exception of  MSS for the \AmpMod{} program.  
}

\DIFaddend Based on the bootstrapped distribution of \DIFaddbegin \DIFadd{the }\DIFaddend scores, the NPSK algorithm ranks the loss functions from 1 (best) to a maximum of 4 (worst). In cases where the distributions are similar, multiple loss functions can be clustered into the same rank.
\DIFaddbegin 

\DIFaddend In Figure~\ref{fig:npsk_all}, we use color-coded violin plots to visualize the best performers for each program. The corresponding colors for each rank are
\colorbox{rank1}{\textcolor{black}{\textbf{1}}} \colorbox{rank2}{\textcolor{white}{\textbf{2}}} \colorbox{rank3}{\textcolor{white}{\textbf{3}}} \colorbox{rank4}{\textcolor{black}{\textcolor{white}{\textbf{4}}}}\DIFaddbegin \DIFadd{.
}\DIFaddend 

\newcommand{\markdiff}[1]{\textbf{\makebox[0pt][r]{\textbf{*}}#1}}

\begin{table*}[p]
\centering
\caption{Ranks for each synthesis method (rows) under the three evaluation metrics (columns), across four targets (\BPNoise, \AddSineSaw, \AmpMod, \FMMod). Values marked with an asterisk are different from the Likert score hearing ranks.}
\small
\begin{tabular}{|c|ccc|ccc|ccc|ccc|}
\hline
\textbf{Function} 
  & \multicolumn{3}{c|}{\BPNoise}
  & \multicolumn{3}{c|}{\AddSineSaw}
  & \multicolumn{3}{c|}{\AmpMod}
  & \multicolumn{3}{c|}{\FMMod} \\
\cline{2-13}
  & MSS & P-LOSS & Hearing 
  & MSS & P-LOSS & Hearing 
  & MSS & P-LOSS & Hearing 
  & MSS & P-LOSS & Hearing \\
\hline

\textbf{SIMSE} 
  & 1    & \markdiff{2}   & 1    
  & 4    & 4    & 4    
  & \markdiff{3}   & 4    & 4    
  & 2    & 2    & 2    \\
\textbf{L1}    
  & 2    & \markdiff{1}   & 2    
  & 2    & 2    & 2    
  & 2    & 2    & 2    
  & 3    & 3    & 3    \\
\textbf{JTFS}  
  & 3    & \markdiff{4}   & 3    
  & 1    & 1    & 1    
  & \markdiff{4}   & 3    & 3    
  & 4    & 4    & 4    \\
\textbf{DTW}   
  & \markdiff{4}   & 3    & 3    
  & 3    & 3    & 3    
  & 1    & 1    & 1    
  & 1    & 1    & 1    \\

\hline
\end{tabular}
\label{tab:combined_ranks}
\end{table*}

\begin{figure*}[p]
  \centering
\subfloat[\BPNoise~bootstrapped distributions and ranks given by NPSK.]{
  \begin{minipage}{\textwidth}
    \begin{minipage}[t]{0.03\textwidth}
      \footnotesize\raggedleft
      \vspace{0.65cm}
      SIMSE\\[0.6cm]
      L1\\[0.65cm]
      JTFS\\[0.65cm]
      DTW
    \end{minipage}%
    \hspace{0.01\textwidth}%
    \begin{minipage}[t]{0.96\textwidth}
      \centering
      \begin{minipage}[t]{0.31\textwidth}
        \centering
        \normalsize\bfseries MSS\\[0.3em]
        \includegraphics[width=\linewidth]{images/npsk_MSS_0.png}
      \end{minipage}
      \hspace{0.015\textwidth}%
      \begin{minipage}[t]{0.31\textwidth}
        \centering
        \normalsize\bfseries P-Loss\\[0.3em]
        \includegraphics[width=\linewidth]{images/npsk_P-Loss_0.png}
      \end{minipage}
      \hspace{0.01\textwidth}%
      \begin{minipage}[t]{0.31\textwidth}
        \centering
        \normalsize\bfseries Likert\\[0.3em]
        \includegraphics[width=\linewidth]{images/npsk_likert_0.png}
      \end{minipage}
    \end{minipage}
  \end{minipage}
  \label{fig:npsk_p0}
}\\[0.5em]

% 2nd row
\subfloat[\AddSineSaw~bootstrapped distributions and ranks given by NPSK.]{
  \begin{minipage}{\textwidth}
    \begin{minipage}{0.03\textwidth}
      \footnotesize\raggedleft
      \vspace{0.5cm}
      SIMSE\\[0.6cm]
      L1\\[0.65cm]
      JTFS\\[0.65cm]
      DTW
    \end{minipage}%
    \begin{minipage}{0.98\textwidth}\centering
      \includegraphics[width=0.31\textwidth]{images/npsk_MSS_1.png}%
      \hspace{0.015\textwidth}%
      \includegraphics[width=0.31\textwidth]{images/npsk_P-Loss_1.png}%
      \hspace{0.015\textwidth}%
      \includegraphics[width=0.31\textwidth]{images/npsk_likert_1.png}
    \end{minipage}
  \end{minipage}
  \label{fig:npsk_p1}
}\\[0.5em]

% 3rd row
\subfloat[\AmpMod~bootstrapped distributions and ranks given by NPSK.]{
  \begin{minipage}{\textwidth}
    \begin{minipage}{0.03\textwidth}
      \footnotesize\raggedleft
      \vspace{0.5cm}
      SIMSE\\[0.6cm]
      L1\\[0.65cm]
      JTFS\\[0.65cm]
      DTW
    \end{minipage}%
    \begin{minipage}{0.98\textwidth}\centering
      \includegraphics[width=0.31\textwidth]{images/npsk_MSS_2.png}%
      \hspace{0.015\textwidth}%
      \includegraphics[width=0.31\textwidth]{images/npsk_P-Loss_2.png}%
      \hspace{0.015\textwidth}%
      \includegraphics[width=0.31\textwidth]{images/npsk_likert_2.png}
    \end{minipage}
  \end{minipage}
  \label{fig:npsk_p2}
}\\[0.5em]

% 4th row
\subfloat[\FMMod~bootstrapped distributions and ranks given by NPSK.]{
  \begin{minipage}{\textwidth}
    \begin{minipage}{0.03\textwidth}
      \footnotesize\raggedleft
      \vspace{0.5cm}
      SIMSE\\[0.6cm]
      L1\\[0.65cm]
      JTFS\\[0.65cm]
      DTW
    \end{minipage}%
    \begin{minipage}{0.98\textwidth}\centering
      \includegraphics[width=0.31\textwidth]{images/npsk_MSS_3.png}%
      \hspace{0.015\textwidth}%
      \includegraphics[width=0.31\textwidth]{images/npsk_P-Loss_3.png}%
      \hspace{0.015\textwidth}%
      \includegraphics[width=0.31\textwidth]{images/npsk_likert_3.png}
    \end{minipage}
  \end{minipage}
  \label{fig:npsk_p3}
}

\caption{Distributions and ranks of the loss functions based on three different performance measures. From left to right, the performance measures are: MSS, P-Loss, and Likert. Higher values indicate better performance. Rank colors are \colorbox{rank1}{\textcolor{black}{\textbf{1}}} \colorbox{rank2}{\textcolor{white}{\textbf{2}}} \colorbox{rank3}{\textcolor{white}{\textbf{3}}} \colorbox{rank4}{\textcolor{white}{\textbf{4}}}.}
\label{fig:npsk_all}
\end{figure*}


\subsubsection{\BPNoise}
For this synthesizer program, the spectrogram-based models performed the best. This makes intuitive sense, as the visual effects of a band-pass filter on white noise are readily apparent in a spectrogram. As shown in Figure~\ref{fig:npsk_p0}, manual hearing test and MSS selected \SIMSESpec~as the best performer and \LoneSpec~as the second-best performer, while P-Loss gave the reverse order. 


\subsubsection{\AddSineSaw}
As shown in Figure~\ref{fig:npsk_p1}, JTFS was the best performing loss function in all evaluation methods. Moreover, all evaluation methods produced identical results. 

\subsubsection{\AmpMod}
\DIFdelbegin \DIFdel{I}\DIFdelend P-Loss  and manual hearing tests yield identical rankings, and MSS results vary for \DIFdelbegin \DIFdel{for }\DIFdelend ranks 2 to 4. It is not surprising that MSS results differ from the hearing tests, as we saw in Table~\ref{tab:kruskal_auto}, MSS showed no significant differences between groups. As shown in Figure~\ref{fig:npsk_p2}, all models selected \DTWEnv~as the best performer. This may be due to \DTWEnv's focus on periodic changes in loudness.

\subsubsection{\FMMod}
\DTWEnv~is again the best performer here. As shown in Figure~\ref{fig:npsk_p3}, all methods of analysis gave identical rankings to all programs.


\subsection{Consistency In Rankings}
\label{sec:consistency_in_rankings}
\subsubsection{Manual Ranks} Agreement between the manually assigned scores is measured using Spearman’s rank correlation, which provides both a correlation coefficient ($\rho$) ranging from –1 (perfect negative correlation) to 1 (perfect positive correlation), and a p-value testing the null hypothesis of no correlation~\cite{spearman1987proof,rebekic2015pearson}. Across all programs, the correlation was very strong ($\rho = 0.86$, $p < 10^{-180}$). Per-program correlations were also very strong: $\rho = 0.71$ for \BPNoise{} ($p < 10^{-25}$), $\rho = 0.64$ for \AddSineSaw{} ($p < 10^{-19}$), $\rho = 0.84$ for \AmpMod{} ($p < 10^{-43}$), and $\rho = 0.85$ for \FMMod{} ($p < 10^{-44}$). 

Table~\ref{tab:combined_ranks} shows the SNPK rankings of bootstrapped evaluation results for each program. Both MSS and P-Loss gave a different rank from the hearing results in 3 out of 16 cases, which shows consistency between automatic and manual hearing tests, at least for the simple programs used in this work. We also observe that top ranks were consistent across performance evaluation methodologies, with the exception of P-Loss in \BPNoise, which narrowly picks a different spectrogram-based loss function.
% \input{extra_figures/ranking_tables}


%DIF >  \section{Takeaways, Practical Implications, and Caveats}
%DIF >  (Q1) To what extent do automatic evaluation metrics agree with manual listening tests? (Q2) Can novel loss functions utilizing Dynamic Time Warping (DTW) and Scale-Invariant Mean Squared Error (SIMSE) provide advantages over standard loss functions, and if so, when? (Q3) Is an iterative differentiable optimization a viable strategy for design of sound-matching experiments?
\DIFaddbegin \DIFadd{The experiment results provide answers regarding the main hypothesis of this work as well as the secondary questions. In the following sections, we will discuss our key findings, caveats, and make recommendations for future research.
}

%DIF >  Future work should therefore investigate hybrid approaches, combining differentiable blocks with surrogate models or gradient-free optimization for non-differentiable components. 
%DIF >  Out-of-domain targets, such as recorded instruments or environmental sounds, represent another direction where the loss–synth interaction may behave differently than in controlled in-domain experiments.


\section{\DIFadd{Discussion}}
\DIFaddend \subsection{\DIFdelbegin \DIFdel{Takeaways}\DIFdelend \DIFaddbegin \DIFadd{Key Findings}\DIFaddend }
\DIFdelbegin \DIFdel{An important takeaway }\DIFdelend \DIFaddbegin \DIFadd{The most important takeaway regarding our main hypothesis }\DIFaddend is that the loss function that yields the best sound-matching outcomes varies depending on the synthesizer program. This is \DIFdelbegin \DIFdel{perhaps }\DIFdelend \DIFaddbegin \DIFadd{likely }\DIFaddend due to the interaction between how the parameters of the synthesizer influence the sound, and the core sonic features \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{used by }\DIFaddend the loss function.

\DIFdelbegin \DIFdel{We also see that the }%DIFDELCMD < \DTWEnv{} %%%
\DIFdel{and }%DIFDELCMD < \SIMSESpec{} %%%
\DIFdel{(to our knowledge, not previously used in iterative sound-matching), lead to the best results in three of the four programs. This shows room for further creative approaches to differentiable sound-similarity measures, particularly since we observed that SOTA loss functions can be ineffective depending on the design of a synthesizer. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Finally, we surprisingly see }\DIFdelend \DIFaddbegin \textbf{\DIFadd{Q1}}\DIFadd{: To what extent do automatic evaluation metrics agree with manual listening tests? 
}\\\DIFadd{We see }\DIFaddend somewhat consistent results between the rankings assigned by manual hearing tests (which we take as the ground truth), and automatic measures of P-Loss and MSS. \DIFdelbegin \DIFdel{Due to the }\DIFdelend \DIFaddbegin \DIFadd{With the exception of P-Loss in \BPNoise{} narrowly selecting a different spectrogram based loss, all measures of performance selected the same top performer. However, given the }\DIFaddend simplicity of our programs and the occasional deviations, we cannot conclude that these automatic measures are substitutes for human hearing tests. \DIFaddbegin \DIFadd{As many previous works have done (see Table~\ref{tab:summary}, the use of MSS and/or P-Loss as general loss functions does seem appropriate, however, we advise the use of listening tests or custom made loss functions for more specific analysis. 
}\DIFaddend 

\DIFaddbegin \textbf{\DIFadd{Q2}}\DIFadd{: Are DTW and SIMSE effective measures of loss? If so, in what contexts?}\\
\DTWEnv{} \DIFadd{consistently outperformed other measures in amplitude-modulated synthesis, where envelope alignment is critical. }\SIMSESpec{} \DIFadd{performed best in subtractive synthesis, where scale-invariance captures noise-filtering effects more robustly than L1-based measures. These results suggest that the advantages of these novel losses are context-dependent, underscoring the need for further exploration of creative differentiable similarity measures. 
}


\textbf{\DIFadd{Q3}}\DIFadd{: Can iterative differentiable optimization be applied effectively in synthesizers using classical DSP functions? }\\
\DIFadd{The approach of designing DSP functions in Faust and transpiling to differentiable Faust code yielded meaningful outputs and enabled comparative evaluation of losses. The use of Faust for synthesizer definition was convenient, and transpiling to Jax enabled quick gradient optimization. We were able to run our 1200 experiments within 72 hours on a laptop without a GPU}\footnote{\DIFadd{Lenovo ThinkPad T480, i5-8350U CPU at 1.70GHz, 32 GB RAM.}}\DIFadd{.
}

\subsection{\DIFadd{Practical Recommendations}}
    \DIFadd{Our findings not only provide answers to the research questions proposed in the introduction, but also serve as guidelines for practitioners in selecting appropriate similarity measures and synthesis methods for future sound-matching experiments.
}\begin{itemize}
    \item \DIFadd{In lieu of listening tests, MSS or P-Loss are generally useful for large-scale benchmarking. However, we encourage confirmation of conclusions with manual listening, particularly if specificity is important. 
    }\item \DIFadd{For amplitude-modulated synthesis, DTW-based losses such as }\DTWEnv{} \DIFadd{are recommended over spectrogram differences and JTFS.
    }\item  \DIFadd{For subtractive/noise-filtered synthesis, spectrogram based losses are most effective. Based on our results, SIMSE appears to be the better method of spectrogram comparisons relative to L1, but the amplitude agnosticism of SIMSE likely plays a role in its poor performance in additive synthesis.  
    }\item \DIFadd{Iterative differentiable optimization via the Faust-to-JAX pipeline is a viable strategy for defining various differentiable DSP functions, requiring only modest hardware resource and providing a more natural approach to synthesizer definition.
    }\item \DIFadd{In our amplitude modulation experiments, \JTFS{} did not outperform other measures, contradicting prior claims of its superiority for mesostructures~\mbox{%DIFAUXCMD
\cite{vahidi2023mesostructures}}\hskip0pt%DIFAUXCMD
. Thus, our results call into question its effectiveness as a general mesostructure measure, and highlight a need for further research into its utility.
}\end{itemize}

\subsection{\DIFadd{Illustrating Loss Landscapes}}
\label{sec:loss_landscape_examples}
\DIFadd{To complement our quantitative analysis, we provide examples of loss landscapes that illustrate why specific losses succeed or fail in certain synthesis contexts. These are illustrative aids rather than conclusive evidence, and highlight the importance of \PeriodicLoss{} discussed earlier in Section~\ref{sec:lacking}.
}

\DIFadd{We show the landscapes of the loss functions with simplified versions of \BPNoise{} (HP-Noise) and \AmpMod{} (S-Noise-AM), each reduced to a single parameter for one-dimensional visualization. HP-Noise uses only a high-pass filter with cut-off between 100–20,000 Hz, while S-Noise-AM varies only the modulation rate between 0.1–20 Hz. Loss values are normalized to 0–1 to allow comparison across functions.
}

\DIFadd{Figure~\ref{fig:loss_landscape_noisebp} shows the HP-Noise results. }\LoneSpec{}\DIFadd{, }\SIMSESpec{}\DIFadd{, and \JTFS{} exhibit clear minima at the correct parameter (red dashed line), whereas }\DTWEnv{} \DIFadd{remains relatively flat, making gradient-based optimization more difficult.
}

\DIFadd{Figure~\ref{fig:loss_landscape_ampmod} shows the S-Noise-AM results. Here, }\DTWEnv{} \DIFadd{produces the smoothest and most informative landscape, while spectrogram losses are minimized near the target but lack consistent correlation with parameter distance. \JTFS{} remains largely flat. This aligns with }\DTWEnv\DIFadd{'s superior performance in amplitude-modulated synthesis observed in our experiments.
}

\begin{figure*}[ht]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/experiment_plots/comparing_loss_landscapes_normalized.png}
        \caption{\DIFaddFL{Loss landscapes for \BPNoise{} with only a high-pass filter parameter. 
        }\LoneSpec{}\DIFaddFL{, }\SIMSESpec{}\DIFaddFL{, and \JTFS{} show clear global minima near the correct parameter, while }\DTWEnv{} \DIFaddFL{remains flat around the target.}}
        \label{fig:loss_landscape_noisebp}
    \end{minipage}%DIF > 
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/experiment_plots/comparing_lanscapes_1_1d_normalized.png}
        \caption{\DIFaddFL{Loss landscapes for a simplified \AmpMod{} synthesizer with only amplitude modulation parameter. 
        }\DTWEnv{} \DIFaddFL{exhibits the smoothest and most informative landscape, explaining its superior performance in amplitude-modulated synthesis.}}
        \label{fig:loss_landscape_ampmod}
    \end{minipage}
\end{figure*}


\subsection{\DIFadd{Applicability to More Complex Synthesizers}}
\DIFadd{The experiments in this study were conducted with differentiable synthesizers chosen to isolate fundamental synthesis principles, each with two parameters. While this level of simplicity facilitates controlled comparisons, it does not fully capture the richness of real-world synthesizers, which can have hundreds of parameters with various routes. As a result, the extent of the generalizability of our findings does require further research. 
}

\DIFadd{Nonetheless, we believe that the insights here can extend naturally to more complex domains. Depending on the feature of interest, the observed dependence of loss performance on synthesis method is likely to remain stable even with added complexities such as layers of modulation, filtering, or nonlinearity. 
}

\DTWEnv{} \DIFadd{measures very specific features of audio, and its use in the recommended settings---where low frequency amplitude modulation matching is required---would likely yield the desired results regardless of synthesizer complexity. Likewise, the use of }\SIMSESpec{} \DIFadd{or }\LoneSpec{} \DIFadd{for matching filter cut-offs is likely to be successful regardless of the underlying sound. 
}

%DIF >  Third, the iterative differentiable optimization framework proved viable even on limited hardware, which indicates that similar pipelines could be applied to larger differentiable synthesizers if computational resources allow.

%DIF >  At the same time, real-world synthesizers present new challenges. 
%DIF >  High-dimensional parameter spaces may exacerbate issues of local minima and make optimization less stable, particularly when loss functions are only partially informative. 


%DIF >  Non-differentiable components (e.g., physical modeling, complex routing) also limit the direct applicability of gradient-based methods. 

\DIFaddend \section{Summary, Weaknesses, and Conclusion}
\label{sec:summary_conclusion}
\textbf{Summary:} Sound-matching is an umbrella term for the algorithmic programming of audio synthesizers, often with the goal of assisting sound designers. Here we provided a history of sound-matching, major issues in the field, and discussed the importance of ``differentiable iterative sound-matching'' as a natural extension of current literature.

The main hypothesis tested here is whether the performance of differentiable loss functions is influenced by the synthesis techniques used for sound-matching. We \DIFdelbegin \DIFdel{created }\DIFdelend \DIFaddbegin \DIFadd{conducted systematic }\DIFaddend iterative sound-matching experiments by combining four different loss functions with four different sound synthesis programs. We ranked the performance of the iterative sound-matching pipelines for every loss function and program, and observed that the success of the pipeline (that is, how closely the output sound matches the target sound) is program dependent. In other words, different synthesizer programs work best with different loss functions. Notably, we see that our novel use of DTW and SIMSE based differentiable loss functions (\DTWEnv{} and \SIMSESpec) can outperform what are regarded as the SOTA loss functions in 3 of 4 cases, although this is highly synthesizer dependent. 



P-Loss and MSS have frequently been used as automatic performance measures, yet their ``preference'' has rarely been compared to human rankings. We observed that automatic performance measures and manual listening tests were often in agreement, despite this, manual verification of sound-matching results remains a necessity for future works.  

\textbf{Weaknesses:} 
While we cannot prove that a universally best similarity measure does not exist, we can advocate for more creativity in the field based on our findings. Compared to previous work, we presented a more cohesive approach to iterative sound-matching which utilizes a variety of loss functions and synthesis methods. However, there are many other methods of synthesis and sound-similarity that can be combined in practically infinite ways. Due to this large search space, we set arbitrary parameters for the various signal processing functions. We used bare-bones versions of STFT and JTFS with fixed parameters. We did not test complex synthesizers using parallel and sequential DSP functions. Arbitrary hyperparameters such as learning rate and max number of iterations were selected for the DL pipeline, and only the RMSProp optimizer was tested. 

 Like the majority of previous works, this work utilizes in-domain sounds; that is, the target sound is made by the synthesizer, and the parameters are already known. This simplifies the issues of measuring sound-similarity, but it is not a realistic scenario for practical sound-matching. This problem is left for future work, discussed in the next section.

\label{sec:future}
\textbf{Future Work: } The problem of periodic loss-landscapes, noted by many previous works, remains unaddressed~\cite{turian2020sorry,vahidi2023mesostructures,uzrad2024diffmoog,bruford2024synthesizer}. Perhaps this problem emerges due to the periodic nature of sound, which requires better loss landscape navigation methods. An optimizer that is more aware of fluctuations in the gradients would perhaps lead to better solutions than simple gradient descent. Viewing the sound-matching problem as ``the navigation of an agent from an arbitrary point in a gradient field to a target'' closely resembles many classical problems in the field of reinforcement learning (RL)~\cite{sutton2018reinforcement}. Naturally, the application of RL and other heuristic search techniques to the problem of iterative sound-matching would be an important contribution.

Contemporary works often involve the application of domain specific and computationally expensive loss functions~\cite{han2023perceptual,uzrad2024diffmoog}, use of large neural networks~\cite{hershey2017cnn,cramer2019look}, or complex ensemble methods~\cite{turian2022hear}. Such models are useful but intractable; furthermore, training them requires the definition of simpler loss functions, which emphasizes the need for further development of differentiable and expressive loss function implementations. 

Like nearly all previous work in sound-matching, the main measure of success here was the accurate \textit{replication} of sounds (or synth parameters) rather than imitation of sounds outside of the synthesizer's domain. Replication of arbitrary features of sounds is an important component of sound-design, though it is much harder to define or measure. Future works could explore loss functions which only measure certain characteristics of sound (such as \DTWEnv), and whether they can pave the way for better imitation in sound-matching.


\DIFdelbegin %DIFDELCMD < \bibliographystyle{IEEEtran}
%DIFDELCMD < %%%
\DIFdelend %DIF >  \bibliographystyle{IEEEtran}
\bibliography{references}
\DIFaddbegin 

 \DIFaddend\end{document}


