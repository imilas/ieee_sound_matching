\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[strings]{underscore} % allow _ in text

\usepackage[numbers]{natbib} %removed cite package
\usepackage{csquotes}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{caption}   

\usepackage{graphicx}   % for \includegraphics
\usepackage{caption}    % better caption control inside minipage
\usepackage{subcaption} % optional, but useful if you want subfigures later


\hypersetup{
  colorlinks=true,
  linkcolor=blue!60!black,
  urlcolor=blue!60!black,
  citecolor=blue!60!black
}
\setcounter{secnumdepth}{1}
% Compact lists
\setlist[itemize]{leftmargin=1.2em,itemsep=0.3em,topsep=0.3em}
\setlist[enumerate]{leftmargin=1.4em,itemsep=0.3em,topsep=0.3em}

% Handy macros used in your text
\newcommand{\BPNoise}{\textsc{BP-Noise}}
\newcommand{\AmpMod}{\textsc{AmpMod}}
\newcommand{\LossSelect}{\textbf{Loss Selection}}
\newcommand{\SynthSelect}{\textbf{Synthesis Selection}}
\newcommand{\PeriodicLoss}{\textbf{Loss Landscape Navigation}}
\newcommand{\OutDomain}{\textbf{Out-Domain Generation}}
\newcommand{\DTWEnv}{\textsc{DTW\_Envelope}}
\newcommand{\SIMSESpec}{\textsc{SIMSE\_Spec}}
\newcommand{\LoneSpec}{\textsc{L1\_Spec}}
\newcommand{\JTFS}{\textsc{JTFS}}

\title{\vspace{-0.5em}Author Response for T-ASL-11868-2025\\
\large Manuscript: \emph{Evaluating Sound Similarity Metrics for Differentiable, Iterative Sound-Matching}}
\author{}
\date{}

\begin{document}
\maketitle
\vspace{-1.25em}

\noindent Dear Editor, Alberto Bernardini, \\
\noindent
We'd like to thank you and the reviewers for the thoughtful and constructive feedback on our manuscript. We have carefully considered all comments and revised the manuscript accordingly. Here you will find our detailed point-by-point response to the feedback. 

For each reviewer, we first provide the review in full, with the key concerns numbered and in bold. Below the full review, we address the concerns in dedicated sections. Each section contains the reviewer's key requests or issues, followed by our response. Some revisions address more general concerns expressed in the body of the reviews (\nameref{R1.1} and \nameref{R1.2} for example). As such, for the sake of brevity we took the liberty to paraphrase the reviewer's requests and concerns as accurately as possible when necessary. We hope these revisions have addressed your concerns and improved the clarity and quality of the manuscript.

\medskip
\noindent Sincerely, Amir Salimi, Abram Hindle, Osmar R. Za{\"i}ane

\section{Reviewer 1}
\subsection{Original Review}
\noindent
The Introduction section lacks a clear articulation of the central research question early on. The core problem that no single loss function performs optimally across synthesizer types is buried within extended historical context and repeated definitions. Moreover, the use of a single subsection label (A. Digital Synthesis with Sound-Matching) without any follow-up sections is structurally inconsistent. \textbf{(R1.1) We recommend tightening the Introduction to focus more directly on problem motivation and contribution, and removing or completing the subsection structure.} \\
\textbf{(R1.2) The current division of the manuscript into Introduction, Background, and Past Works results in significant conceptual overlap and repetition.} Historical framing, loss function taxonomy, and method classification are spread across all three sections. \textbf{A clearer organizational approach would be to merge Background, and Past Works into a unified Background and Related Work section, where formal definitions, theoretical framing, and literature review are presented coherently.} This would allow the Introduction to focus solely on the motivation, problem framing, and contribution. \\
Section IV titled ``Programs, Methodology, and Results'' is overloaded and blends experimental setup with results and interpretation. \textbf{(R1.3) For clear clarity and alignment with structural norms, we recommend splitting this into three sections: (1) Experimental Setup (including synthesizer programs, loss definitions, optimization procedure); (2) Experimental Results (performance metrics, statistical tests, and rankings); and (3) Discussion (qualitative interpretation, synthesis-specific observations, and practical implications).} \\
Conceptually, the work serves primarily as a benchmarking study rather than a novel methodological contribution. While the comparison is comprehensive, the novel losses (DTW\_Envelope, SIMSE\_Spec) are adaptations of known techniques and are not framed with sufficient theoretical justification. \textbf{(R1.4) There is also limited interpretability analysis; for example, the paper does not explore why DTW performs well on modulated noise or why SIMSE outperforms L1 in certain scenarios.} Additionally, the synthesizers used are very simple (only 2--3 parameters), and all experiments are conducted in-domain, limiting the generalizability of the findings to real-world synthesis environments with more complex parameter spaces or out-of-domain targets. \\
\textbf{(R1.5) The manuscript would also benefit from clearer practical implications.} While the authors argue that sound-matching is a creative process and reject the idea of a universal loss, they stop short of offering actionable heuristics. \textbf{Some guidance (e.g., Use DTW for AM synthesis; SIMSE for subtractive synthesis) would strengthen the paper’s relevance to practitioners.} \\
\textbf{(R1.1) The paper would be improved by formally stating additional hypotheses that are currently only implicit.} For example, the observed agreement between manual and automatic evaluation metrics, the effectiveness of the novel losses, and the usability of iterative differentiable optimization as a general strategy are all important insights, but are not positioned as testable hypotheses upfront. \\
\textbf{Suggested Actions for Revision:}
\begin{enumerate}
  \item \textbf{(R1.1) Clearly state additional hypotheses regarding metric agreement, perceptual evaluation, and loss novelty.}
  \item \textbf{(R1.4) Deepen the technical and perceptual justification for DTW\_Envelope and SIMSE\_Spec.}
  \item \textbf{(R1.5) Add qualitative or spectral analyses of where specific losses succeed or fail.}
  \item \textbf{(R1.6) Expand discussion to address applicability to more complex or real-world synthesizers.}
  \item \textbf{(R1.7) Provide practical takeaways or usage guidelines for different loss-synth combinations.}
  \item \textbf{(R1.3) Restructure the manuscript to clarify the boundary between methodology, results, and discussion.}
\end{enumerate}

\subsection*{R1.1: Introduction Clarity}
\label{R1.1}
\noindent\textbf{Reviewer:}\\
\textbf{We recommend tightening the Introduction to focus more directly on problem motivation and contribution, and removing or completing the subsection structure.
\\}

\noindent\textbf{Response:} \\
We agree with the reviewer’s comment regarding lack of clarity and focus in the Introduction. The Introduction has been revised accordingly: 
\begin{itemize}
  \item The Introduction is now a single continuous section, without subsection headings.  
  \item We have removed unnecessary historical context and repeated definitions, reducing the length by roughly 25\%. We moved some of this material to the new \textbf{II. Background and Related Work} section.  
  \item The central hypothesis now appears at the beginning of the third paragraph (previously it appeared mid-way through paragraph 10).  
  \item In addition to the main hypothesis, we have added three explicit research questions  (answered in \textbf{IV.B Practical Recommendations}), covering metric agreement (Q1), the utility of DTW and SIMSE in implementing novel losses (Q2), and the viability of iterative optimization (Q3). These are stated directly after the central hypothesis:

  \begin{displayquote}
     Beyond this central hypothesis, we also investigate three additional research questions: (Q1) To what extent do automatic evaluation metrics agree with manual listening tests? (Q2) Can loss functions based on Dynamic Time Warping (DTW) and Scale-Invariant Mean Squared Error (SIMSE) provide advantages over SOTA loss functions, and under what conditions?
    (Q3) Is iterative differentiable optimization a viable strategy for design of sound-matching experiments?
\end{displayquote}

  \item The Introduction concludes with a concise statement of our contributions:

  \begin{displayquote}
      The contributions of this paper include (1) an evaluation of multiple differentiable losses across multiple synthesis methods, (2) the introduction and justification of loss functions utilizing DTW and SIMSE (3) a discussion on the utility and agreement between manual and automatic evaluation metrics (4) a nomenclature of different sound-matching approaches and unsolved issues in the field. 
  \end{displayquote}
\end{itemize}

\subsection{R1.2 Improve Background and Related Work}
\label{R1.2}
\noindent\textbf{Reviewer:}\\
\noindent\textbf{The current division of the manuscript into Introduction, Background, and Past Works results in significant conceptual overlap and repetition. Historical framing, loss function taxonomy, and method classification are spread across all three sections. A clearer organizational approach would be to merge Background, and Past Works into a unified Background and Related Work section, where formal definitions, theoretical framing, and literature review are presented coherently. This would allow the Introduction to focus solely on the motivation, problem framing, and contribution.}\\

\noindent\textbf{Response:} \\
We agree that the organization and writing in these sections could be improved. We merged \emph{Background} and \emph{Past Works} into a unified section, \textbf{II. Background and Related Work}. We applied major edits and rewrites to these sections to improve reading and remove redundancies between the various subsections while taking precautions against compromising the flow.
\textbf{II. Background and Related Work} contains the following topics in order:  
\begin{itemize}
  \item A formal definition of sound-matching and its components.
  \item Definitions of synthesizers, losses, and optimization heuristics. 
  \item A chronological review of prior sound-matching approaches.
  \item A discussion of gaps in the field and four critical issues.
  \item A justification for our methodology, and how our experiments address the two critical issues of \emph{Loss and Synthesizer Selection}.
\end{itemize}


\subsection*{R1.3 Organize Methodology, Results, and Discussion.}
\label{R1.3}
\noindent\textbf{Reviewer:}\\
\textbf{Section IV titled ``Programs, Methodology, and Results'' is overloaded and blends experimental setup with results and interpretation. For clear clarity and alignment with structural norms, we recommend splitting this into three sections: (1) Experimental Setup (including synthesizer programs, loss definitions, optimization procedure); (2) Experimental Results (performance metrics, statistical tests, and rankings); and (3) Discussion (qualitative interpretation, synthesis-specific observations, and practical implications).}
\\

\noindent\textbf{Response:} \\
We agree that the earlier draft blurred the boundaries between setup, results, and discussion. In the revised manuscript we implemented the reviewer’s suggestion as follows:

\begin{itemize}
  \item \textbf{III. Experimental Setup and Results A-E} sections now exclusively describe the methodological components: the synthesizer programs, the definitions of losses, the optimization procedure, evaluation metrics, and statistical tests. Each subsection is focused on describing how the experiments are designed, without interpretation of outcomes. For instance, the synthesizer section defines parameter ranges, while the evaluation section specifies how statistical significance is tested. The only partial overlap is at the beginning of the \textbf{III-F Best Performers for Each Program} section, which serves as a transition between results and discussion.

  \item Experimental analysis are presented in \textbf{III-F Best Performers for Each Program} and \textbf{III-G Consistency in Rankings}, which together form the \emph{Results} component. Section III-F reports which losses performed best for each synthesizer, while Section III-G quantifies the consistency of rankings across bootstrap resamples. Keeping all numerical findings here avoids overlap with later interpretation.  

  \item \textbf{IV. Discussion} is structured into four dedicated subsections:  
    \begin{enumerate}
      \item \textbf{IV-A. Key Findings}: summarizes the central empirical trends, such as the dependency of loss performance on synthesis method.  
      \item \textbf{IV-B. Practical Recommendations}: translates results into concrete recommendations for practitioners and researchers.  
      \item \textbf{IV-C. Illustrating Loss Landscapes}: adds visualizations that explain why certain losses succeed or fail in specific contexts.  
      \item \textbf{IV-D. Applicability to More Complex Synthesizers}: discussion on how observed patterns extend beyond the simplified testbed to richer, real-world synthesizers.  
    \end{enumerate}

  \item To improve clarity, we also relocated explanatory material about experiment design (e.g., parameter settings, optimizer configuration) previously scattered across the results section into Sections III-A–E, ensuring that the results sections remain strictly empirical, while the discussion is reserved for interpretation. 
\end{itemize}

Overall, this restructuring produces a clearer separation of topics: methodology is isolated in Section III-A–E, empirical findings are concentrated in Section III-F–G, and broader interpretation is reserved for Section IV. We believe this major restructuring and rewrites of appropriate sections has improved flow of the manuscript and made the boundary between methodology, results, and discussion much clearer.

\subsection*{R1.4 Justification for \DTWEnv{} and \SIMSESpec}
\label{R1.4}
\noindent\textbf{Reviewer:} \\
\textbf{\noindent While the comparison is comprehensive, the novel losses (DTW\_Envelope, SIMSE\_Spec) are adaptations of known techniques and are not framed with sufficient theoretical justification. }\\

\noindent\textbf{Response:} \\
We have expanded the theoretical justifications for DTW\_Envelope and \SIMSESpec{} in subsection \textbf{II.C Sound Representation and Loss Functions}. In \textbf{II.C.2 Fourier Spectrograms}, we discuss the possible application of SIMSE as an alternative to L1:
\begin{displayquote}
    A common limitation of spectrogram-based losses is their sensitivity to global gain: two signals with identical spectral shape but different overall amplitude can yield large errors under L1/L2 norms. 
    Here we propose and test Scale-Invariant Mean Squared Error (SIMSE) as an alternative to L1/L2 norms in spectrogram comparisons. SIMSE normalizes the amplitudes before comparison and emphasizes proportional differences in spectral envelopes~\cite{barron2014shapessimse}. 
    Perceptually, listeners are often tolerant of loudness changes while remaining sensitive to timbral shape, therefore this property could be advantageous for subtractive synthesis, where filter cutoffs reshape spectral balance without predictable changes in total energy. Although SIMSE has been applied in other contexts such as audio reconstruction, to our knowledge its use as a differentiable spectrogram loss in iterative sound-matching is novel.
\end{displayquote}

 In \textbf{II.C.4 Dynamic Time Warping} For DTW\_Envelope, we clarify its perceptual motivation: aligning amplitude envelopes is crucial for AM synthesis, where traditional spectrogram distances fail to capture similarity: 
 \begin{displayquote}
     DTW provides robustness to local temporal shifts, making it well suited for comparing modulated signals where the perceptual similarity lies in envelope dynamics rather than precise onset alignment. 
    For example, two tremolo signals with slightly different phase are perceptually similar but would appear distant under spectrogram L1. 
    A possible use case to consider is the application of DTW to amplitude envelopes, which may be able to match two sounds with similar rates of loudness modulation regardless of alignments. 
    To our knowledge, DTW applied in this way has not been used as a loss in sound-matching (whether differentiable and iterative or not).
 \end{displayquote}
 
 We explicitly frame these losses not as wholly new techniques, but as carefully adapted methods to be tested in an iterative differentiable setting, a contribution that has not previously been made.  

We did not modify the definitions in \textbf{III.A Loss Function Implementation Details}, dedicating this section to only technical explanations. 

\subsection*{R1.5 Analysis of Failure or Success.}
\label{R1.5}
\noindent\textbf{Reviewer:} \\
\textbf{Add qualitative or spectral analyses of where specific losses succeed or fail}\\

\noindent\textbf{Response:} \\
This is an interesting suggestion. Our original experiments were designed to emphasize quantitative, statistically robust measurements of loss function performance, which is why we initially avoided focusing on individual examples of sounds, spectrograms, or loss landscapes (in part to reduce the risk of cherry-picking). We do concede that illustrative examples can provide helpful intuition for readers and motivate future research, however, we believe that an analysis of loss landscapes (rather than spectrograms or qualitative tests) is a more useful approach to an explanation for \textit{where} and \textit{why} losses might succeed or fail.

To this end, we have added a new Section \textbf{IV.C Illustrating Loss Landscapes} where qualitative visualizations of loss landscapes  are added. These examples highlight why specific losses may succeed or fail and reinforce the broader importance of loss landscapes in assessing the appropriateness of similarity measures~\cite{vahidi2023mesostructures,turian2020sorry}, supporting our argument that effective navigation of loss landscapes represents an important gap in current knowledge and a possible direction for future work:


\begin{displayquote}
We show the landscapes of the loss functions with simplified versions of \BPNoise{} (HP-Noise) and \AmpMod{} (S-Noise-AM), each reduced to a single parameter for one-dimensional visualization. HP-Noise uses only a high-pass filter with cutoff between 100–20,000 Hz, while S-Noise-AM varies only the modulation rate between 0.1–20 Hz. Loss values are normalized to 0–1 to allow comparison across functions.

    Figure~\ref{fig:loss_landscape_noisebp} [\textbf{Figure 4 in paper}] shows the HP-Noise results. \LoneSpec{}, \SIMSESpec{}, and \JTFS{} exhibit clear minima at the correct parameter (red dashed line), whereas \DTWEnv{} remains relatively flat, making gradient-based optimization more difficult.

    Figure~\ref{fig:loss_landscape_ampmod} [\textbf{Figure 5 in paper}]  shows the S-Noise-AM results. Here, \DTWEnv{} produces the smoothest and most informative landscape, while spectrogram losses are minimized near the target but lack consistent correlation with parameter distance. \JTFS{} remains largely flat. This aligns with \DTWEnv's superior performance in amplitude-modulated synthesis observed in our experiments.
\end{displayquote}

\begin{figure*}[ht]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/experiment_plots/comparing_loss_landscapes_normalized.png}
        \caption{Loss landscapes for \BPNoise{} with only a high-pass filter parameter. 
        \LoneSpec{}, \SIMSESpec{}, and \JTFS{} show clear global minima near the correct parameter, while \DTWEnv{} remains flat around the target.}
        \label{fig:loss_landscape_noisebp}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/experiment_plots/comparing_lanscapes_1_1d_normalized.png}
        \caption{Loss landscapes for a simplified \AmpMod{} synthesizer with only amplitude modulation parameter. 
        \DTWEnv{} exhibits the smoothest and most informative landscape, explaining its superior performance in amplitude-modulated synthesis.}
        \label{fig:loss_landscape_ampmod}
    \end{minipage}
\end{figure*}

While this may not be the "qualitative" or "spectral" analysis, we believe it is instructive on the mechanisms underlying our quantitative findings and highlight the importance of loss landscape navigation as an open area for future research (See also~\nameref{R4.5}).



\subsection*{R1.6 Applicability to More Complex or Real-world Synthesizers.}
\label{R1.6}
\noindent\textbf{Review:} \\
\noindent \textbf{Expand the discussion to address applicability to more complex or real-world synthesizers}
\\

\noindent\textbf{Response:} \\
We added a new subsection, \textbf{IV.D Applicability to More Complex Synthesizers}, that explicitly discusses how the insights from our controlled two-parameter DSP-based synthesizers generalize to richer synthesizers with many parameters and complex routing. While acknowledging the limitations of our simplified test bed, we argue that the observed dependencies between synthesis method and loss function remain informative even in more complex settings:

\begin{displayquote}
    The experiments in this study were conducted with differentiable synthesizers chosen to isolate fundamental synthesis principle with 2 parameters each. While this level of simplicity facilitates controlled comparisons, it does not fully capture the richness of real-world synthesizers, which can have hundreds of parameters with various routes. As a result, the extent of the generalizability of our findings does require further research. 

    Nonetheless, we believe that the insights here can extend naturally to more complex domains. Depending on the feature of interest, the observed dependence of loss performance on synthesis method will likely remain stable even with added complexities such as layers of modulation, filtering, or nonlinearity. 
    
    \DTWEnv{} measures very specific features of audio, and its use in the recommended settings---where low frequency amplitude modulation matching is required---would likely yield the desired results regardless of synthesizer complexity. Likewise, the use of \SIMSESpec{} or \LoneSpec{} for matching filter cut-offs is likely to be successful regardless of the underlying sound. 
    \end{displayquote}
    

\subsection{R1.7 Provide Practical Takeaways}
\label{R1.7}
\noindent\textbf{Review:} \\
\noindent \textbf{Provide practical takeaways or usage guidelines for different loss-synth combinations. } 
\\

\noindent\textbf{Response} \\
In \textbf{IV.A Key Findings}, we discuss important findings and provide answers to our research questions. The following section, \textbf{IV.B Practical Recommendations}, distills our findings into actionable heuristics for practitioners. 

\begin{displayquote}
    Our findings not only provide answers to the research questions proposed in the introduction, but also serve as guidelines for practitioners in selecting appropriate similarity measures and synthesis methods for future sound-matching experiments.
\begin{itemize}
    \item In lieu of listening tests, MSS or P-Loss are generally useful for large-scale benchmarking. However, we encourage confirming the results with manual listening, particularly if specificity is important. 
    \item For amplitude-modulated synthesis, DTW-based losses such as \DTWEnv{} are recommended over spectrogram differences and JTFS.
    \item  For subtractive/noise-filtered synthesis, spectrogram based losses are most effective. Based on our results, SIMSE appears to be the better method of spectrogram comparisons relative to L1, but the amplitude agnosticism of SIMSE likely plays a role in its poor performance in additive synthesis.
    \item Iterative differentiable optimization via the Faust-to-JAX pipeline is a viable strategy for defining various differentiable DSP functions, requiring only modest hardware resource and providing a more natural approach to synthesizer definition.
    \item In our amplitude modulation experiments, \JTFS{} did not outperform other measures, contradicting prior claims of its superiority for mesostructures~\cite{vahidi2023mesostructures}. Thus, our results call into question its effectiveness as a general mesostructure measure, and highlight a need for further research into its utility.
\end{itemize}

\end{displayquote}
\section{Reviewer 2}
\subsection{Original Review}
\noindent
The paper considers the evaluation of different audio similarity metrics for sound-matching applications.  The authors categorize prior methods largely as direct optimization via gradient-free iterative search or open-loop inference via a supervised deep learning model.  Moreover, different approaches which may or may not accommodate out-of-domain sounds.  Accordingly, the authors propose the combined use of differentiable synthesizers and closed-loop iteration as a sound means of achieving synthesizer sound-matching, effectively combining the advantages of otherwise disparate methods (i.e. access to gradients while iteratively improving matching results).  Additionally, within their evaluation of different similarity metrics (parameter losses, spectral distances, etc.), they also introduce a DTW-based similarity metric to partially neutralize the effect of time alignment on similarity scores.  A primary conclusion of the work is that there is no one best sound similarity measure capable of working across all synthesizers.  \\
The subject matter of the work is relevant, the experimental setup is satisfactory, and I do not have reason to doubt the correctness of their analysis.  However, my primary concerns with this work are that:  
\begin{enumerate}
  \item \textbf{(R2.1) It is not clear what the proposed method really is, and the paper reads largely as a readout of evaluations that were conducted by the authors.}  
  \item \textbf{(R2.2) The authors claim that differentiable iterative sound-matching is largely unexplored, encouraging development of techniques in this area.}  
    \begin{itemize}
      \item \textbf{Barkan et. al, ``Inversynth II: sound-matching via Self-Supervised Synthesizer-Proxy and Inference-Time Finetuning'' explored precisely this, training an initial ``one-shot'' supervised model, and then performing inference-time optimization, updating model weights to improve the matching against test audio.  Moreover, their use of a synthesizer-proxy negates the explicit need of implementing the synthesizer math differentiably, while still falling under the category of differentiable iterative sound-matching.}  
      \item \textbf{Cherep et. al, ``Creative Text-to-Audio Generation via Synthesizer Programming'' uses a differentiable synthesizer and the CLAP score as a similarity metric for iterative optimization.  While the authors lean into synthesizer sound-matching against a text query, the method would be readily applicable to audio-to-audio sound-matching using the CLAP audio tower alone.}  
    \end{itemize}
  \item \textbf{(R2.3) This work addresses the problems on ``Loss Selection'' and ``Synthesis Selection.''  However, the results as presented here are largely inconclusive (i.e. it is not imminently clear how to select losses/synthesizer pairings), leaving very little in the way of clear takeaways sparking future research.}
\end{enumerate}

\subsection{R2.1 Clarity of Proposed Method}
\label{R2.1}
\noindent\textbf{Reviewer:}\\
\noindent \textbf{It is not clear what the proposed method really is, and the paper reads largely as a readout of evaluations. } 
\\

\noindent\textbf{Response:} \\
We have taken several measures to emphasize the importance and reasoning behind our methodology design and ensure that the paper is not read as a set of evaluation results.  

\medskip
\noindent In \textbf{II.B Digital Signal Processing and Synthesis}, we state the importance of isolated sound-matching experiments for analysis of loss-synthesizer interactions:
\begin{displayquote}
    
Other synthesis methods studied in \textit{isolation} include additive and subtractive synthesis~\cite{engel2020ddsp,masuda2023improving,salimi2020make} and physical modeling~\cite{riionheimo2003parameter,han2024learning}. By \textit{isolation}, we mean settings where the effect of individual parameters on the output sound remains tractable. For example, we exclude studies using commercial Virtual Studio Technology (VST) which can obscure the interactions between modules, losses, and outputs. 
\end{displayquote}

\noindent To further clarify the methodology and its importance, we have added a new subsection, \textbf{II.H What Approach Should We Take?} placed immediately before the Methodology section. This subsection clarifies our experimental design and explains why our contribution is unique. Specifically, we use:  
\begin{enumerate}
  \item Simple differentiable DSP synthesizers to isolate synthesis--loss interactions,  
  \item Multiple losses tested side by side within the same framework, and  
  \item Direct gradient descent optimization without neural proxies.  
\end{enumerate}

This quote from the aforementioned section in the paper makes clear that our work is not simply a report of evaluations, but a controlled benchmarking study designed to reveal how losses and synthesis methods interact: 

\begin{displayquote}
    We argue that isolated benchmarking is a necessary precursor to building robust, generalizable sound-matching systems. To address the problems of \LossSelect{} and \SynthSelect{}, we adopt a methodology with three key characteristics that have not previously appeared together.

\begin{enumerate}
    \item Use simple differentiable synthesizers built from common DSP functions (oscillators, filters, envelopes) to address the \SynthSelect{} problem. Restriction to small-scale, low-parameter models avoids the confounding effects of neural proxies or embedding models, enabling direct analysis of how synthesis method interacts with loss functions. 
    \item Evaluate experiments under multiple loss functions to address the \LossSelect{} problem. Prior work has typically tested losses in isolation, making direct comparisons of loss functions rare~\cite{vahidi2023mesostructures,han2023perceptual,uzrad2024diffmoog}, particularly if comparisons are done in the context of different methods of synthesis. 
    \item Optimize synthesizer parameters directly with gradient descent, without neural approximations. This complements prior proxy-based methods by providing the missing “low-level experiments” that aim to analyze fundamental interactions between DSP functions and similarity measures. 
\end{enumerate}

Together, these design choices supply the controlled experimental evidence missing from the literature, 
clarifying how losses and synthesis methods interact in differentiable, iterative settings, 
and offering insights likely to generalize to more complex domains.

\end{displayquote}


\medskip
\noindent We restate our methodology in the opening paragraph of the \textbf{III. Experimental Setup} section:

\begin{displayquote}
    At a glance, our methodology is to conduct controlled, low-level differentiable experiments—without proxy networks—by pairing four DSP-based synthesizers with four differentiable loss functions. We optimize parameters directly via gradients in order to isolate loss–synthesizer interactions. This controlled setup systematically evaluates the performance of iterative sound-matching pipelines, where “performance” is defined as the similarity of the synthesized output to a target. Similarity is assessed with P-Loss, MSS, and—most importantly—manual listening tests. This design allows us to directly test our central hypothesis: that the effectiveness of a loss function depends on the synthesis method, and that no universal “best” similarity measure exists.
\end{displayquote}

\subsection{R2.2 Relation to Prior Work}
\label{R2.2}
\noindent\textbf{Reviewer:}\\
\noindent \textbf{Review: The authors claim that differentiable iterative sound-matching is largely unexplored, but prior work (Barkan et al., Cherep et al.) could be considered in this category.}\\

\noindent\textbf{Response:} \\
We thank the reviewer for pointing out sound-matching works that match the ``differentiable and iterative'' description, but were not covered in our literature review. We believe that the term ``under-explored'' is perhaps more appropriate. We've added these works to \textbf{Table 1} which provides a summary of notable previous work, and discussed them in more detail in the final paragraphs of \textbf{II.F Historical Framing of Sound-Matching}: 
\begin{itemize}
  \item \textbf{Barkan et al.:} Their work relies on neural proxies for the synthesizer, with inference-time finetuning of the encoder. This constitutes iterative optimization, but the optimization occurs on an approximate model rather than directly on differentiable DSP functions. By contrast, our approach avoids proxy error and directly analyzes how losses behave relative to known synthesis structures.  
  \item \textbf{Cherep et al.:} Their work uses CLAP embeddings with evolutionary search for text-to-audio tasks. While applicable to audio-to-audio matching, CLAP acts as a black-box representation and does not isolate loss--synth interactions. Our approach is complementary, focusing on controlled experiments with simple differentiable DSP synthesizers to expose the strengths and weaknesses of different similarity measures.  In addition, embedding models such as CLAP use simpler representations at their core---commonly Fourier transformations---and our work explores the low level functions that are often used for the training of embedding models.
\end{itemize}
Looking at these works, we see that our contribution is distinct: we provide the missing \textbf{low-level benchmarking} of loss functions across multiple synthesis methods, which is not addressed by proxy-based or embedding-based approaches.

\subsection*{R2.3 Clear Takeaways}
\noindent\textbf{Reviewer:}\\
\textbf{This work addresses the problems on ``Loss Selection'' and ``Synthesis Selection.''  However, the results as presented here are largely inconclusive (i.e. it is not imminently clear how to select losses/synthesizer pairings), leaving very little in the way of clear takeaways sparking future research.\\}

\noindent\textbf{Response:} \\
We acknowledge the reviewer’s concern that our main result may appear inconclusive. However, we see this as an important finding in itself: much of the prior literature implicitly over-claims general effectiveness of particular loss functions based on narrow or in-domain settings. Our revisions now explicitly emphasize that the lack of a “silver bullet” for loss functions is a core contribution of this work. By showing that loss effectiveness depends on the synthesis context, the inconclusiveness can be seen as evidence against over-generalization, and an argument for more thorough, context-sensitive benchmarks in measuring sound similarity.\\

As for clear takeaways, we've added two new sections which address the valid issues raised here and by Reviewer 1. 

\begin{itemize}
  \item \textbf{IV.B Practical Recommendations} which distills our findings into actionable guidelines for practitioners. DTW\_Env is recommended for amplitude-modulated synthesis, SIMSE for subtractive/noise-filtered synthesis, and MSS or P-Loss as large-scale proxies for sound-matching evaluation (with manual validation being strongly advised). We also note that our results challenge prior claims about JTFS’s universal superiority for mesostructures.
  \item \textbf{IV.D Applicability to More Complex Synthesizers} that explicitly discusses how the insights from our controlled two-parameter DSP-based synthesizers generalize to richer synthesizers with many parameters and complex routing. While acknowledging the limitations of our simplified testbed, we argue that the observed dependencies between synthesis method and loss function remain informative even in more complex settings.
\end{itemize}
For a more detailed response, see our response to Reviewer 1 in Sections~\nameref{R1.6} and \nameref{R1.7}. Our response to \nameref{R1.5} may also be of interest.


\section{Reviewer 4}
Here we've addressed the four bigger concerns first, leaving the minor edits for \nameref{r4.minor}.


\subsection{Original Review}
Well written paper with comprehensive literature review and extensive experiments to bridge existing research gap. Although this paper does not have any ground-breaking new methods that could achieve SOTA performance, the experimental results could still serve as a useful reference for researchers who want to try this field. So I believe it should be accepted with some minor modifications.

(\textbf{R4.1}) Page 3, Left Column, the first equation.
\textbf{I am not sure if `t` is a common symbol to represent `target` in sound marking, but my field of audio/signal processing, `t` usually represents `time`. So it is a bit confusing to me when reading it for the first time.
Would something like $x_\theta$ and $x_t$ be better?} So x always represent the sound. And the subscript indicated if the sound is from the output given the parameter theta, or the target sound. Even better, probably we can use $x_0$ instead of $x_t$ to represent the target sound?

Page 3 Right column section C.
Isn't it better to move the symbol explanation earlier to where the first equation is? I could only under the equations after reading section C.

(\textbf{R4.2}) \textbf{Page 3 Right column at the end
You mentioned "There is no optimal or correct version of an 8-bit snare sound".
This statement is true when you are given only the text description of the sound. However, if you have the target sound $x_0$ (or t in your current draft), the target sound itself is "correct", isn't it?} So your hypothesis only holds if you don't have the target sound but the text description, am I understanding it correctly?

Page 6 Right column Section C Point 4.
"out-of-domain sounds is an unexplored area". Unexplored means no one has done it before to me. But in Table 1, you have already cited a few work that tried to do it. So, should it be "underexplored" instead of "unexplored"?


Page 6 Right column Section IV A
You mentioned "four different synthesizers" here, but you never elaborate on which four, not until page 8. Probably you can specify the four synthesizers earlier?

(\textbf{R4.3}) \textbf{Also, in the blinded hearing test, the two listeners are the authors, it seems a bit too biased to me.} Not sure how difficult is it to add one more listener, but I believe having an extra sound engineer/musician who is not one of the authors would yield a more convincing listening results.

Page 7 Left Column 1st Paragraph
You described the Likert scale from 1 to 5 here. But in the next paragraph, you describe the Likert scores again. Not sure if I misunderstood anything, but It looks a bit redundant to me. Would it be possible to combine both to reduce redundancy?

(\textbf{R4.4}) \textbf{Page 7 Right Column section B
"If one of these tones is slightly shifted in time, then the spectrogram will no longer overlap". This is true if we don't apply any techniques like DTW to realign them, right?}
So the STFT losses doesn't work for the cases where there's time shift?

Page 11, the "weakness" section under Summary
Just a minor grammar question, Is there a missing subject in the first sentence?
While [we] cannot prove ...


(\textbf{R4.5}) O\textbf{ne final question that is just out of curiosity.
In your iterative algorithm, you always run it for 200 steps, right?
In my field of machine learning, if we set 200 steps, the result in the final step might not be the optimal one due to overfitting. So sometimes we obtain the optimal results at around 100 steps, and the model gets worse after this.
Do you have the same thing in your algorithm? Or the final step is always the best step?}



\subsection{\textbf{R4.1} Use of Appropriate Symbols}
\noindent\textbf{Reviewer:}\\
\textbf{The notation \textbf{[for the equation introduced in II.A]} is confusing. In the manuscript, $t$ is used to denote the target sound, but in audio/signal processing $t$ usually represents time.  
A clearer alternative would be to write $x_\theta$ and $x_0$, so that $x$ always denotes a sound waveform, with the subscript indicating whether it is generated from parameters $\theta$ or given as the target.  
This avoids overloading $t$ and improves readability.}
\\

\noindent\textbf{Response:}
This is a helpful suggestion, and we have made the switch from $x$ and $t$ to $x_{\theta}$ and $x_0$ throughout the manuscript. 

\subsection{\textbf{R4.2} Optimal Targets}
\noindent\textbf{Reviewer:}\\
\textbf{You mentioned "There is no optimal or correct version of an 8-bit snare sound".
This statement is true when you are given only the text description of the sound. However, if you have the target sound $x_0$  the target sound itself is "correct", isn't it? So your hypothesis only holds if you don't have the target sound but the text description, am I understanding it correctly?\\}

\noindent\textbf{Response:}
The original statement referred to how realistically in sound-design, a ``correct" solution rarely exists due to our subjective experience of sound and personal goals. The existence of an objective sound-matching goal is only true in \textit{in-domain replication} scenarios, which are far removed from real life applications. To reduce confusion, the section has been rephrased as follows:
\begin{displayquote}
    Moreover, the ``correct'' solution may not exist in a strict sense: depending on the artistic goal, multiple parameter settings may yield acceptable or even preferable outputs.  In the following sections we will discuss the various components of sound-matching, the subjectivity of ``correct'' solutions, and important issues in the field.
\end{displayquote}

\subsection{R4.3 Hearing Tests}
\label{}
\noindent\textbf{Reviewer:}\\
\textbf{Also, in the blinded hearing test, the two listeners are the authors, it seems a bit too biased to me. Not sure how difficult is it to add one more listener, but I believe having an extra sound engineer/musician who is not one of the author would yield a more convincing listening results.
}
\\

\noindent{\textbf{Response:}}
The listening tests were done without use of any labels or previous exposure to outputs, which we believe minimizes the risk of bias. Additionally the strong correlation between the two listeners adds to our confidence in using the listening test results as the ground truth. Unfortunately, adding another listener was not possible given the allotted time for the revisions. If the reviewer feels that this is necessary, it can be done for the next round of reviews. 

\subsection{\textbf{R4.4} STFT Losses and Time-Shifts}
\noindent\textbf{Reviewer:}\\
\textbf{"If one of these tones is slightly shifted in time, then the spectrogram will no longer overlap". This is true if we don't apply any techniques like DTW to realign them, right?
So the STFT losses doesn't work for the cases where there's time shift?}
\\

\noindent\textbf{Response:}
What we refer to as STFT losses refers to the near ubiquitous approach of applying L1/L2 to STFT spectrograms. This approach does not account for two sounds that are nearly identical but slightly time-shifted~\cite{vahidi2023mesostructures,turian2020sorry}. We've shown in this work that DTW can be used in loss functions to account for envelope time-shifts.

\subsection{\textbf{R4.5} Can We Overfit? Is it a Concern?}
\label{R4.5}
\noindent\textbf{Reviewer:}\\
\textbf{One final question that is just out of curiosity.
In your iterative algorithm, you always run it for 200 steps, right?
In my field of machine learning, if we set 200 steps, the result in the final step might not be the optimal one due to overfitting. So sometimes we obtain the optimal results at around 100 steps, and the model gets worse after this.
Do you have the same thing in your algorithm? Or the final step is always the best step?
}
\\

\noindent\textbf{Response:} 
Since we are not training a model that will be applied to other scenarios, overfitting here just means that the parameter updates are stuck around a local or global minimum. This means that \textit{if} the target parameters have been reached, subsequent updates will not diverge from it (or at least stay very close). We share some visualizations of the gradient update paths in 2D, which we felt was not statistically robust enough for the paper. In Figures~\ref{fig:p0_simse_iterations} and~\ref{fig:p2_dtw_iterations}, we see that once the gradient updates (small red dots connected by lines) take the parameters from the starting position (green circle) to the target (green star), subsequent updates fluctuate slightly and randomly around the target. 


\begin{figure*}[h]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/experiment_plots/p0_SIMSE_Spec.png}
        \caption{SIMSE\_Spec reaching target and not diverging in \BPNoise}
        \label{fig:p0_simse_iterations}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/experiment_plots/p2_DTW_Onset.png}
        \caption{DTW\_Env reaching target and not diverging in \AmpMod}
        \label{fig:p2_dtw_iterations}
    \end{minipage}
\end{figure*}


\subsection{R4 Minor Edits}
\label{r4.minor}
\noindent\textbf{Reviewer:}\\
Isn't it better to move the symbol explanation earlier to where the first equation is? I could only under the equations after reading section C.

\noindent\textbf{Response:}
We have moved symbol explanations right after the introduction in 
\textbf{II. A Formalization of Sound-Matching}. 
\\

\noindent\textbf{Reviewer:}\\
"out-of-domain sounds is an unexplored area". Unexplored means no one has done it before to me. But in Table 1, you have already cited a few work that tried to do it. So, should it be "underexplored" instead of "unexplored"?

\noindent\textbf{Response:}
We've changed our phrasing where necessary. See also \nameref{R2.2}.
\\

\noindent\textbf{Reviewer:}\\
You mentioned "four different synthesizers" here, but you never elaborate on which four, not until page 8. Probably you can specify the four synthesizers earlier?

\noindent\textbf{Response:}
We've modified the beginning of Section \textbf{III. Experimental Setup and Results}

\begin{displayquote}
    At a glance, our methodology is to conduct controlled, low-level differentiable experiments without the use of proxy networks. We pair four differentiable losses with four differentiable DSP-based synthesizers: BP-Noise (band-pass noise with LP/HP filters), Add-SineSaw (additive sine and saw oscillators), Noise-AM (noise modulated by an LFO), and SineSaw-AM (sine-modulated saw oscillator). 
\end{displayquote}

\noindent\textbf{Reviewer:}\\
You described the Likert scale from 1 to 5 here. But in the next paragraph, you describe the Likert scores again. 

\noindent\textbf{Response:}
This has been fixed in our rewrite of the methodology section (\textbf{III. Experimental Setup and Results}).
\\

\noindent\textbf{Reviewer:}\\
Fix "While [we] cannot prove ..."

\noindent\textbf{Response:}
Fixed, thank you.
\\


\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
